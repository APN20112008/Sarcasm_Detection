{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR5Hn_eerXIA",
        "outputId": "bb26a3fb-f28b-4a6b-866d-8880d17fbe74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9HkWEgKq8FG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAdjujLerUY9",
        "outputId": "7dd48ada-0910-4bc6-a689-c6ef02e7eb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTgJE4idq8FL"
      },
      "source": [
        "## Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pcaa-h-q8FO"
      },
      "outputs": [],
      "source": [
        "data_v1= pd.read_json(\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/Sarcasm_Headlines_Dataset.json\",lines=True)\n",
        "data_v2= pd.read_json(\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/Sarcasm_Headlines_Dataset_v2.json\",lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "krgHLer9q8FP",
        "outputId": "b6602325-dfe1-427a-a72d-b650f76bbd7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        article_link  \\\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
              "2  https://local.theonion.com/mom-starting-to-fea...   \n",
              "3  https://politics.theonion.com/boehner-just-wan...   \n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
              "\n",
              "                                            headline  is_sarcastic  \n",
              "0  former versace store clerk sues over secret 'b...             0  \n",
              "1  the 'roseanne' revival catches up to our thorn...             0  \n",
              "2  mom starting to fear son's web series closest ...             1  \n",
              "3  boehner just wants wife to listen, not come up...             1  \n",
              "4  j.k. rowling wishes snape happy birthday in th...             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd91c170-f8cb-4f73-8094-24abd0e5d0d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd91c170-f8cb-4f73-8094-24abd0e5d0d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd91c170-f8cb-4f73-8094-24abd0e5d0d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd91c170-f8cb-4f73-8094-24abd0e5d0d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data_v1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBKEWnw1q8FQ",
        "outputId": "0f65afd0-9b7a-4980-c6a7-f5a3877f55e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data v1 columns : Index(['article_link', 'headline', 'is_sarcastic'], dtype='object')\n",
            "data v2 columns : Index(['article_link', 'headline', 'is_sarcastic'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print (\"data v1 columns :\", data_v1.columns)\n",
        "print (\"data v2 columns :\", data_v1.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avz4hKDOq8FR"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FswnEZGq8FS"
      },
      "outputs": [],
      "source": [
        "sarcastic_data_v1_counts=data_v1.is_sarcastic.value_counts()\n",
        "sarcastic_data_v2_counts= data_v2.is_sarcastic.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJCoIB-vq8FS",
        "outputId": "c4b690d9-aa61-4411-fc45-24591dea15d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of sarcastic comments in v1=11724\n",
            "number of non-sarcastic comments in v1=14985\n",
            "ratio: 0.7823823823823823\n"
          ]
        }
      ],
      "source": [
        "sarcasm_count_v1= sarcastic_data_v1_counts[1]\n",
        "non_sarcastic_count_v1= sarcastic_data_v1_counts[0]\n",
        "ratio_sarc_nonsarc_v1 = sarcasm_count_v1 / non_sarcastic_count_v1\n",
        "\n",
        "print (f\"number of sarcastic comments in v1={sarcasm_count_v1}\")\n",
        "print (f\"number of non-sarcastic comments in v1={non_sarcastic_count_v1}\")\n",
        "print(f\"ratio: {ratio_sarc_nonsarc_v1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk5jKwcuq8FT",
        "outputId": "f8d66044-a135-4e3e-f622-613f265cb4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of sarcastic comments in v2=13634\n",
            "number of non-sarcastic comments in v2=14985\n",
            "ratio: 0.9098431765098431\n"
          ]
        }
      ],
      "source": [
        "sarcasm_count_v2= sarcastic_data_v2_counts[1]\n",
        "non_sarcastic_count_v2= sarcastic_data_v2_counts[0]\n",
        "ratio_sarc_nonsarc_v2 = sarcasm_count_v2 / non_sarcastic_count_v2\n",
        "\n",
        "print (f\"number of sarcastic comments in v2={sarcasm_count_v2}\")\n",
        "print (f\"number of non-sarcastic comments in v2={non_sarcastic_count_v2}\")\n",
        "print(f\"ratio: {ratio_sarc_nonsarc_v2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zexf1LoSq8FU",
        "outputId": "b8d7b21f-0938-4b81-bd66-d09eed6c9610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "V2 DATASET IS MORE BALANCED\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print (\"\"\"\n",
        "V2 DATASET IS MORE BALANCED\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_v1 = data_v1.loc[data_v1.is_sarcastic==1]\n",
        "non_sarcasm_v1 = data_v1.loc[data_v1.is_sarcastic==0]\n",
        "\n",
        "sarcasm_v2 = data_v2.loc[data_v2.is_sarcastic==1]\n",
        "non_sarcasm_v2 = data_v2.loc[data_v2.is_sarcastic==0]"
      ],
      "metadata": {
        "id": "KHpFS9_dszEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkglGYQpq8FV",
        "outputId": "6520ca8b-e293-4d0d-b5a0-50e4393a5e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of common sarcastic comments =  11724\n",
            "number of common sarcastic comments =  14985\n"
          ]
        }
      ],
      "source": [
        "#check if v2 and v1 have repetitive data\n",
        "sarc_common_comment=[]\n",
        "non_sarc_common_comment=[]\n",
        "for comment in sarcasm_v1.headline.values :\n",
        "    if comment in sarcasm_v2.headline.values:\n",
        "        sarc_common_comment.append(comment)\n",
        "print(\"number of common sarcastic comments = \", sarc_common_comment.__len__() )\n",
        "\n",
        "for comment in non_sarcasm_v1.headline.values :\n",
        "    if comment in non_sarcasm_v2.headline.values:\n",
        "        non_sarc_common_comment.append(comment)\n",
        "print(\"number of common sarcastic comments = \", non_sarc_common_comment.__len__() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lGE9Lehq8FW",
        "outputId": "f89c3035-3ae2-4deb-9d45-12ca7586c382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of common sarcastic comments is the same as total number of sarcastic comments in V1.\n",
            "Number of common non sarcastic comments is the same as total number of non sarcastic comments in V1.\n",
            "\n",
            "It's enough information to conclude that V2 dataset just has more sarcastic comments than V1 dataset's.\n",
            "\n",
            "Henceforth, use only V2 dataset.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print (\"\"\"\n",
        "Number of common sarcastic comments is the same as total number of sarcastic comments in V1.\n",
        "Number of common non sarcastic comments is the same as total number of non sarcastic comments in V1.\n",
        "\n",
        "It's enough information to conclude that V2 dataset just has more sarcastic comments than V1 dataset's.\n",
        "\n",
        "Henceforth, use only V2 dataset.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDhlaKwCq8FW",
        "outputId": "241affca-20ed-48fd-acc8-26e6f26f9e5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   is_sarcastic                                           headline\n",
              "0             1  thirtysomething scientists unveil doomsday clo...\n",
              "1             0  dem rep. totally nails why congress is falling...\n",
              "2             0  eat your veggies: 9 deliciously different recipes\n",
              "3             1  inclement weather prevents liar from getting t...\n",
              "4             1  mother comes pretty close to using word 'strea..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-762a46b5-d4ab-4ef7-bdbf-6fd7fc9d3837\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-762a46b5-d4ab-4ef7-bdbf-6fd7fc9d3837')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-762a46b5-d4ab-4ef7-bdbf-6fd7fc9d3837 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-762a46b5-d4ab-4ef7-bdbf-6fd7fc9d3837');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "data= data_v2.iloc[:,:-1] #'article link' column isn't required\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE1JXuu4q8FX"
      },
      "outputs": [],
      "source": [
        "sarcastic_df = data.loc[data.is_sarcastic==1]\n",
        "not_sarcastic_df = data.loc[data.is_sarcastic==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPSV7RJ_q8FX"
      },
      "outputs": [],
      "source": [
        "sarc_commentsList = sarcastic_df['headline'].astype(str).tolist()\n",
        "notSarc_commentsList = not_sarcastic_df['headline'].astype(str).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NbgiYneq8FX"
      },
      "outputs": [],
      "source": [
        "def countSymbols(commentsList,typename):\n",
        "    symbolList= \",.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-\"\n",
        "    symCountDict= {'Symbols':[],'Count':[],'Type':[]}\n",
        "    for symbol in symbolList:\n",
        "        symCountDict['Symbols'].append(symbol)\n",
        "        symCountDict['Count'].append (sum([comm.count(symbol) for comm in commentsList]))\n",
        "        symCountDict['Type'].append(typename)\n",
        "    return symCountDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REfeP1DFq8FY"
      },
      "outputs": [],
      "source": [
        "sarcSymCount= countSymbols(sarc_commentsList, 'sarcastic')\n",
        "non_sarcSymCount=countSymbols(notSarc_commentsList,'not_sarcastic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6WY_95iq8FY"
      },
      "outputs": [],
      "source": [
        "sarcdf= pd.DataFrame(sarcSymCount)\n",
        "nonsarcdf= pd.DataFrame(non_sarcSymCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-y1DXRYq8FY",
        "outputId": "71466076-8ffe-4c4b-ba58-6e0e05cfc088"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Symbols  Count       Type\n",
              "0       ,   1756  sarcastic\n",
              "1       .    886  sarcastic\n",
              "2       \"     15  sarcastic\n",
              "3       '   5506  sarcastic\n",
              "4       !     39  sarcastic"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41f0d03f-2c1f-4dee-9c08-83df7e642bd8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbols</th>\n",
              "      <th>Count</th>\n",
              "      <th>Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>,</td>\n",
              "      <td>1756</td>\n",
              "      <td>sarcastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.</td>\n",
              "      <td>886</td>\n",
              "      <td>sarcastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"</td>\n",
              "      <td>15</td>\n",
              "      <td>sarcastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'</td>\n",
              "      <td>5506</td>\n",
              "      <td>sarcastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>!</td>\n",
              "      <td>39</td>\n",
              "      <td>sarcastic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41f0d03f-2c1f-4dee-9c08-83df7e642bd8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41f0d03f-2c1f-4dee-9c08-83df7e642bd8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41f0d03f-2c1f-4dee-9c08-83df7e642bd8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "allsarcdf = sarcdf.append(nonsarcdf)\n",
        "allsarcdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSrZq-fpq8FZ",
        "outputId": "4aa2601f-3d01-497a-df30-001bf370082f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1067.75x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.set_style('darkgrid')\n",
        "#style.use('dark_background')\n",
        "matplotlib.rcParams['font.size']='20'\n",
        "matplotlib.rcParams['axes.labelsize']='20'\n",
        "plot= sns.catplot(\n",
        "    x='Symbols',\n",
        "    y= 'Count' ,\n",
        "    data=allsarcdf,\n",
        "    hue='Type',\n",
        "    kind='bar',\n",
        "    palette='summer',\n",
        "    height=7,\n",
        "    aspect=12/7,\n",
        "    legend=True,\n",
        "    margin_titles=False,\n",
        ")\n",
        "\n",
        "plot.fig.suptitle(\"sarc non sarc stuff\",\n",
        "                  fontsize=20,fontfamily= 'fantasy');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMWzffMdq8FZ"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilFXml8hq8FZ",
        "outputId": "7fc68923-381c-4206-b438-f1f7674f0ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "is_sarcastic    0\n",
              "headline        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLkCXc0Bq8Fa",
        "outputId": "ec482e02-f6d3-4309-f76f-a50674846a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " NO NULL ENTRIES\n"
          ]
        }
      ],
      "source": [
        "print ( \" NO NULL ENTRIES\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKY6WlMDq8Fa"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    text = pattern.sub('', text)\n",
        "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
        "    emoji = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    text = emoji.sub(r'', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\" im |^im\", \"where is\", text)\n",
        "\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\" hes |^hes\", \"he is\", text)\n",
        "\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\" shes |^shes\", \"she is\", text)\n",
        "\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\" thats |^thats\", \"that is\", text)\n",
        "\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\" whats |^whats\", \"what is\", text)\n",
        "\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\" wheres |^wheres\", \"where is\", text)\n",
        "\n",
        "    text = re.sub(r\"ain't\", \"is not\", text)\n",
        "    text = re.sub(r\" aint |^aint\", \"is not\", text)\n",
        "\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\" wont |^wont\", \"will not\", text)\n",
        "\n",
        "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
        "    text = re.sub(r\" wasnt |^wasnt\", \"was not\", text)\n",
        "\n",
        "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
        "    text = re.sub(r\" hasnt |^hasnt\", \"has not\", text)\n",
        "\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\" dont |^dont\", \"do not\", text)\n",
        "\n",
        "    text = re.sub(r\"didn't\", \"did not\", text)\n",
        "    text = re.sub(r\" didnt |^didnt\", \"did not\", text)\n",
        "\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\" cant |^cant\", \"can not\", text)\n",
        "\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\" its |^its\", \"it is\", text)\n",
        "\n",
        "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "    text = re.sub(r\" coudlnt |^couldnt\", \"could not\", text)\n",
        "\n",
        "    text = re.sub(r\"haven't\", \"have not\", text)\n",
        "    text = re.sub(r\" havent |^havent\", \"have not\", text)\n",
        "\n",
        "    text = re.sub(r\" theyre |^theyre\", \"they are\", text)\n",
        "\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "\n",
        "    #based on EDA, symbols{' : ? - } should be preserved\n",
        "    text = re.sub(r\"[.!,\\\"@#$%^&*(){}/;`~<>+=]\", \" \", text)\n",
        "    \"\"\"\n",
        "    rather than subbing with \"\", it's better to sub with \" \", to show that there was something there,\n",
        "    not necessarily useful, but not worth nothing either. Situations in which 2 words are separated by , or - for example,\n",
        "    would retain their form as individual words. Subbing with \"\" would cause them to effectively get concatenated, which\n",
        "    might result in some unknown token unnecessarily.\n",
        "    \"\"\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsnxUDy1q8Fb"
      },
      "outputs": [],
      "source": [
        "def clean_data(df):\n",
        "    sent = { 'comment': [] ,'label': [] }\n",
        "    for row in df.values:\n",
        "        assert type (row[1]) == str , f\"{row[1]}\"\n",
        "        sent['comment'].append(clean_text(row[1]) )\n",
        "        sent['label'].append(row[0])\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hfi9bHiq8Fb"
      },
      "outputs": [],
      "source": [
        "train_dict = clean_data(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcuZJS7-q8Fb"
      },
      "outputs": [],
      "source": [
        "training_examples= pd.DataFrame(train_dict, columns=['comment', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA5ATNHgq8Fb",
        "outputId": "004103a4-94c8-404e-a8af-ce932203ee42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             comment  label\n",
              "0  thirtysomething scientists unveil doomsday clo...      1\n",
              "1  dem rep  totally nails why congress is falling...      0\n",
              "2  eat your veggies: 9 deliciously different recipes      0\n",
              "3  inclement weather prevents liar from getting t...      1\n",
              "4  mother comes pretty close to using word 'strea...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb87b940-f051-4e50-9b67-7088f6e2e91c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dem rep  totally nails why congress is falling...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb87b940-f051-4e50-9b67-7088f6e2e91c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb87b940-f051-4e50-9b67-7088f6e2e91c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb87b940-f051-4e50-9b67-7088f6e2e91c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "training_examples.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfnIhjNUq8Fb",
        "outputId": "5822b581-b6c4-41c0-df66-7d7dc05d2061"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28619, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "training_examples.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtuNdaU9q8Fc"
      },
      "source": [
        "## Data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvIlgURkq8Fc"
      },
      "outputs": [],
      "source": [
        "#use only a fraction of the training data:\n",
        "train_df, test_df = train_test_split(training_examples, train_size= 0.8, stratify = training_examples.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL3W_pONq8Fc"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(train_df, train_size= 0.75, stratify = train_df.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGl-9gzMq8Fc",
        "outputId": "e5bfbbbc-35a6-473c-e993-8cf1f30ecd34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17171, 5724, 5724)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#SPLIT RATIO:\n",
        "#train : 60%, test: 20%, val: 20%\n",
        "len(train_df), len(test_df) , len(val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AQQj71-q8Fc"
      },
      "source": [
        "## Tokenization, embeddings and Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIxyiXyRtQAc",
        "outputId": "efeb8f74-5ac7-49ec-dc9d-e88f12d6ab74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6SY4W25q8Fd"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KeVPP49q8Fi"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "5b2b5d9155204ae6b1344b2341b6f623",
            "18026e903e764ad1b4fb625eadda641e",
            "87df88b8775a4405be41b879dfa47480",
            "a9a786df8e2f4548a3b474a11a0f1f99",
            "2557296f57a243e2ad2fb672aa0ceb66",
            "1d134ede89774f108a18df74d0ad4ea8",
            "0b3fff64d0254888ba5e85fac07758a6",
            "c0fe8afb716543d3a1b05cb80cccf645",
            "9a2023d6cca84ca8a2d556b6a8e62e27",
            "8f9f752d4c7f4d3e8377c5cf95832704",
            "8bc9a56fac744b19a35de6b20a6fdcd3",
            "1437cbf59fbe4293987c3891a06e82b2",
            "7ef46eefed6b40809ac7400b7966d4fd",
            "dd497e9863a84a13b846a6416fe20e27",
            "28b4dcb1aca549c0b99c61c2d46eeed5",
            "3950f1e69ca642eea38407cd4cd6df60",
            "401dddf58e634617b22a5e050288f4ef",
            "68140a16ba084d2d8dcac19280519382",
            "196a3d5d57fd45918173749e32ddac70",
            "5a7832b284344c0e92b3372f964c15cf",
            "9bcd9292e2ce4140ba14bee9a9998a14",
            "f2d6d8a9731e4161bb2da99ab7623fb9",
            "10e52e18a3754354a9541be8a5019fa8",
            "c2978a7ad33442429fc9344405a6c5f8",
            "7b7052eb10504ec88bdb8a3eda51aaf9",
            "6c77e47405f24b9ea21b288a4aee06e1",
            "443b9ec2fab740efa025b56c346cfff8",
            "0a51c8347b63499cbbbf767c54732d0c",
            "c006b2c659e9480194520fbb5a6f443c",
            "46c2917ddf564299820e6c258345df5e",
            "a03bc90aa5ad4685b2bee138f51f6bf8",
            "4f82ce3bc30e41c0a19e4bafd4b19a98",
            "9d5b4c6088a94e23bdd4aca71d325c05"
          ]
        },
        "id": "JzpPpjRRq8Fi",
        "outputId": "79b2fa45-dfdc-4eff-9ad4-d62b3acad08f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b2b5d9155204ae6b1344b2341b6f623"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1437cbf59fbe4293987c3891a06e82b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10e52e18a3754354a9541be8a5019fa8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_tokenizer= BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-8PZCFzq8Fi"
      },
      "outputs": [],
      "source": [
        "class dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length, isbert=False):\n",
        "        self.text= df['comment'].astype(str).tolist()\n",
        "        self.tokenizer= tokenizer\n",
        "        self.labels= df.label.values\n",
        "        self.max_length = max_length\n",
        "        self.isbert= isbert\n",
        "    def __len__(self):\n",
        "        return self.text.__len__()\n",
        "    def __getitem__(self,index):\n",
        "        \"\"\"\n",
        "        FOR BERT:\n",
        "        #start with CLS_TOKEN\n",
        "        tokenized_text= [CLS_TOKEN]\n",
        "        tokenized_text.append (self.tokenizer(self.text[index]) )\n",
        "        #add SEP_TOKEN\n",
        "        tokenized_text.append(SEP_TOKEN)\n",
        "        \"\"\"\n",
        "        if not self.isbert:\n",
        "            tokenized_text= self.tokenizer(self.text[index])\n",
        "            #add padding using PAD_TOKEN\n",
        "\n",
        "            length_diff= self.max_length-tokenized_text.__len__()\n",
        "\n",
        "            if length_diff >= 0 :\n",
        "                tokenized_text.extend([PAD_TOKEN]*length_diff)\n",
        "            else: #truncate if length_diff is negative, i.e, tokenized_text should not be longer than max length\n",
        "                tokenized_text = tokenized_text[:length_diff]\n",
        "\n",
        "            #ensure all token lists are of equal length\n",
        "            assert len(tokenized_text)== self.max_length, f\"text:{tokenized_text}\\nlendif:{length_diff}\\nlength of tokenized text({len(tokenized_text)}) != {self.max_length} after padding\"\n",
        "\n",
        "            #For GloVe embeddings:\n",
        "            vecs= glove_emb.get_vecs_by_tokens(tokenized_text)\n",
        "            return {'text': tokenized_text,\n",
        "                    'embedding': vecs,\n",
        "                   'label':self.labels[index],\n",
        "                   'length': len(tokenized_text)}\n",
        "        else:\n",
        "            text = self.text[index]\n",
        "            tokenized_text= self.tokenizer.encode_plus (text,\n",
        "                                                       None,\n",
        "                                                       max_length= self.max_length,\n",
        "                                                       padding= 'max_length',\n",
        "                                                       truncation= 'longest_first',\n",
        "                                                       return_tensors ='pt',\n",
        "                                                       return_token_type_ids= True,\n",
        "                                                       return_attention_mask = True)\n",
        "            return {'input_ids':tokenized_text['input_ids'].flatten(),\n",
        "                   'attention_mask':tokenized_text['attention_mask'].flatten(),\n",
        "                   'token_type_ids':tokenized_text['token_type_ids'].flatten(),\n",
        "                   'targets':self.labels[index]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGQlu2_fq8Fj"
      },
      "source": [
        "### Create dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVkCvbWSq8Fj"
      },
      "outputs": [],
      "source": [
        "#BERT DATASETS\n",
        "bert_train_ds= dataset(train_df, bert_tokenizer, max_length=MAX_LENGTH, isbert= True)\n",
        "bert_val_ds  = dataset(val_df, bert_tokenizer, max_length=MAX_LENGTH, isbert= True)\n",
        "bert_test_ds = dataset(test_df, bert_tokenizer, max_length=MAX_LENGTH, isbert= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYJ7x4q3q8Fk",
        "outputId": "f7c99670-5876-4526-89bf-860f3e4db1cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17171, 5724, 5724)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "len(bert_train_ds), len(bert_val_ds), len(bert_test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkjWaMp9q8Fl"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0WG8Kebq8Fl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0WMqlNxq8Fl"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE= 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkq-gC6Rq8Fl"
      },
      "outputs": [],
      "source": [
        "bert_trainLoader= DataLoader (bert_train_ds,\n",
        "                        batch_size= BATCH_SIZE,\n",
        "                        shuffle= True)\n",
        "bert_valLoader= DataLoader(bert_val_ds,\n",
        "                     batch_size= BATCH_SIZE)\n",
        "bert_testLoader= DataLoader(bert_test_ds,\n",
        "                      batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UC5PKbrq8Fm"
      },
      "source": [
        "## Model Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zhZ-FYMq8Fm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWl9Ko9iq8Fm"
      },
      "outputs": [],
      "source": [
        "BERT_BUPO_ATTEMPT= {1: {'count': 1, 'comment':\"Dropout 0.3, no reg\"},\n",
        "                    2: {'count': 2, 'comment': 'Dropout 0.1, no reg'},\n",
        "                    }\n",
        "BERT_SOL_ATTEMPT= {1: {'count': 1, 'comment': 'Dropout 0.3, no reg'},\n",
        "                   2: {'count': 2, 'comment': 'Dropout 0.1, no reg'},\n",
        "                   3: {'count': 3, 'comment': 'Dropout 0'}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_MNR6mBq8Fm"
      },
      "outputs": [],
      "source": [
        "bert_bupo_attempt= BERT_BUPO_ATTEMPT[2]['count']\n",
        "bert_sol_attempt= BERT_SOL_ATTEMPT[2]['count']\n",
        "base_path= f\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/headlines_checkpoints/models_{MAX_LENGTH}/\"\n",
        "\n",
        "# ATTEMPT- 1 : Because I implemented LSTM AND RNN models first\n",
        "# UPDATE {ATTEMPT -1 } to #{ATTEMPT} after finishing the first attempt for BERT\n",
        "# UPDATE: Using a separate value holder for BERT attempts\n",
        "bert_bupo_ckpt= base_path+f\"BERT/bert_bupo_ckpt_{MAX_LENGTH}_{bert_bupo_attempt}\"\n",
        "bert_bupo_best= base_path+f\"BERT/bert_bupo_best_{MAX_LENGTH}_{bert_bupo_attempt}.pt\"\n",
        "\n",
        "bert_sol_ckpt= base_path+f\"BERT/bert_sol_ckpt_{MAX_LENGTH}_{bert_sol_attempt}\"\n",
        "bert_sol_best= base_path+f\"BERT/bert_sol_best_{MAX_LENGTH}_{bert_sol_attempt}.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Dbd4_Mq8Fn"
      },
      "outputs": [],
      "source": [
        "#LR = 0.005\n",
        "BERT_EPOCHS = 4\n",
        "DROPOUT_RATE = 0.1\n",
        "bupo_LR = 0.005\n",
        "sol_LR = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Models"
      ],
      "metadata": {
        "id": "wme9xd0It7Ok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUfcl-mBq8Fn"
      },
      "outputs": [],
      "source": [
        "#bert= BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uAw3eUJq8Fo"
      },
      "outputs": [],
      "source": [
        "bert_address= \"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/headlines_checkpoints/MODEL_BASE_STATE/bert.pt\"\n",
        "#torch.save({'state_dict': bert.state_dict()}, bert_address)\n",
        "checkpt = torch.load(bert_address)\n",
        "#use same initial weights for all models\n",
        "bert = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True, state_dict = checkpt['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_waRqvWyq8Fo"
      },
      "outputs": [],
      "source": [
        "class BERT_BUPO(nn.Module):\n",
        "    def __init__(self, bert, dropout_rate):\n",
        "        super(BERT_BUPO, self).__init__()\n",
        "        self.base_model = bert;\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear= nn.Linear(768,1)\n",
        "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
        "        bert_op = self.base_model(input_ids,attention_mask,token_type_ids)\n",
        "\n",
        "        pooler_op= bert_op.pooler_output\n",
        "\n",
        "        dropout_op= self.dropout(pooler_op)\n",
        "\n",
        "        lin = self.linear(dropout_op)\n",
        "\n",
        "        return lin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG80B4SAq8Fo"
      },
      "outputs": [],
      "source": [
        "class BERT_SOL(nn.Module):\n",
        "    def __init__(self, bert, dropout_rate):\n",
        "        super(BERT_SOL, self).__init__()\n",
        "        self.base_model = bert;\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear= nn.Linear(768,1)\n",
        "        self.batch_norm= nn.BatchNorm1d(768)\n",
        "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
        "        bert_op = self.base_model(input_ids,attention_mask,token_type_ids)\n",
        "        hidden_states= bert_op.hidden_states\n",
        "\n",
        "        last_4_layers= torch.stack(hidden_states[-4:]) # shape : (4, BATCH_SIZE, MAX_LENGTH, 768)\n",
        "        sum_of_last_4_layers= last_4_layers.sum(0) #shape: (BATCH_SIZE,MAX_LENGTH,768)\n",
        "\n",
        "\n",
        "        batch_norm_op= self.batch_norm(sum_of_last_4_layers.permute(0,2,1)) # shape : (BATCH_SIZE,768, MAX_LENGTH)\n",
        "        # permute because nn.BatchNorm1d uses learnable parameters of size feature_size, which is 768 in this case\n",
        "        #The object expects a shape (N,C,L): N-> BATCH_SIZE, C->NUMBER OF FEATURES, L-> SEQUENCE_LENGTH (so MAX_LENGTH here)\n",
        "\n",
        "        perm_batch_op = batch_norm_op.permute(0,2,1) # shape : (BATCH_SIZE, MAX_LENGTH, 768)\n",
        "        mean_embed = perm_batch_op.mean(1) #shape : (BATCH_SIZE, 768)\n",
        "\n",
        "        lin = self.linear(mean_embed) #shape [BATCH_sIZE]\n",
        "        return lin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnDc3Mceq8Fo"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqAVkwSkq8Fo"
      },
      "source": [
        "### Loss function, optimizer object, f1_measure, save and load function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-LyDt0hq8Fo"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    #fn = nn.BCEWithLogitsLoss()\n",
        "    #op  =outputs.flatten()\n",
        "\n",
        "    assert outputs.flatten().shape == targets.shape , f\" output and targets vector should have same shape\\nop:{op.shape}\\ntarget:{targets.shape}\"\n",
        "    assert outputs.dtype == targets.dtype, f\" output and targets should have same type:\\noutput: {outputs.dtype}\\ntargets: {targets.dtype}\"\n",
        "\n",
        "    return nn.BCEWithLogitsLoss()(outputs.flatten(),targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaFDAphNq8Fp"
      },
      "outputs": [],
      "source": [
        "def calc_avg_loss(dt, index):\n",
        "    dt['avg'] =  (dt['avg']*dt['len'] + dt[index][-1])/ (dt['len']+1)\n",
        "    dt['len']+=1\n",
        "    return dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLTtRnCTq8Fp"
      },
      "outputs": [],
      "source": [
        "def optimizer_fn(model, learn_rate,weight_decay=0.0):\n",
        "  print(\"weight_decay: \", weight_decay)\n",
        "  print(\"learning_rate: \", learn_rate)\n",
        "  return torch.optim.Adam(params=model.parameters(), lr=learn_rate, weight_decay=weight_decay )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLE5KWccq8Fp"
      },
      "outputs": [],
      "source": [
        "def get_f1(outputs, targets):\n",
        "    op = (outputs.cpu().flatten().detach().numpy() > 0.5 )\n",
        "    #print(outputs,\"\\n\", op)\n",
        "    targ= (targets.cpu().numpy() == 1.)\n",
        "    assert op.shape == targ.shape, f\"shape mismatch\\n op: {op.shape}\\ntarg: {targ.shape}\"\n",
        "    return f1_score(y_pred=op, y_true= targ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKK16C-yq8Fp"
      },
      "source": [
        "#### Load and Save pytorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmupAtIlq8Fp"
      },
      "outputs": [],
      "source": [
        "def load_ckp(ckpt_path,model,optimizer=None):\n",
        "    checkpoint= torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    if optimizer!=None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    valid_loss_min= checkpoint['valid_loss_min']\n",
        "    return model, optimizer, checkpoint['epoch'],valid_loss_min.item()\n",
        "\n",
        "\n",
        "def save_ckp(state, is_best, ckpt_path,best_model_path):\n",
        "    torch.save(state, ckpt_path)\n",
        "    if is_best:\n",
        "        shutil.copyfile(ckpt_path,best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBFOad5Iq8Fp"
      },
      "source": [
        "#### Export and Load dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueIQdz90q8Fp"
      },
      "outputs": [],
      "source": [
        "def export(dt, filepath):\n",
        "    file= open(filepath,\"wb\")\n",
        "    pickle.dump(dt, file)\n",
        "    file.close()\n",
        "def import_dict(filepath):\n",
        "    file = open(filepath,\"rb\")\n",
        "    dt= pickle.load(file)\n",
        "    file.close()\n",
        "    return dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRiIWNU-q8Fq"
      },
      "source": [
        "### Initialize Model objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Gz-T_eq8Fr"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSbpYHDRq8Fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcd2d7a-ef16-4b6b-f16e-5a105b6b4e24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "bert_bupo= BERT_BUPO(bert,DROPOUT_RATE)\n",
        "bert_bupo.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/headlines_checkpoints/MODEL_BASE_STATE/bupo\"), strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVE BASE STATE\n",
        "#torch.save(bert_bupo.state_dict(),\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/headlines_checkpoints/MODEL_BASE_STATE/bupo\")"
      ],
      "metadata": {
        "id": "KuDsP-ARAACV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_bupo.to(device);"
      ],
      "metadata": {
        "id": "Q-2b46dBhrk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouYhM2sTq8Fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a69900d-8702-4256-d8ec-1e9be89c3ab2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "bert_sol= BERT_SOL(bert,DROPOUT_RATE)\n",
        "bert_sol.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/headlines_checkpoints/MODEL_BASE_STATE/sol\"), strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(bert_sol.state_dict(),\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/Headlines/headlines_checkpoints/MODEL_BASE_STATE/sol\")"
      ],
      "metadata": {
        "id": "f3O86TQCAa_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_sol.to(device);"
      ],
      "metadata": {
        "id": "8HknY_Drh6N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu4RSAQ9q8Fr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training function for BERT"
      ],
      "metadata": {
        "id": "OmAYy_zehdvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_train(epochs,model,trainLoader,valLoader,optimizer,ckpt_path,best_path, attempt, save=False):\n",
        "    model.train()\n",
        "    #print(model.state_dict)\n",
        "\n",
        "    val_loss_min=np.Inf\n",
        "\n",
        "    train_loss_list= dict()\n",
        "    val_loss_list =dict()\n",
        "    f1_train_avg = dict()\n",
        "    f1_val_avg = dict()\n",
        "\n",
        "    f1_train_avg['len']= 0\n",
        "    f1_val_avg['len'] = 0\n",
        "    train_loss_list['len']=0\n",
        "    val_loss_list['len'] = 0\n",
        "\n",
        "    f1_train_avg['avg']= 0.0\n",
        "    f1_val_avg['avg'] = 0.0\n",
        "    train_loss_list['avg']=0.0\n",
        "    val_loss_list['avg'] = 0.0\n",
        "\n",
        "    for epoch in range(1,epochs+1):\n",
        "\n",
        "        print (\"EPOCH\", epoch)\n",
        "        print(\"=\"*20)\n",
        "        f1_train_avg[epoch]= []\n",
        "        f1_val_avg[epoch] = []\n",
        "        train_loss_list[epoch]=[]\n",
        "        val_loss_list[epoch] = []\n",
        "\n",
        "        for index, batch in enumerate(trainLoader):\n",
        "          print(f\"BATCH: {index+1}/{len(trainLoader)}\")\n",
        "          input_ids = batch['input_ids'].to(device, dtype= torch.long)\n",
        "          attention_mask = batch['attention_mask'].to(device, dtype= torch.long)\n",
        "          token_type_ids = batch['token_type_ids'].to(device, dtype= torch.long)\n",
        "          targets = batch['targets'].to(device, dtype= torch.float)\n",
        "          output= model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss = loss_fn(output, targets)\n",
        "          loss.backward()\n",
        "\n",
        "          sigmoid_op = nn.Sigmoid()(output)\n",
        "          f1= get_f1 (sigmoid_op, targets)\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          print(f\" train loss item: {loss.item()}\")\n",
        "\n",
        "          #train_loss=train_loss + ((1/(index+1))*(loss.item()-train_loss))\n",
        "\n",
        "          train_loss_list[epoch].append(loss.item())\n",
        "          train_loss_list= calc_avg_loss(train_loss_list,epoch)\n",
        "\n",
        "          #f1_avg=f1_avg + ((1/(index+1))*(f1-f1_avg))\n",
        "\n",
        "          f1_train_avg[epoch].append(f1)\n",
        "          f1_train_avg = calc_avg_loss(f1_train_avg, epoch)\n",
        "\n",
        "          print(\" f1:{:.8f}\\n avg_train_loss: {:.8f} \".format(f1_train_avg['avg'], train_loss_list['avg']))\n",
        "        print(\"\\n\\n\")\n",
        "        print(f\"VALIDATION FOR EPOCH {epoch}\")\n",
        "        #VALIDATION\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for index, batch in enumerate(valLoader):\n",
        "              print(f\"BATCH: {index+1}/{len(valLoader)}\")\n",
        "              input_ids = batch['input_ids'].to(device, dtype= torch.long)\n",
        "              attention_mask = batch['attention_mask'].to(device, dtype= torch.long)\n",
        "              token_type_ids = batch['token_type_ids'].to(device, dtype= torch.long)\n",
        "              targets = batch['targets'].to(device, dtype= torch.float)\n",
        "              output= model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "              loss= loss_fn(output, targets)\n",
        "              print(f\"val loss item: {loss.item()}\")\n",
        "              val_loss_list[epoch].append(loss.item())\n",
        "              val_loss_list = calc_avg_loss(val_loss_list,epoch)\n",
        "\n",
        "              sigmoid_op= nn.Sigmoid()(output)\n",
        "              f1= get_f1(sigmoid_op,targets)\n",
        "              f1_val_avg[epoch].append(f1)\n",
        "              f1_val_avg = calc_avg_loss(f1_val_avg, epoch)\n",
        "\n",
        "              print(\" f1_val:{:.8f}\\n avg_val_loss: {:.8f} \".format(f1_val_avg['avg'], val_loss_list['avg']))\n",
        "\n",
        "            checkpoint= {'epoch': epoch,\n",
        "                        'val_loss_min':val_loss_list['avg'],\n",
        "                        'optimizer':optimizer.state_dict(),\n",
        "                        'model_state_dict': model.state_dict()\n",
        "                           }\n",
        "            if save:\n",
        "              save_ckp(checkpoint,False,ckpt_path,best_path)\n",
        "\n",
        "              if val_loss_list['avg'] < val_loss_min:\n",
        "                save_ckp(checkpoint, True,ckpt_path, best_path)\n",
        "                val_loss_min = loss.item()\n",
        "                print(\"SAVED\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return {'train_loss':train_loss_list,\n",
        "            'val_loss':val_loss_list,\n",
        "            'f1_train':f1_train_avg,\n",
        "            'f1_val':f1_val_avg,\n",
        "            'EPOCHS': epochs,\n",
        "            'ATTEMPT': attempt,\n",
        "            'weight_decay': optimizer.param_groups[0]['weight_decay'],\n",
        "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
        "           'dropout': model.dropout,\n",
        "           'train_batch_size': trainLoader.batch_size,\n",
        "           'val_batch_size': valLoader.batch_size} , model.state_dict()"
      ],
      "metadata": {
        "id": "N02D8xCy0Mg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT Training"
      ],
      "metadata": {
        "id": "tWNTX4dvhOYu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVjFjRGWq8Fs",
        "outputId": "c5d3a3bc-6385-4852-837a-44b6d006c0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " f1:0.29906812\n",
            " avg_train_loss: 0.77677349 \n",
            "BATCH: 185/1074\n",
            " train loss item: 0.7925060391426086\n",
            " f1:0.29898034\n",
            " avg_train_loss: 0.77677810 \n",
            "BATCH: 186/1074\n",
            " train loss item: 0.7851098775863647\n",
            " f1:0.29907122\n",
            " avg_train_loss: 0.77678055 \n",
            "BATCH: 187/1074\n",
            " train loss item: 0.8948736190795898\n",
            " f1:0.29917905\n",
            " avg_train_loss: 0.77681519 \n",
            "BATCH: 188/1074\n",
            " train loss item: 1.0088121891021729\n",
            " f1:0.29926982\n",
            " avg_train_loss: 0.77688322 \n",
            "BATCH: 189/1074\n",
            " train loss item: 0.8827359080314636\n",
            " f1:0.29934199\n",
            " avg_train_loss: 0.77691426 \n",
            "BATCH: 190/1074\n",
            " train loss item: 0.7309324741363525\n",
            " f1:0.29925426\n",
            " avg_train_loss: 0.77690078 \n",
            "BATCH: 191/1074\n",
            " train loss item: 0.9547647833824158\n",
            " f1:0.29916658\n",
            " avg_train_loss: 0.77695289 \n",
            "BATCH: 192/1074\n",
            " train loss item: 0.8127958178520203\n",
            " f1:0.29907895\n",
            " avg_train_loss: 0.77696339 \n",
            "BATCH: 193/1074\n",
            " train loss item: 0.755715548992157\n",
            " f1:0.29899137\n",
            " avg_train_loss: 0.77695717 \n",
            "BATCH: 194/1074\n",
            " train loss item: 0.7715630531311035\n",
            " f1:0.29909900\n",
            " avg_train_loss: 0.77695559 \n",
            "BATCH: 195/1074\n",
            " train loss item: 1.0046076774597168\n",
            " f1:0.29918961\n",
            " avg_train_loss: 0.77702222 \n",
            "BATCH: 196/1074\n",
            " train loss item: 0.8851357698440552\n",
            " f1:0.29929712\n",
            " avg_train_loss: 0.77705385 \n",
            "BATCH: 197/1074\n",
            " train loss item: 0.8777531385421753\n",
            " f1:0.29934886\n",
            " avg_train_loss: 0.77708330 \n",
            "BATCH: 198/1074\n",
            " train loss item: 0.7807384729385376\n",
            " f1:0.29926133\n",
            " avg_train_loss: 0.77708437 \n",
            "BATCH: 199/1074\n",
            " train loss item: 0.6987764835357666\n",
            " f1:0.29917385\n",
            " avg_train_loss: 0.77706148 \n",
            "BATCH: 200/1074\n",
            " train loss item: 0.6611398458480835\n",
            " f1:0.29908643\n",
            " avg_train_loss: 0.77702760 \n",
            "BATCH: 201/1074\n",
            " train loss item: 0.9578995704650879\n",
            " f1:0.29899905\n",
            " avg_train_loss: 0.77708044 \n",
            "BATCH: 202/1074\n",
            " train loss item: 0.4977407455444336\n",
            " f1:0.29891173\n",
            " avg_train_loss: 0.77699886 \n",
            "BATCH: 203/1074\n",
            " train loss item: 0.6213881373405457\n",
            " f1:0.29882445\n",
            " avg_train_loss: 0.77695343 \n",
            "BATCH: 204/1074\n",
            " train loss item: 0.6616557836532593\n",
            " f1:0.29873723\n",
            " avg_train_loss: 0.77691977 \n",
            "BATCH: 205/1074\n",
            " train loss item: 0.735348105430603\n",
            " f1:0.29865006\n",
            " avg_train_loss: 0.77690764 \n",
            "BATCH: 206/1074\n",
            " train loss item: 0.7185057997703552\n",
            " f1:0.29874050\n",
            " avg_train_loss: 0.77689061 \n",
            "BATCH: 207/1074\n",
            " train loss item: 0.7909630537033081\n",
            " f1:0.29881245\n",
            " avg_train_loss: 0.77689471 \n",
            "BATCH: 208/1074\n",
            " train loss item: 0.7231757640838623\n",
            " f1:0.29890280\n",
            " avg_train_loss: 0.77687905 \n",
            "BATCH: 209/1074\n",
            " train loss item: 0.704044759273529\n",
            " f1:0.29881568\n",
            " avg_train_loss: 0.77685782 \n",
            "BATCH: 210/1074\n",
            " train loss item: 0.6642606258392334\n",
            " f1:0.29872861\n",
            " avg_train_loss: 0.77682501 \n",
            "BATCH: 211/1074\n",
            " train loss item: 0.7411137819290161\n",
            " f1:0.29864160\n",
            " avg_train_loss: 0.77681461 \n",
            "BATCH: 212/1074\n",
            " train loss item: 0.7367861270904541\n",
            " f1:0.29855463\n",
            " avg_train_loss: 0.77680295 \n",
            "BATCH: 213/1074\n",
            " train loss item: 0.6857846975326538\n",
            " f1:0.29846771\n",
            " avg_train_loss: 0.77677646 \n",
            "BATCH: 214/1074\n",
            " train loss item: 0.6912468075752258\n",
            " f1:0.29838085\n",
            " avg_train_loss: 0.77675156 \n",
            "BATCH: 215/1074\n",
            " train loss item: 0.7028188705444336\n",
            " f1:0.29847114\n",
            " avg_train_loss: 0.77673005 \n",
            "BATCH: 216/1074\n",
            " train loss item: 0.7005990743637085\n",
            " f1:0.29856137\n",
            " avg_train_loss: 0.77670791 \n",
            "BATCH: 217/1074\n",
            " train loss item: 0.6938953399658203\n",
            " f1:0.29847455\n",
            " avg_train_loss: 0.77668383 \n",
            "BATCH: 218/1074\n",
            " train loss item: 0.6729637384414673\n",
            " f1:0.29838779\n",
            " avg_train_loss: 0.77665368 \n",
            "BATCH: 219/1074\n",
            " train loss item: 0.7179304957389832\n",
            " f1:0.29830107\n",
            " avg_train_loss: 0.77663661 \n",
            "BATCH: 220/1074\n",
            " train loss item: 0.6617363691329956\n",
            " f1:0.29821441\n",
            " avg_train_loss: 0.77660323 \n",
            "BATCH: 221/1074\n",
            " train loss item: 0.7254769802093506\n",
            " f1:0.29812779\n",
            " avg_train_loss: 0.77658838 \n",
            "BATCH: 222/1074\n",
            " train loss item: 0.6859651803970337\n",
            " f1:0.29804123\n",
            " avg_train_loss: 0.77656207 \n",
            "BATCH: 223/1074\n",
            " train loss item: 0.7028148174285889\n",
            " f1:0.29795471\n",
            " avg_train_loss: 0.77654066 \n",
            "BATCH: 224/1074\n",
            " train loss item: 0.7132121324539185\n",
            " f1:0.29804489\n",
            " avg_train_loss: 0.77652228 \n",
            "BATCH: 225/1074\n",
            " train loss item: 0.7062638998031616\n",
            " f1:0.29815183\n",
            " avg_train_loss: 0.77650190 \n",
            "BATCH: 226/1074\n",
            " train loss item: 0.6853755116462708\n",
            " f1:0.29827417\n",
            " avg_train_loss: 0.77647547 \n",
            "BATCH: 227/1074\n",
            " train loss item: 0.6990746259689331\n",
            " f1:0.29838099\n",
            " avg_train_loss: 0.77645303 \n",
            "BATCH: 228/1074\n",
            " train loss item: 0.7026818990707397\n",
            " f1:0.29845260\n",
            " avg_train_loss: 0.77643165 \n",
            "BATCH: 229/1074\n",
            " train loss item: 0.6858149766921997\n",
            " f1:0.29836612\n",
            " avg_train_loss: 0.77640539 \n",
            "BATCH: 230/1074\n",
            " train loss item: 0.5839505195617676\n",
            " f1:0.29827969\n",
            " avg_train_loss: 0.77634964 \n",
            "BATCH: 231/1074\n",
            " train loss item: 0.8361145257949829\n",
            " f1:0.29819330\n",
            " avg_train_loss: 0.77636694 \n",
            "BATCH: 232/1074\n",
            " train loss item: 0.7620474100112915\n",
            " f1:0.29810697\n",
            " avg_train_loss: 0.77636280 \n",
            "BATCH: 233/1074\n",
            " train loss item: 0.6663271188735962\n",
            " f1:0.29802069\n",
            " avg_train_loss: 0.77633095 \n",
            "BATCH: 234/1074\n",
            " train loss item: 0.7253692150115967\n",
            " f1:0.29793445\n",
            " avg_train_loss: 0.77631620 \n",
            "BATCH: 235/1074\n",
            " train loss item: 0.7222071886062622\n",
            " f1:0.29802435\n",
            " avg_train_loss: 0.77630055 \n",
            "BATCH: 236/1074\n",
            " train loss item: 0.7376090288162231\n",
            " f1:0.29813095\n",
            " avg_train_loss: 0.77628936 \n",
            "BATCH: 237/1074\n",
            " train loss item: 0.6628791093826294\n",
            " f1:0.29826715\n",
            " avg_train_loss: 0.77625658 \n",
            "BATCH: 238/1074\n",
            " train loss item: 0.8093018531799316\n",
            " f1:0.29833859\n",
            " avg_train_loss: 0.77626613 \n",
            "BATCH: 239/1074\n",
            " train loss item: 0.7149291634559631\n",
            " f1:0.29836796\n",
            " avg_train_loss: 0.77624840 \n",
            "BATCH: 240/1074\n",
            " train loss item: 0.7282644510269165\n",
            " f1:0.29828178\n",
            " avg_train_loss: 0.77623454 \n",
            "BATCH: 241/1074\n",
            " train loss item: 0.8450530767440796\n",
            " f1:0.29819565\n",
            " avg_train_loss: 0.77625442 \n",
            "BATCH: 242/1074\n",
            " train loss item: 0.9656693935394287\n",
            " f1:0.29810956\n",
            " avg_train_loss: 0.77630910 \n",
            "BATCH: 243/1074\n",
            " train loss item: 1.022524356842041\n",
            " f1:0.29802353\n",
            " avg_train_loss: 0.77638016 \n",
            "BATCH: 244/1074\n",
            " train loss item: 0.705830454826355\n",
            " f1:0.29811316\n",
            " avg_train_loss: 0.77635980 \n",
            "BATCH: 245/1074\n",
            " train loss item: 0.960284948348999\n",
            " f1:0.29818450\n",
            " avg_train_loss: 0.77641285 \n",
            "BATCH: 246/1074\n",
            " train loss item: 0.86883944272995\n",
            " f1:0.29829075\n",
            " avg_train_loss: 0.77643950 \n",
            "BATCH: 247/1074\n",
            " train loss item: 0.6769723892211914\n",
            " f1:0.29842651\n",
            " avg_train_loss: 0.77641083 \n",
            "BATCH: 248/1074\n",
            " train loss item: 0.6891065835952759\n",
            " f1:0.29854800\n",
            " avg_train_loss: 0.77638567 \n",
            "BATCH: 249/1074\n",
            " train loss item: 0.6760645508766174\n",
            " f1:0.29846199\n",
            " avg_train_loss: 0.77635677 \n",
            "BATCH: 250/1074\n",
            " train loss item: 0.6906839609146118\n",
            " f1:0.29837603\n",
            " avg_train_loss: 0.77633209 \n",
            "BATCH: 251/1074\n",
            " train loss item: 0.9103590250015259\n",
            " f1:0.29829011\n",
            " avg_train_loss: 0.77637068 \n",
            "BATCH: 252/1074\n",
            " train loss item: 0.9401195049285889\n",
            " f1:0.29820425\n",
            " avg_train_loss: 0.77641782 \n",
            "BATCH: 253/1074\n",
            " train loss item: 0.7394052743911743\n",
            " f1:0.29811844\n",
            " avg_train_loss: 0.77640717 \n",
            "BATCH: 254/1074\n",
            " train loss item: 0.7063648104667664\n",
            " f1:0.29803267\n",
            " avg_train_loss: 0.77638702 \n",
            "BATCH: 255/1074\n",
            " train loss item: 0.6704916954040527\n",
            " f1:0.29818130\n",
            " avg_train_loss: 0.77635656 \n",
            "BATCH: 256/1074\n",
            " train loss item: 0.8449275493621826\n",
            " f1:0.29828725\n",
            " avg_train_loss: 0.77637628 \n",
            "BATCH: 257/1074\n",
            " train loss item: 0.5816571116447449\n",
            " f1:0.29844789\n",
            " avg_train_loss: 0.77632031 \n",
            "BATCH: 258/1074\n",
            " train loss item: 1.1785131692886353\n",
            " f1:0.29853704\n",
            " avg_train_loss: 0.77643588 \n",
            "BATCH: 259/1074\n",
            " train loss item: 1.1388553380966187\n",
            " f1:0.29858807\n",
            " avg_train_loss: 0.77653999 \n",
            "BATCH: 260/1074\n",
            " train loss item: 0.692940354347229\n",
            " f1:0.29850232\n",
            " avg_train_loss: 0.77651598 \n",
            "BATCH: 261/1074\n",
            " train loss item: 0.6466922760009766\n",
            " f1:0.29841662\n",
            " avg_train_loss: 0.77647871 \n",
            "BATCH: 262/1074\n",
            " train loss item: 0.9533692002296448\n",
            " f1:0.29833096\n",
            " avg_train_loss: 0.77652948 \n",
            "BATCH: 263/1074\n",
            " train loss item: 1.9036407470703125\n",
            " f1:0.29824536\n",
            " avg_train_loss: 0.77685290 \n",
            "BATCH: 264/1074\n",
            " train loss item: 0.9760466814041138\n",
            " f1:0.29815980\n",
            " avg_train_loss: 0.77691004 \n",
            "BATCH: 265/1074\n",
            " train loss item: 0.6220981478691101\n",
            " f1:0.29807430\n",
            " avg_train_loss: 0.77686564 \n",
            "BATCH: 266/1074\n",
            " train loss item: 0.6637722253799438\n",
            " f1:0.29820938\n",
            " avg_train_loss: 0.77683322 \n",
            "BATCH: 267/1074\n",
            " train loss item: 1.004973292350769\n",
            " f1:0.29829837\n",
            " avg_train_loss: 0.77689861 \n",
            "BATCH: 268/1074\n",
            " train loss item: 1.0054850578308105\n",
            " f1:0.29840392\n",
            " avg_train_loss: 0.77696411 \n",
            "BATCH: 269/1074\n",
            " train loss item: 0.7295796275138855\n",
            " f1:0.29853879\n",
            " avg_train_loss: 0.77695053 \n",
            "BATCH: 270/1074\n",
            " train loss item: 0.8402407169342041\n",
            " f1:0.29860950\n",
            " avg_train_loss: 0.77696866 \n",
            "BATCH: 271/1074\n",
            " train loss item: 0.6869285106658936\n",
            " f1:0.29852401\n",
            " avg_train_loss: 0.77694288 \n",
            "BATCH: 272/1074\n",
            " train loss item: 0.8042872548103333\n",
            " f1:0.29843857\n",
            " avg_train_loss: 0.77695071 \n",
            "BATCH: 273/1074\n",
            " train loss item: 0.8066092729568481\n",
            " f1:0.29835318\n",
            " avg_train_loss: 0.77695919 \n",
            "BATCH: 274/1074\n",
            " train loss item: 0.805798351764679\n",
            " f1:0.29826784\n",
            " avg_train_loss: 0.77696744 \n",
            "BATCH: 275/1074\n",
            " train loss item: 1.057032585144043\n",
            " f1:0.29818254\n",
            " avg_train_loss: 0.77704753 \n",
            "BATCH: 276/1074\n",
            " train loss item: 0.6995577812194824\n",
            " f1:0.29809730\n",
            " avg_train_loss: 0.77702538 \n",
            "BATCH: 277/1074\n",
            " train loss item: 0.9391244649887085\n",
            " f1:0.29814820\n",
            " avg_train_loss: 0.77707170 \n",
            "BATCH: 278/1074\n",
            " train loss item: 0.8446182608604431\n",
            " f1:0.29825349\n",
            " avg_train_loss: 0.77709100 \n",
            "BATCH: 279/1074\n",
            " train loss item: 0.7978554964065552\n",
            " f1:0.29835872\n",
            " avg_train_loss: 0.77709693 \n",
            "BATCH: 280/1074\n",
            " train loss item: 0.7101737260818481\n",
            " f1:0.29846389\n",
            " avg_train_loss: 0.77707782 \n",
            "BATCH: 281/1074\n",
            " train loss item: 0.7226450443267822\n",
            " f1:0.29837869\n",
            " avg_train_loss: 0.77706229 \n",
            "BATCH: 282/1074\n",
            " train loss item: 0.6640205979347229\n",
            " f1:0.29829354\n",
            " avg_train_loss: 0.77703002 \n",
            "BATCH: 283/1074\n",
            " train loss item: 0.6229561567306519\n",
            " f1:0.29820843\n",
            " avg_train_loss: 0.77698607 \n",
            "BATCH: 284/1074\n",
            " train loss item: 0.8400692939758301\n",
            " f1:0.29812337\n",
            " avg_train_loss: 0.77700406 \n",
            "BATCH: 285/1074\n",
            " train loss item: 0.8824043273925781\n",
            " f1:0.29803837\n",
            " avg_train_loss: 0.77703411 \n",
            "BATCH: 286/1074\n",
            " train loss item: 0.6920502185821533\n",
            " f1:0.29795341\n",
            " avg_train_loss: 0.77700989 \n",
            "BATCH: 287/1074\n",
            " train loss item: 0.43210065364837646\n",
            " f1:0.29815348\n",
            " avg_train_loss: 0.77691159 \n",
            "BATCH: 288/1074\n",
            " train loss item: 0.9417511224746704\n",
            " f1:0.29827366\n",
            " avg_train_loss: 0.77695856 \n",
            "BATCH: 289/1074\n",
            " train loss item: 1.707535982131958\n",
            " f1:0.29832433\n",
            " avg_train_loss: 0.77722360 \n",
            "BATCH: 290/1074\n",
            " train loss item: 1.3828611373901367\n",
            " f1:0.29837498\n",
            " avg_train_loss: 0.77739605 \n",
            "BATCH: 291/1074\n",
            " train loss item: 0.6453993916511536\n",
            " f1:0.29852199\n",
            " avg_train_loss: 0.77735848 \n",
            "BATCH: 292/1074\n",
            " train loss item: 0.7717468738555908\n",
            " f1:0.29843704\n",
            " avg_train_loss: 0.77735688 \n",
            "BATCH: 293/1074\n",
            " train loss item: 0.8536185622215271\n",
            " f1:0.29835213\n",
            " avg_train_loss: 0.77737858 \n",
            "BATCH: 294/1074\n",
            " train loss item: 1.415076494216919\n",
            " f1:0.29826728\n",
            " avg_train_loss: 0.77755995 \n",
            "BATCH: 295/1074\n",
            " train loss item: 0.6695019006729126\n",
            " f1:0.29818247\n",
            " avg_train_loss: 0.77752922 \n",
            "BATCH: 296/1074\n",
            " train loss item: 0.7176918983459473\n",
            " f1:0.29823307\n",
            " avg_train_loss: 0.77751221 \n",
            "BATCH: 297/1074\n",
            " train loss item: 0.7640875577926636\n",
            " f1:0.29832129\n",
            " avg_train_loss: 0.77750840 \n",
            "BATCH: 298/1074\n",
            " train loss item: 0.7354698181152344\n",
            " f1:0.29842594\n",
            " avg_train_loss: 0.77749646 \n",
            "BATCH: 299/1074\n",
            " train loss item: 0.7336938977241516\n",
            " f1:0.29851406\n",
            " avg_train_loss: 0.77748402 \n",
            "BATCH: 300/1074\n",
            " train loss item: 0.6941620707511902\n",
            " f1:0.29842930\n",
            " avg_train_loss: 0.77746036 \n",
            "BATCH: 301/1074\n",
            " train loss item: 0.6909959316253662\n",
            " f1:0.29834459\n",
            " avg_train_loss: 0.77743582 \n",
            "BATCH: 302/1074\n",
            " train loss item: 0.7558131814002991\n",
            " f1:0.29825993\n",
            " avg_train_loss: 0.77742968 \n",
            "BATCH: 303/1074\n",
            " train loss item: 0.7431817054748535\n",
            " f1:0.29817532\n",
            " avg_train_loss: 0.77741996 \n",
            "BATCH: 304/1074\n",
            " train loss item: 0.7227838039398193\n",
            " f1:0.29809075\n",
            " avg_train_loss: 0.77740447 \n",
            "BATCH: 305/1074\n",
            " train loss item: 0.7008107304573059\n",
            " f1:0.29819525\n",
            " avg_train_loss: 0.77738275 \n",
            "BATCH: 306/1074\n",
            " train loss item: 0.7421056032180786\n",
            " f1:0.29829970\n",
            " avg_train_loss: 0.77737275 \n",
            "BATCH: 307/1074\n",
            " train loss item: 0.6660265922546387\n",
            " f1:0.29843314\n",
            " avg_train_loss: 0.77734120 \n",
            "BATCH: 308/1074\n",
            " train loss item: 0.534722626209259\n",
            " f1:0.29860258\n",
            " avg_train_loss: 0.77727247 \n",
            "BATCH: 309/1074\n",
            " train loss item: 0.9184778928756714\n",
            " f1:0.29867249\n",
            " avg_train_loss: 0.77731246 \n",
            "BATCH: 310/1074\n",
            " train loss item: 0.7317232489585876\n",
            " f1:0.29877668\n",
            " avg_train_loss: 0.77729955 \n",
            "BATCH: 311/1074\n",
            " train loss item: 0.694873034954071\n",
            " f1:0.29869211\n",
            " avg_train_loss: 0.77727622 \n",
            "BATCH: 312/1074\n",
            " train loss item: 0.7972882986068726\n",
            " f1:0.29860759\n",
            " avg_train_loss: 0.77728189 \n",
            "BATCH: 313/1074\n",
            " train loss item: 0.6854224801063538\n",
            " f1:0.29852312\n",
            " avg_train_loss: 0.77725590 \n",
            "BATCH: 314/1074\n",
            " train loss item: 0.6705360412597656\n",
            " f1:0.29843870\n",
            " avg_train_loss: 0.77722572 \n",
            "BATCH: 315/1074\n",
            " train loss item: 0.7528097629547119\n",
            " f1:0.29835432\n",
            " avg_train_loss: 0.77721882 \n",
            "BATCH: 316/1074\n",
            " train loss item: 0.6935067772865295\n",
            " f1:0.29845842\n",
            " avg_train_loss: 0.77719516 \n",
            "BATCH: 317/1074\n",
            " train loss item: 0.66583251953125\n",
            " f1:0.29859145\n",
            " avg_train_loss: 0.77716369 \n",
            "BATCH: 318/1074\n",
            " train loss item: 0.6635603308677673\n",
            " f1:0.29872439\n",
            " avg_train_loss: 0.77713160 \n",
            "BATCH: 319/1074\n",
            " train loss item: 0.7865203619003296\n",
            " f1:0.29882830\n",
            " avg_train_loss: 0.77713425 \n",
            "BATCH: 320/1074\n",
            " train loss item: 0.9394543766975403\n",
            " f1:0.29885687\n",
            " avg_train_loss: 0.77718008 \n",
            "BATCH: 321/1074\n",
            " train loss item: 0.7057560682296753\n",
            " f1:0.29877252\n",
            " avg_train_loss: 0.77715992 \n",
            "BATCH: 322/1074\n",
            " train loss item: 0.6692426204681396\n",
            " f1:0.29868821\n",
            " avg_train_loss: 0.77712947 \n",
            "BATCH: 323/1074\n",
            " train loss item: 0.7927314639091492\n",
            " f1:0.29860396\n",
            " avg_train_loss: 0.77713387 \n",
            "BATCH: 324/1074\n",
            " train loss item: 0.8793536424636841\n",
            " f1:0.29851975\n",
            " avg_train_loss: 0.77716269 \n",
            "BATCH: 325/1074\n",
            " train loss item: 0.7213640213012695\n",
            " f1:0.29843559\n",
            " avg_train_loss: 0.77714696 \n",
            "BATCH: 326/1074\n",
            " train loss item: 0.7069564461708069\n",
            " f1:0.29835147\n",
            " avg_train_loss: 0.77712718 \n",
            "BATCH: 327/1074\n",
            " train loss item: 0.9202369451522827\n",
            " f1:0.29835639\n",
            " avg_train_loss: 0.77716750 \n",
            "BATCH: 328/1074\n",
            " train loss item: 0.6910194754600525\n",
            " f1:0.29847516\n",
            " avg_train_loss: 0.77714324 \n",
            "BATCH: 329/1074\n",
            " train loss item: 0.735240638256073\n",
            " f1:0.29854471\n",
            " avg_train_loss: 0.77713144 \n",
            "BATCH: 330/1074\n",
            " train loss item: 0.7167360782623291\n",
            " f1:0.29846066\n",
            " avg_train_loss: 0.77711443 \n",
            "BATCH: 331/1074\n",
            " train loss item: 0.7237693667411804\n",
            " f1:0.29837666\n",
            " avg_train_loss: 0.77709942 \n",
            "BATCH: 332/1074\n",
            " train loss item: 0.5353912115097046\n",
            " f1:0.29829270\n",
            " avg_train_loss: 0.77703141 \n",
            "BATCH: 333/1074\n",
            " train loss item: 0.8622913360595703\n",
            " f1:0.29820880\n",
            " avg_train_loss: 0.77705539 \n",
            "BATCH: 334/1074\n",
            " train loss item: 0.728748083114624\n",
            " f1:0.29812493\n",
            " avg_train_loss: 0.77704181 \n",
            "BATCH: 335/1074\n",
            " train loss item: 0.6944162249565125\n",
            " f1:0.29804112\n",
            " avg_train_loss: 0.77701858 \n",
            "BATCH: 336/1074\n",
            " train loss item: 0.6877694129943848\n",
            " f1:0.29795735\n",
            " avg_train_loss: 0.77699349 \n",
            "BATCH: 337/1074\n",
            " train loss item: 0.710824728012085\n",
            " f1:0.29804467\n",
            " avg_train_loss: 0.77697490 \n",
            "BATCH: 338/1074\n",
            " train loss item: 0.7387140989303589\n",
            " f1:0.29811416\n",
            " avg_train_loss: 0.77696415 \n",
            "BATCH: 339/1074\n",
            " train loss item: 0.6957835555076599\n",
            " f1:0.29816417\n",
            " avg_train_loss: 0.77694136 \n",
            "BATCH: 340/1074\n",
            " train loss item: 0.697264552116394\n",
            " f1:0.29808046\n",
            " avg_train_loss: 0.77691899 \n",
            "BATCH: 341/1074\n",
            " train loss item: 0.9223873019218445\n",
            " f1:0.29799680\n",
            " avg_train_loss: 0.77695982 \n",
            "BATCH: 342/1074\n",
            " train loss item: 0.9689449667930603\n",
            " f1:0.29791319\n",
            " avg_train_loss: 0.77701368 \n",
            "BATCH: 343/1074\n",
            " train loss item: 0.7113712430000305\n",
            " f1:0.29801663\n",
            " avg_train_loss: 0.77699527 \n",
            "BATCH: 344/1074\n",
            " train loss item: 0.7113158106803894\n",
            " f1:0.29814877\n",
            " avg_train_loss: 0.77697685 \n",
            "BATCH: 345/1074\n",
            " train loss item: 1.0044856071472168\n",
            " f1:0.29825208\n",
            " avg_train_loss: 0.77704063 \n",
            "BATCH: 346/1074\n",
            " train loss item: 0.8551983833312988\n",
            " f1:0.29837028\n",
            " avg_train_loss: 0.77706254 \n",
            "BATCH: 347/1074\n",
            " train loss item: 0.6221998333930969\n",
            " f1:0.29851499\n",
            " avg_train_loss: 0.77701915 \n",
            "BATCH: 348/1074\n",
            " train loss item: 0.7445275783538818\n",
            " f1:0.29858416\n",
            " avg_train_loss: 0.77701005 \n",
            "BATCH: 349/1074\n",
            " train loss item: 0.6970627307891846\n",
            " f1:0.29850054\n",
            " avg_train_loss: 0.77698766 \n",
            "BATCH: 350/1074\n",
            " train loss item: 1.0977232456207275\n",
            " f1:0.29841698\n",
            " avg_train_loss: 0.77707745 \n",
            "BATCH: 351/1074\n",
            " train loss item: 0.8770982027053833\n",
            " f1:0.29833346\n",
            " avg_train_loss: 0.77710544 \n",
            "BATCH: 352/1074\n",
            " train loss item: 0.7222191095352173\n",
            " f1:0.29824998\n",
            " avg_train_loss: 0.77709009 \n",
            "BATCH: 353/1074\n",
            " train loss item: 0.7352188229560852\n",
            " f1:0.29835304\n",
            " avg_train_loss: 0.77707838 \n",
            "BATCH: 354/1074\n",
            " train loss item: 0.9139516353607178\n",
            " f1:0.29843982\n",
            " avg_train_loss: 0.77711665 \n",
            "BATCH: 355/1074\n",
            " train loss item: 0.8918848037719727\n",
            " f1:0.29852656\n",
            " avg_train_loss: 0.77714874 \n",
            "BATCH: 356/1074\n",
            " train loss item: 0.7187313437461853\n",
            " f1:0.29862945\n",
            " avg_train_loss: 0.77713241 \n",
            "BATCH: 357/1074\n",
            " train loss item: 0.6680105924606323\n",
            " f1:0.29854601\n",
            " avg_train_loss: 0.77710192 \n",
            "BATCH: 358/1074\n",
            " train loss item: 0.6274297833442688\n",
            " f1:0.29846262\n",
            " avg_train_loss: 0.77706011 \n",
            "BATCH: 359/1074\n",
            " train loss item: 1.0952401161193848\n",
            " f1:0.29837927\n",
            " avg_train_loss: 0.77714896 \n",
            "BATCH: 360/1074\n",
            " train loss item: 0.9396584033966064\n",
            " f1:0.29829597\n",
            " avg_train_loss: 0.77719433 \n",
            "BATCH: 361/1074\n",
            " train loss item: 0.764836311340332\n",
            " f1:0.29821272\n",
            " avg_train_loss: 0.77719088 \n",
            "BATCH: 362/1074\n",
            " train loss item: 0.6972293257713318\n",
            " f1:0.29831552\n",
            " avg_train_loss: 0.77716857 \n",
            "BATCH: 363/1074\n",
            " train loss item: 0.9223138689994812\n",
            " f1:0.29838446\n",
            " avg_train_loss: 0.77720906 \n",
            "BATCH: 364/1074\n",
            " train loss item: 0.6928948163986206\n",
            " f1:0.29851576\n",
            " avg_train_loss: 0.77718555 \n",
            "BATCH: 365/1074\n",
            " train loss item: 0.8530213832855225\n",
            " f1:0.29860223\n",
            " avg_train_loss: 0.77720669 \n",
            "BATCH: 366/1074\n",
            " train loss item: 0.7467443346977234\n",
            " f1:0.29867103\n",
            " avg_train_loss: 0.77719820 \n",
            "BATCH: 367/1074\n",
            " train loss item: 0.6977907419204712\n",
            " f1:0.29858782\n",
            " avg_train_loss: 0.77717607 \n",
            "BATCH: 368/1074\n",
            " train loss item: 0.719936728477478\n",
            " f1:0.29850464\n",
            " avg_train_loss: 0.77716013 \n",
            "BATCH: 369/1074\n",
            " train loss item: 1.1769769191741943\n",
            " f1:0.29842152\n",
            " avg_train_loss: 0.77727147 \n",
            "BATCH: 370/1074\n",
            " train loss item: 1.0405571460723877\n",
            " f1:0.29833844\n",
            " avg_train_loss: 0.77734477 \n",
            "BATCH: 371/1074\n",
            " train loss item: 0.7229208946228027\n",
            " f1:0.29836673\n",
            " avg_train_loss: 0.77732962 \n",
            "BATCH: 372/1074\n",
            " train loss item: 0.6661430597305298\n",
            " f1:0.29849775\n",
            " avg_train_loss: 0.77729868 \n",
            "BATCH: 373/1074\n",
            " train loss item: 0.8551210165023804\n",
            " f1:0.29860016\n",
            " avg_train_loss: 0.77732033 \n",
            "BATCH: 374/1074\n",
            " train loss item: 0.9131798148155212\n",
            " f1:0.29868639\n",
            " avg_train_loss: 0.77735811 \n",
            "BATCH: 375/1074\n",
            " train loss item: 0.661591649055481\n",
            " f1:0.29881721\n",
            " avg_train_loss: 0.77732593 \n",
            "BATCH: 376/1074\n",
            " train loss item: 0.6900532245635986\n",
            " f1:0.29873416\n",
            " avg_train_loss: 0.77730167 \n",
            "BATCH: 377/1074\n",
            " train loss item: 0.6212355494499207\n",
            " f1:0.29865115\n",
            " avg_train_loss: 0.77725831 \n",
            "BATCH: 378/1074\n",
            " train loss item: 0.8680237531661987\n",
            " f1:0.29856819\n",
            " avg_train_loss: 0.77728352 \n",
            "BATCH: 379/1074\n",
            " train loss item: 1.0268827676773071\n",
            " f1:0.29848528\n",
            " avg_train_loss: 0.77735283 \n",
            "BATCH: 380/1074\n",
            " train loss item: 0.8862879276275635\n",
            " f1:0.29840241\n",
            " avg_train_loss: 0.77738308 \n",
            "BATCH: 381/1074\n",
            " train loss item: 0.8096493482589722\n",
            " f1:0.29831959\n",
            " avg_train_loss: 0.77739203 \n",
            "BATCH: 382/1074\n",
            " train loss item: 1.0718469619750977\n",
            " f1:0.29834781\n",
            " avg_train_loss: 0.77747373 \n",
            "BATCH: 383/1074\n",
            " train loss item: 0.8421312570571899\n",
            " f1:0.29846477\n",
            " avg_train_loss: 0.77749167 \n",
            "BATCH: 384/1074\n",
            " train loss item: 1.0902912616729736\n",
            " f1:0.29853326\n",
            " avg_train_loss: 0.77757841 \n",
            "BATCH: 385/1074\n",
            " train loss item: 0.7379096746444702\n",
            " f1:0.29863532\n",
            " avg_train_loss: 0.77756742 \n",
            "BATCH: 386/1074\n",
            " train loss item: 0.6643099784851074\n",
            " f1:0.29855255\n",
            " avg_train_loss: 0.77753602 \n",
            "BATCH: 387/1074\n",
            " train loss item: 0.7265350818634033\n",
            " f1:0.29846983\n",
            " avg_train_loss: 0.77752189 \n",
            "BATCH: 388/1074\n",
            " train loss item: 1.185546875\n",
            " f1:0.29838715\n",
            " avg_train_loss: 0.77763492 \n",
            "BATCH: 389/1074\n",
            " train loss item: 1.0873394012451172\n",
            " f1:0.29830452\n",
            " avg_train_loss: 0.77772069 \n",
            "BATCH: 390/1074\n",
            " train loss item: 0.7101155519485474\n",
            " f1:0.29822193\n",
            " avg_train_loss: 0.77770197 \n",
            "BATCH: 391/1074\n",
            " train loss item: 0.7220467925071716\n",
            " f1:0.29830786\n",
            " avg_train_loss: 0.77768657 \n",
            "BATCH: 392/1074\n",
            " train loss item: 0.8631497621536255\n",
            " f1:0.29839375\n",
            " avg_train_loss: 0.77771021 \n",
            "BATCH: 393/1074\n",
            " train loss item: 0.8933163285255432\n",
            " f1:0.29847959\n",
            " avg_train_loss: 0.77774219 \n",
            "BATCH: 394/1074\n",
            " train loss item: 0.7017643451690674\n",
            " f1:0.29859616\n",
            " avg_train_loss: 0.77772118 \n",
            "BATCH: 395/1074\n",
            " train loss item: 0.6938260197639465\n",
            " f1:0.29869792\n",
            " avg_train_loss: 0.77769799 \n",
            "BATCH: 396/1074\n",
            " train loss item: 0.7747230529785156\n",
            " f1:0.29861536\n",
            " avg_train_loss: 0.77769716 \n",
            "BATCH: 397/1074\n",
            " train loss item: 0.6936184167861938\n",
            " f1:0.29853285\n",
            " avg_train_loss: 0.77767393 \n",
            "BATCH: 398/1074\n",
            " train loss item: 0.7766396999359131\n",
            " f1:0.29845038\n",
            " avg_train_loss: 0.77767365 \n",
            "BATCH: 399/1074\n",
            " train loss item: 0.693344235420227\n",
            " f1:0.29853606\n",
            " avg_train_loss: 0.77765036 \n",
            "BATCH: 400/1074\n",
            " train loss item: 0.6664040088653564\n",
            " f1:0.29866601\n",
            " avg_train_loss: 0.77761964 \n",
            "BATCH: 401/1074\n",
            " train loss item: 0.7898520231246948\n",
            " f1:0.29875158\n",
            " avg_train_loss: 0.77762302 \n",
            "BATCH: 402/1074\n",
            " train loss item: 0.8132714629173279\n",
            " f1:0.29881966\n",
            " avg_train_loss: 0.77763286 \n",
            "BATCH: 403/1074\n",
            " train loss item: 0.6893317103385925\n",
            " f1:0.29893585\n",
            " avg_train_loss: 0.77760850 \n",
            "BATCH: 404/1074\n",
            " train loss item: 0.7066748142242432\n",
            " f1:0.29885340\n",
            " avg_train_loss: 0.77758893 \n",
            "BATCH: 405/1074\n",
            " train loss item: 0.7302660942077637\n",
            " f1:0.29877101\n",
            " avg_train_loss: 0.77757589 \n",
            "BATCH: 406/1074\n",
            " train loss item: 0.8535466194152832\n",
            " f1:0.29868866\n",
            " avg_train_loss: 0.77759683 \n",
            "BATCH: 407/1074\n",
            " train loss item: 0.7296216487884521\n",
            " f1:0.29873757\n",
            " avg_train_loss: 0.77758361 \n",
            "BATCH: 408/1074\n",
            " train loss item: 0.7363629341125488\n",
            " f1:0.29882296\n",
            " avg_train_loss: 0.77757225 \n",
            "BATCH: 409/1074\n",
            " train loss item: 0.6281156539916992\n",
            " f1:0.29897672\n",
            " avg_train_loss: 0.77753109 \n",
            "BATCH: 410/1074\n",
            " train loss item: 0.7955392003059387\n",
            " f1:0.29904458\n",
            " avg_train_loss: 0.77753605 \n",
            "BATCH: 411/1074\n",
            " train loss item: 0.6670767664909363\n",
            " f1:0.29917400\n",
            " avg_train_loss: 0.77750564 \n",
            "BATCH: 412/1074\n",
            " train loss item: 0.6861391663551331\n",
            " f1:0.29928981\n",
            " avg_train_loss: 0.77748050 \n",
            "BATCH: 413/1074\n",
            " train loss item: 0.6884841918945312\n",
            " f1:0.29940555\n",
            " avg_train_loss: 0.77745602 \n",
            "BATCH: 414/1074\n",
            " train loss item: 0.705410361289978\n",
            " f1:0.29947322\n",
            " avg_train_loss: 0.77743620 \n",
            "BATCH: 415/1074\n",
            " train loss item: 0.707222580909729\n",
            " f1:0.29939088\n",
            " avg_train_loss: 0.77741690 \n",
            "BATCH: 416/1074\n",
            " train loss item: 0.7401350736618042\n",
            " f1:0.29930858\n",
            " avg_train_loss: 0.77740665 \n",
            "BATCH: 417/1074\n",
            " train loss item: 0.666581392288208\n",
            " f1:0.29945024\n",
            " avg_train_loss: 0.77737620 \n",
            "BATCH: 418/1074\n",
            " train loss item: 0.7157374620437622\n",
            " f1:0.29956578\n",
            " avg_train_loss: 0.77735926 \n",
            "BATCH: 419/1074\n",
            " train loss item: 0.902482271194458\n",
            " f1:0.29965068\n",
            " avg_train_loss: 0.77739363 \n",
            "BATCH: 420/1074\n",
            " train loss item: 0.9911025166511536\n",
            " f1:0.29967823\n",
            " avg_train_loss: 0.77745231 \n",
            "BATCH: 421/1074\n",
            " train loss item: 0.6765754222869873\n",
            " f1:0.29959597\n",
            " avg_train_loss: 0.77742462 \n",
            "BATCH: 422/1074\n",
            " train loss item: 0.865778923034668\n",
            " f1:0.29951376\n",
            " avg_train_loss: 0.77744886 \n",
            "BATCH: 423/1074\n",
            " train loss item: 0.5913500189781189\n",
            " f1:0.29943158\n",
            " avg_train_loss: 0.77739781 \n",
            "BATCH: 424/1074\n",
            " train loss item: 1.1079591512680054\n",
            " f1:0.29934946\n",
            " avg_train_loss: 0.77748847 \n",
            "BATCH: 425/1074\n",
            " train loss item: 0.9622899889945984\n",
            " f1:0.29926738\n",
            " avg_train_loss: 0.77753914 \n",
            "BATCH: 426/1074\n",
            " train loss item: 0.6250643134117126\n",
            " f1:0.29918534\n",
            " avg_train_loss: 0.77749735 \n",
            "BATCH: 427/1074\n",
            " train loss item: 0.6967902183532715\n",
            " f1:0.29928605\n",
            " avg_train_loss: 0.77747523 \n",
            "BATCH: 428/1074\n",
            " train loss item: 0.8183374404907227\n",
            " f1:0.29937082\n",
            " avg_train_loss: 0.77748642 \n",
            "BATCH: 429/1074\n",
            " train loss item: 0.9068162441253662\n",
            " f1:0.29943822\n",
            " avg_train_loss: 0.77752185 \n",
            "BATCH: 430/1074\n",
            " train loss item: 0.7964802980422974\n",
            " f1:0.29948662\n",
            " avg_train_loss: 0.77752704 \n",
            "BATCH: 431/1074\n",
            " train loss item: 0.7257111072540283\n",
            " f1:0.29940463\n",
            " avg_train_loss: 0.77751285 \n",
            "BATCH: 432/1074\n",
            " train loss item: 0.7053911089897156\n",
            " f1:0.29932270\n",
            " avg_train_loss: 0.77749312 \n",
            "BATCH: 433/1074\n",
            " train loss item: 0.9444302320480347\n",
            " f1:0.29924080\n",
            " avg_train_loss: 0.77753879 \n",
            "BATCH: 434/1074\n",
            " train loss item: 0.9489572644233704\n",
            " f1:0.29915895\n",
            " avg_train_loss: 0.77758568 \n",
            "BATCH: 435/1074\n",
            " train loss item: 0.8483672142028809\n",
            " f1:0.29907715\n",
            " avg_train_loss: 0.77760503 \n",
            "BATCH: 436/1074\n",
            " train loss item: 0.6910037994384766\n",
            " f1:0.29920568\n",
            " avg_train_loss: 0.77758136 \n",
            "BATCH: 437/1074\n",
            " train loss item: 1.046827793121338\n",
            " f1:0.29932068\n",
            " avg_train_loss: 0.77765494 \n",
            "BATCH: 438/1074\n",
            " train loss item: 1.995848536491394\n",
            " f1:0.29934819\n",
            " avg_train_loss: 0.77798778 \n",
            "BATCH: 439/1074\n",
            " train loss item: 1.262878656387329\n",
            " f1:0.29941541\n",
            " avg_train_loss: 0.77812023 \n",
            "BATCH: 440/1074\n",
            " train loss item: 0.771015465259552\n",
            " f1:0.29941988\n",
            " avg_train_loss: 0.77811829 \n",
            "BATCH: 441/1074\n",
            " train loss item: 0.9386439323425293\n",
            " f1:0.29933814\n",
            " avg_train_loss: 0.77816211 \n",
            "BATCH: 442/1074\n",
            " train loss item: 1.5961312055587769\n",
            " f1:0.29925644\n",
            " avg_train_loss: 0.77838536 \n",
            "BATCH: 443/1074\n",
            " train loss item: 1.7775561809539795\n",
            " f1:0.29917479\n",
            " avg_train_loss: 0.77865798 \n",
            "BATCH: 444/1074\n",
            " train loss item: 0.9961273670196533\n",
            " f1:0.29909318\n",
            " avg_train_loss: 0.77871730 \n",
            "BATCH: 445/1074\n",
            " train loss item: 1.5986335277557373\n",
            " f1:0.29901162\n",
            " avg_train_loss: 0.77894090 \n",
            "BATCH: 446/1074\n",
            " train loss item: 0.6152998208999634\n",
            " f1:0.29893010\n",
            " avg_train_loss: 0.77889628 \n",
            "BATCH: 447/1074\n",
            " train loss item: 0.8625326156616211\n",
            " f1:0.29901453\n",
            " avg_train_loss: 0.77891908 \n",
            "BATCH: 448/1074\n",
            " train loss item: 1.00812828540802\n",
            " f1:0.29911471\n",
            " avg_train_loss: 0.77898153 \n",
            "BATCH: 449/1074\n",
            " train loss item: 1.1372666358947754\n",
            " f1:0.29919904\n",
            " avg_train_loss: 0.77907913 \n",
            "BATCH: 450/1074\n",
            " train loss item: 0.765203595161438\n",
            " f1:0.29931363\n",
            " avg_train_loss: 0.77907535 \n",
            "BATCH: 451/1074\n",
            " train loss item: 0.6746021509170532\n",
            " f1:0.29944157\n",
            " avg_train_loss: 0.77904691 \n",
            "BATCH: 452/1074\n",
            " train loss item: 0.7849193215370178\n",
            " f1:0.29936007\n",
            " avg_train_loss: 0.77904851 \n",
            "BATCH: 453/1074\n",
            " train loss item: 0.6648498773574829\n",
            " f1:0.29927861\n",
            " avg_train_loss: 0.77901743 \n",
            "BATCH: 454/1074\n",
            " train loss item: 0.7187379598617554\n",
            " f1:0.29919720\n",
            " avg_train_loss: 0.77900104 \n",
            "BATCH: 455/1074\n",
            " train loss item: 0.7468454241752625\n",
            " f1:0.29911583\n",
            " avg_train_loss: 0.77899229 \n",
            "BATCH: 456/1074\n",
            " train loss item: 0.7029268145561218\n",
            " f1:0.29903450\n",
            " avg_train_loss: 0.77897161 \n",
            "BATCH: 457/1074\n",
            " train loss item: 0.6972934007644653\n",
            " f1:0.29913443\n",
            " avg_train_loss: 0.77894941 \n",
            "BATCH: 458/1074\n",
            " train loss item: 0.6290283799171448\n",
            " f1:0.29927456\n",
            " avg_train_loss: 0.77890867 \n",
            "BATCH: 459/1074\n",
            " train loss item: 0.8553465604782104\n",
            " f1:0.29935862\n",
            " avg_train_loss: 0.77892943 \n",
            "BATCH: 460/1074\n",
            " train loss item: 0.9785823822021484\n",
            " f1:0.29938595\n",
            " avg_train_loss: 0.77898366 \n",
            "BATCH: 461/1074\n",
            " train loss item: 0.696060836315155\n",
            " f1:0.29930466\n",
            " avg_train_loss: 0.77896114 \n",
            "BATCH: 462/1074\n",
            " train loss item: 0.7085710763931274\n",
            " f1:0.29922342\n",
            " avg_train_loss: 0.77894204 \n",
            "BATCH: 463/1074\n",
            " train loss item: 0.7682030200958252\n",
            " f1:0.29914222\n",
            " avg_train_loss: 0.77893912 \n",
            "BATCH: 464/1074\n",
            " train loss item: 0.6339207887649536\n",
            " f1:0.29906106\n",
            " avg_train_loss: 0.77889978 \n",
            "BATCH: 465/1074\n",
            " train loss item: 0.6951520442962646\n",
            " f1:0.29897995\n",
            " avg_train_loss: 0.77887706 \n",
            "BATCH: 466/1074\n",
            " train loss item: 0.7685024738311768\n",
            " f1:0.29889888\n",
            " avg_train_loss: 0.77887425 \n",
            "BATCH: 467/1074\n",
            " train loss item: 0.6990550756454468\n",
            " f1:0.29881786\n",
            " avg_train_loss: 0.77885261 \n",
            "BATCH: 468/1074\n",
            " train loss item: 0.6633665561676025\n",
            " f1:0.29894534\n",
            " avg_train_loss: 0.77882132 \n",
            "BATCH: 469/1074\n",
            " train loss item: 0.7478479146957397\n",
            " f1:0.29905942\n",
            " avg_train_loss: 0.77881293 \n",
            "BATCH: 470/1074\n",
            " train loss item: 0.8727145195007324\n",
            " f1:0.29915898\n",
            " avg_train_loss: 0.77883836 \n",
            "BATCH: 471/1074\n",
            " train loss item: 0.9347350597381592\n",
            " f1:0.29922568\n",
            " avg_train_loss: 0.77888057 \n",
            "BATCH: 472/1074\n",
            " train loss item: 0.6659751534461975\n",
            " f1:0.29936525\n",
            " avg_train_loss: 0.77885001 \n",
            "BATCH: 473/1074\n",
            " train loss item: 0.7067230939865112\n",
            " f1:0.29928423\n",
            " avg_train_loss: 0.77883049 \n",
            "BATCH: 474/1074\n",
            " train loss item: 0.6633083820343018\n",
            " f1:0.29920326\n",
            " avg_train_loss: 0.77879923 \n",
            "BATCH: 475/1074\n",
            " train loss item: 0.7288451194763184\n",
            " f1:0.29912233\n",
            " avg_train_loss: 0.77878572 \n",
            "BATCH: 476/1074\n",
            " train loss item: 0.6724355220794678\n",
            " f1:0.29904144\n",
            " avg_train_loss: 0.77875696 \n",
            "BATCH: 477/1074\n",
            " train loss item: 0.8310263156890869\n",
            " f1:0.29896059\n",
            " avg_train_loss: 0.77877109 \n",
            "BATCH: 478/1074\n",
            " train loss item: 0.6862673759460449\n",
            " f1:0.29887979\n",
            " avg_train_loss: 0.77874609 \n",
            "BATCH: 479/1074\n",
            " train loss item: 0.6854170560836792\n",
            " f1:0.29899358\n",
            " avg_train_loss: 0.77872087 \n",
            "BATCH: 480/1074\n",
            " train loss item: 0.8082873821258545\n",
            " f1:0.29904144\n",
            " avg_train_loss: 0.77872886 \n",
            "BATCH: 481/1074\n",
            " train loss item: 0.7074138522148132\n",
            " f1:0.29912507\n",
            " avg_train_loss: 0.77870960 \n",
            "BATCH: 482/1074\n",
            " train loss item: 0.6679702997207642\n",
            " f1:0.29904431\n",
            " avg_train_loss: 0.77867971 \n",
            "BATCH: 483/1074\n",
            " train loss item: 0.8159537315368652\n",
            " f1:0.29896360\n",
            " avg_train_loss: 0.77868977 \n",
            "BATCH: 484/1074\n",
            " train loss item: 0.7716708779335022\n",
            " f1:0.29888293\n",
            " avg_train_loss: 0.77868787 \n",
            "BATCH: 485/1074\n",
            " train loss item: 0.5995608568191528\n",
            " f1:0.29880230\n",
            " avg_train_loss: 0.77863955 \n",
            "BATCH: 486/1074\n",
            " train loss item: 0.6897042989730835\n",
            " f1:0.29872172\n",
            " avg_train_loss: 0.77861557 \n",
            "BATCH: 487/1074\n",
            " train loss item: 0.705281138420105\n",
            " f1:0.29864118\n",
            " avg_train_loss: 0.77859579 \n",
            "BATCH: 488/1074\n",
            " train loss item: 0.6897172331809998\n",
            " f1:0.29856068\n",
            " avg_train_loss: 0.77857184 \n",
            "BATCH: 489/1074\n",
            " train loss item: 0.688897967338562\n",
            " f1:0.29867425\n",
            " avg_train_loss: 0.77854767 \n",
            "BATCH: 490/1074\n",
            " train loss item: 0.6854419112205505\n",
            " f1:0.29878775\n",
            " avg_train_loss: 0.77852259 \n",
            "BATCH: 491/1074\n",
            " train loss item: 0.7750040292739868\n",
            " f1:0.29885418\n",
            " avg_train_loss: 0.77852164 \n",
            "BATCH: 492/1074\n",
            " train loss item: 0.673409104347229\n",
            " f1:0.29898083\n",
            " avg_train_loss: 0.77849334 \n",
            "BATCH: 493/1074\n",
            " train loss item: 0.6943337917327881\n",
            " f1:0.29907981\n",
            " avg_train_loss: 0.77847069 \n",
            "BATCH: 494/1074\n",
            " train loss item: 0.6932892799377441\n",
            " f1:0.29899932\n",
            " avg_train_loss: 0.77844776 \n",
            "BATCH: 495/1074\n",
            " train loss item: 0.695500373840332\n",
            " f1:0.29891888\n",
            " avg_train_loss: 0.77842545 \n",
            "BATCH: 496/1074\n",
            " train loss item: 0.6754007339477539\n",
            " f1:0.29883848\n",
            " avg_train_loss: 0.77839774 \n",
            "BATCH: 497/1074\n",
            " train loss item: 0.7293976545333862\n",
            " f1:0.29875813\n",
            " avg_train_loss: 0.77838456 \n",
            "BATCH: 498/1074\n",
            " train loss item: 0.6700479984283447\n",
            " f1:0.29867782\n",
            " avg_train_loss: 0.77835544 \n",
            "BATCH: 499/1074\n",
            " train loss item: 0.718537449836731\n",
            " f1:0.29859755\n",
            " avg_train_loss: 0.77833936 \n",
            "BATCH: 500/1074\n",
            " train loss item: 0.6897432804107666\n",
            " f1:0.29851732\n",
            " avg_train_loss: 0.77831556 \n",
            "BATCH: 501/1074\n",
            " train loss item: 0.6883930563926697\n",
            " f1:0.29864376\n",
            " avg_train_loss: 0.77829141 \n",
            "BATCH: 502/1074\n",
            " train loss item: 0.7274264693260193\n",
            " f1:0.29872701\n",
            " avg_train_loss: 0.77827775 \n",
            "BATCH: 503/1074\n",
            " train loss item: 0.7719389200210571\n",
            " f1:0.29877466\n",
            " avg_train_loss: 0.77827605 \n",
            "BATCH: 504/1074\n",
            " train loss item: 0.6874826550483704\n",
            " f1:0.29869447\n",
            " avg_train_loss: 0.77825168 \n",
            "BATCH: 505/1074\n",
            " train loss item: 0.6992625594139099\n",
            " f1:0.29861433\n",
            " avg_train_loss: 0.77823049 \n",
            "BATCH: 506/1074\n",
            " train loss item: 1.0050047636032104\n",
            " f1:0.29853423\n",
            " avg_train_loss: 0.77829132 \n",
            "BATCH: 507/1074\n",
            " train loss item: 0.6435965299606323\n",
            " f1:0.29845417\n",
            " avg_train_loss: 0.77825520 \n",
            "BATCH: 508/1074\n",
            " train loss item: 0.693340003490448\n",
            " f1:0.29837415\n",
            " avg_train_loss: 0.77823243 \n",
            "BATCH: 509/1074\n",
            " train loss item: 0.7009464502334595\n",
            " f1:0.29847287\n",
            " avg_train_loss: 0.77821172 \n",
            "BATCH: 510/1074\n",
            " train loss item: 0.738470196723938\n",
            " f1:0.29855599\n",
            " avg_train_loss: 0.77820107 \n",
            "BATCH: 511/1074\n",
            " train loss item: 0.7339823246002197\n",
            " f1:0.29862213\n",
            " avg_train_loss: 0.77818922 \n",
            "BATCH: 512/1074\n",
            " train loss item: 0.6856615543365479\n",
            " f1:0.29854216\n",
            " avg_train_loss: 0.77816444 \n",
            "BATCH: 513/1074\n",
            " train loss item: 0.7028725743293762\n",
            " f1:0.29846223\n",
            " avg_train_loss: 0.77814428 \n",
            "BATCH: 514/1074\n",
            " train loss item: 0.730156660079956\n",
            " f1:0.29838234\n",
            " avg_train_loss: 0.77813144 \n",
            "BATCH: 515/1074\n",
            " train loss item: 0.8812686204910278\n",
            " f1:0.29830249\n",
            " avg_train_loss: 0.77815904 \n",
            "BATCH: 516/1074\n",
            " train loss item: 0.7457188963890076\n",
            " f1:0.29822269\n",
            " avg_train_loss: 0.77815036 \n",
            "BATCH: 517/1074\n",
            " train loss item: 0.7565007209777832\n",
            " f1:0.29832123\n",
            " avg_train_loss: 0.77814457 \n",
            "BATCH: 518/1074\n",
            " train loss item: 0.9670886993408203\n",
            " f1:0.29840422\n",
            " avg_train_loss: 0.77819509 \n",
            "BATCH: 519/1074\n",
            " train loss item: 1.0108802318572998\n",
            " f1:0.29847026\n",
            " avg_train_loss: 0.77825729 \n",
            "BATCH: 520/1074\n",
            " train loss item: 0.7432664036750793\n",
            " f1:0.29855316\n",
            " avg_train_loss: 0.77824794 \n",
            "BATCH: 521/1074\n",
            " train loss item: 0.7984089851379395\n",
            " f1:0.29847340\n",
            " avg_train_loss: 0.77825332 \n",
            "BATCH: 522/1074\n",
            " train loss item: 0.8085843920707703\n",
            " f1:0.29839368\n",
            " avg_train_loss: 0.77826142 \n",
            "BATCH: 523/1074\n",
            " train loss item: 0.8686767816543579\n",
            " f1:0.29831400\n",
            " avg_train_loss: 0.77828557 \n",
            "BATCH: 524/1074\n",
            " train loss item: 0.7502293586730957\n",
            " f1:0.29823436\n",
            " avg_train_loss: 0.77827808 \n",
            "BATCH: 525/1074\n",
            " train loss item: 0.7036592364311218\n",
            " f1:0.29833269\n",
            " avg_train_loss: 0.77825816 \n",
            "BATCH: 526/1074\n",
            " train loss item: 0.8877660036087036\n",
            " f1:0.29839862\n",
            " avg_train_loss: 0.77828738 \n",
            "BATCH: 527/1074\n",
            " train loss item: 0.8686841130256653\n",
            " f1:0.29846452\n",
            " avg_train_loss: 0.77831149 \n",
            "BATCH: 528/1074\n",
            " train loss item: 0.7102009057998657\n",
            " f1:0.29854725\n",
            " avg_train_loss: 0.77829333 \n",
            "BATCH: 529/1074\n",
            " train loss item: 0.728327751159668\n",
            " f1:0.29846766\n",
            " avg_train_loss: 0.77828001 \n",
            "BATCH: 530/1074\n",
            " train loss item: 0.6846470832824707\n",
            " f1:0.29838811\n",
            " avg_train_loss: 0.77825505 \n",
            "BATCH: 531/1074\n",
            " train loss item: 0.8582644462585449\n",
            " f1:0.29830861\n",
            " avg_train_loss: 0.77827637 \n",
            "BATCH: 532/1074\n",
            " train loss item: 0.5655416250228882\n",
            " f1:0.29822914\n",
            " avg_train_loss: 0.77821970 \n",
            "BATCH: 533/1074\n",
            " train loss item: 0.8987786769866943\n",
            " f1:0.29814972\n",
            " avg_train_loss: 0.77825181 \n",
            "BATCH: 534/1074\n",
            " train loss item: 0.6871712803840637\n",
            " f1:0.29827514\n",
            " avg_train_loss: 0.77822756 \n",
            "BATCH: 535/1074\n",
            " train loss item: 0.9229938983917236\n",
            " f1:0.29834093\n",
            " avg_train_loss: 0.77826609 \n",
            "BATCH: 536/1074\n",
            " train loss item: 1.0746448040008545\n",
            " f1:0.29838826\n",
            " avg_train_loss: 0.77834496 \n",
            "BATCH: 537/1074\n",
            " train loss item: 0.7349653244018555\n",
            " f1:0.29848623\n",
            " avg_train_loss: 0.77833342 \n",
            "BATCH: 538/1074\n",
            " train loss item: 0.7216470837593079\n",
            " f1:0.29840684\n",
            " avg_train_loss: 0.77831834 \n",
            "BATCH: 539/1074\n",
            " train loss item: 0.6622790098190308\n",
            " f1:0.29832750\n",
            " avg_train_loss: 0.77828749 \n",
            "BATCH: 540/1074\n",
            " train loss item: 0.9090529680252075\n",
            " f1:0.29824820\n",
            " avg_train_loss: 0.77832225 \n",
            "BATCH: 541/1074\n",
            " train loss item: 0.7409162521362305\n",
            " f1:0.29816894\n",
            " avg_train_loss: 0.77831231 \n",
            "BATCH: 542/1074\n",
            " train loss item: 0.6759178042411804\n",
            " f1:0.29808973\n",
            " avg_train_loss: 0.77828511 \n",
            "BATCH: 543/1074\n",
            " train loss item: 0.6999647617340088\n",
            " f1:0.29817223\n",
            " avg_train_loss: 0.77826430 \n",
            "BATCH: 544/1074\n",
            " train loss item: 0.6948151588439941\n",
            " f1:0.29827007\n",
            " avg_train_loss: 0.77824214 \n",
            "BATCH: 545/1074\n",
            " train loss item: 0.6886690855026245\n",
            " f1:0.29838203\n",
            " avg_train_loss: 0.77821837 \n",
            "BATCH: 546/1074\n",
            " train loss item: 0.6706589460372925\n",
            " f1:0.29851909\n",
            " avg_train_loss: 0.77818982 \n",
            "BATCH: 547/1074\n",
            " train loss item: 0.7996265292167664\n",
            " f1:0.29856623\n",
            " avg_train_loss: 0.77819551 \n",
            "BATCH: 548/1074\n",
            " train loss item: 0.7240091562271118\n",
            " f1:0.29863171\n",
            " avg_train_loss: 0.77818114 \n",
            "BATCH: 549/1074\n",
            " train loss item: 0.6468029618263245\n",
            " f1:0.29855252\n",
            " avg_train_loss: 0.77814630 \n",
            "BATCH: 550/1074\n",
            " train loss item: 0.8059437274932861\n",
            " f1:0.29847337\n",
            " avg_train_loss: 0.77815367 \n",
            "BATCH: 551/1074\n",
            " train loss item: 0.8550188541412354\n",
            " f1:0.29839426\n",
            " avg_train_loss: 0.77817404 \n",
            "BATCH: 552/1074\n",
            " train loss item: 0.8457626700401306\n",
            " f1:0.29831520\n",
            " avg_train_loss: 0.77819195 \n",
            "BATCH: 553/1074\n",
            " train loss item: 0.6860350370407104\n",
            " f1:0.29823617\n",
            " avg_train_loss: 0.77816753 \n",
            "BATCH: 554/1074\n",
            " train loss item: 0.8546015024185181\n",
            " f1:0.29824082\n",
            " avg_train_loss: 0.77818778 \n",
            "BATCH: 555/1074\n",
            " train loss item: 0.6853337287902832\n",
            " f1:0.29835249\n",
            " avg_train_loss: 0.77816319 \n",
            "BATCH: 556/1074\n",
            " train loss item: 0.706913948059082\n",
            " f1:0.29841789\n",
            " avg_train_loss: 0.77814433 \n",
            "BATCH: 557/1074\n",
            " train loss item: 0.6664926409721375\n",
            " f1:0.29833893\n",
            " avg_train_loss: 0.77811479 \n",
            "BATCH: 558/1074\n",
            " train loss item: 0.6686016321182251\n",
            " f1:0.29826000\n",
            " avg_train_loss: 0.77808582 \n",
            "BATCH: 559/1074\n",
            " train loss item: 0.8294038772583008\n",
            " f1:0.29818112\n",
            " avg_train_loss: 0.77809939 \n",
            "BATCH: 560/1074\n",
            " train loss item: 0.9689727425575256\n",
            " f1:0.29810227\n",
            " avg_train_loss: 0.77814986 \n",
            "BATCH: 561/1074\n",
            " train loss item: 0.7059639096260071\n",
            " f1:0.29802347\n",
            " avg_train_loss: 0.77813078 \n",
            "BATCH: 562/1074\n",
            " train loss item: 0.8500487804412842\n",
            " f1:0.29810558\n",
            " avg_train_loss: 0.77814978 \n",
            "BATCH: 563/1074\n",
            " train loss item: 0.8177467584609985\n",
            " f1:0.29821704\n",
            " avg_train_loss: 0.77816024 \n",
            "BATCH: 564/1074\n",
            " train loss item: 0.8971414566040039\n",
            " f1:0.29831436\n",
            " avg_train_loss: 0.77819167 \n",
            "BATCH: 565/1074\n",
            " train loss item: 0.7725514769554138\n",
            " f1:0.29841163\n",
            " avg_train_loss: 0.77819018 \n",
            "BATCH: 566/1074\n",
            " train loss item: 0.694005012512207\n",
            " f1:0.29849354\n",
            " avg_train_loss: 0.77816796 \n",
            "BATCH: 567/1074\n",
            " train loss item: 0.7741419076919556\n",
            " f1:0.29841476\n",
            " avg_train_loss: 0.77816689 \n",
            "BATCH: 568/1074\n",
            " train loss item: 0.7939506769180298\n",
            " f1:0.29833602\n",
            " avg_train_loss: 0.77817106 \n",
            "BATCH: 569/1074\n",
            " train loss item: 0.8719764947891235\n",
            " f1:0.29825733\n",
            " avg_train_loss: 0.77819580 \n",
            "BATCH: 570/1074\n",
            " train loss item: 0.5735796093940735\n",
            " f1:0.29817867\n",
            " avg_train_loss: 0.77814184 \n",
            "BATCH: 571/1074\n",
            " train loss item: 0.7474257349967957\n",
            " f1:0.29810006\n",
            " avg_train_loss: 0.77813375 \n",
            "BATCH: 572/1074\n",
            " train loss item: 0.6949899792671204\n",
            " f1:0.29819720\n",
            " avg_train_loss: 0.77811183 \n",
            "BATCH: 573/1074\n",
            " train loss item: 0.6622722148895264\n",
            " f1:0.29832132\n",
            " avg_train_loss: 0.77808131 \n",
            "BATCH: 574/1074\n",
            " train loss item: 0.8694464564323425\n",
            " f1:0.29840309\n",
            " avg_train_loss: 0.77810538 \n",
            "BATCH: 575/1074\n",
            " train loss item: 0.7762854099273682\n",
            " f1:0.29850008\n",
            " avg_train_loss: 0.77810490 \n",
            "BATCH: 576/1074\n",
            " train loss item: 0.730412483215332\n",
            " f1:0.29858175\n",
            " avg_train_loss: 0.77809234 \n",
            "BATCH: 577/1074\n",
            " train loss item: 0.6862051486968994\n",
            " f1:0.29850315\n",
            " avg_train_loss: 0.77806815 \n",
            "BATCH: 578/1074\n",
            " train loss item: 0.7378407716751099\n",
            " f1:0.29842460\n",
            " avg_train_loss: 0.77805757 \n",
            "BATCH: 579/1074\n",
            " train loss item: 0.5629408955574036\n",
            " f1:0.29834609\n",
            " avg_train_loss: 0.77800097 \n",
            "BATCH: 580/1074\n",
            " train loss item: 0.9167581796646118\n",
            " f1:0.29826762\n",
            " avg_train_loss: 0.77803747 \n",
            "BATCH: 581/1074\n",
            " train loss item: 0.7600974440574646\n",
            " f1:0.29818919\n",
            " avg_train_loss: 0.77803275 \n",
            "BATCH: 582/1074\n",
            " train loss item: 0.6068730354309082\n",
            " f1:0.29811080\n",
            " avg_train_loss: 0.77798775 \n",
            "BATCH: 583/1074\n",
            " train loss item: 0.6954834461212158\n",
            " f1:0.29803245\n",
            " avg_train_loss: 0.77796607 \n",
            "BATCH: 584/1074\n",
            " train loss item: 0.7226905822753906\n",
            " f1:0.29809746\n",
            " avg_train_loss: 0.77795155 \n",
            "BATCH: 585/1074\n",
            " train loss item: 0.6857007741928101\n",
            " f1:0.29820828\n",
            " avg_train_loss: 0.77792732 \n",
            "BATCH: 586/1074\n",
            " train loss item: 0.6990482807159424\n",
            " f1:0.29830504\n",
            " avg_train_loss: 0.77790660 \n",
            "BATCH: 587/1074\n",
            " train loss item: 0.713602602481842\n",
            " f1:0.29836993\n",
            " avg_train_loss: 0.77788972 \n",
            "BATCH: 588/1074\n",
            " train loss item: 0.672869861125946\n",
            " f1:0.29829162\n",
            " avg_train_loss: 0.77786216 \n",
            "BATCH: 589/1074\n",
            " train loss item: 0.6233288645744324\n",
            " f1:0.29821335\n",
            " avg_train_loss: 0.77782161 \n",
            "BATCH: 590/1074\n",
            " train loss item: 0.9054244756698608\n",
            " f1:0.29813512\n",
            " avg_train_loss: 0.77785508 \n",
            "BATCH: 591/1074\n",
            " train loss item: 0.6842721700668335\n",
            " f1:0.29805693\n",
            " avg_train_loss: 0.77783054 \n",
            "BATCH: 592/1074\n",
            " train loss item: 0.747653067111969\n",
            " f1:0.29797878\n",
            " avg_train_loss: 0.77782262 \n",
            "BATCH: 593/1074\n",
            " train loss item: 0.687411904335022\n",
            " f1:0.29790067\n",
            " avg_train_loss: 0.77779893 \n",
            "BATCH: 594/1074\n",
            " train loss item: 0.7642103433609009\n",
            " f1:0.29794739\n",
            " avg_train_loss: 0.77779537 \n",
            "BATCH: 595/1074\n",
            " train loss item: 0.7761930227279663\n",
            " f1:0.29795207\n",
            " avg_train_loss: 0.77779495 \n",
            "BATCH: 596/1074\n",
            " train loss item: 0.7206466794013977\n",
            " f1:0.29787403\n",
            " avg_train_loss: 0.77777998 \n",
            "BATCH: 597/1074\n",
            " train loss item: 0.8507097959518433\n",
            " f1:0.29779603\n",
            " avg_train_loss: 0.77779907 \n",
            "BATCH: 598/1074\n",
            " train loss item: 0.8800117373466492\n",
            " f1:0.29771807\n",
            " avg_train_loss: 0.77782583 \n",
            "BATCH: 599/1074\n",
            " train loss item: 0.6736177802085876\n",
            " f1:0.29764016\n",
            " avg_train_loss: 0.77779856 \n",
            "BATCH: 600/1074\n",
            " train loss item: 0.7157978415489197\n",
            " f1:0.29772154\n",
            " avg_train_loss: 0.77778234 \n",
            "BATCH: 601/1074\n",
            " train loss item: 0.715361475944519\n",
            " f1:0.29781805\n",
            " avg_train_loss: 0.77776601 \n",
            "BATCH: 602/1074\n",
            " train loss item: 0.738480806350708\n",
            " f1:0.29789935\n",
            " avg_train_loss: 0.77775574 \n",
            "BATCH: 603/1074\n",
            " train loss item: 0.6880567669868469\n",
            " f1:0.29800970\n",
            " avg_train_loss: 0.77773228 \n",
            "BATCH: 604/1074\n",
            " train loss item: 0.7009751796722412\n",
            " f1:0.29793181\n",
            " avg_train_loss: 0.77771222 \n",
            "BATCH: 605/1074\n",
            " train loss item: 0.7038671970367432\n",
            " f1:0.29785396\n",
            " avg_train_loss: 0.77769293 \n",
            "BATCH: 606/1074\n",
            " train loss item: 0.6925613880157471\n",
            " f1:0.29797710\n",
            " avg_train_loss: 0.77767069 \n",
            "BATCH: 607/1074\n",
            " train loss item: 0.7297812700271606\n",
            " f1:0.29805825\n",
            " avg_train_loss: 0.77765818 \n",
            "BATCH: 608/1074\n",
            " train loss item: 0.7620382308959961\n",
            " f1:0.29812284\n",
            " avg_train_loss: 0.77765410 \n",
            "BATCH: 609/1074\n",
            " train loss item: 0.6934676170349121\n",
            " f1:0.29821904\n",
            " avg_train_loss: 0.77763213 \n",
            "BATCH: 610/1074\n",
            " train loss item: 0.7383320331573486\n",
            " f1:0.29814122\n",
            " avg_train_loss: 0.77762187 \n",
            "BATCH: 611/1074\n",
            " train loss item: 0.6702858209609985\n",
            " f1:0.29806343\n",
            " avg_train_loss: 0.77759387 \n",
            "BATCH: 612/1074\n",
            " train loss item: 0.6263675093650818\n",
            " f1:0.29798569\n",
            " avg_train_loss: 0.77755443 \n",
            "BATCH: 613/1074\n",
            " train loss item: 0.8365634679794312\n",
            " f1:0.29790799\n",
            " avg_train_loss: 0.77756981 \n",
            "BATCH: 614/1074\n",
            " train loss item: 0.7253575325012207\n",
            " f1:0.29783033\n",
            " avg_train_loss: 0.77755620 \n",
            "BATCH: 615/1074\n",
            " train loss item: 0.6786499619483948\n",
            " f1:0.29775271\n",
            " avg_train_loss: 0.77753042 \n",
            "BATCH: 616/1074\n",
            " train loss item: 0.6933982968330383\n",
            " f1:0.29784883\n",
            " avg_train_loss: 0.77750850 \n",
            "BATCH: 617/1074\n",
            " train loss item: 0.720737099647522\n",
            " f1:0.29791333\n",
            " avg_train_loss: 0.77749372 \n",
            "BATCH: 618/1074\n",
            " train loss item: 0.6964666843414307\n",
            " f1:0.29793991\n",
            " avg_train_loss: 0.77747261 \n",
            "BATCH: 619/1074\n",
            " train loss item: 0.6988385319709778\n",
            " f1:0.29786234\n",
            " avg_train_loss: 0.77745214 \n",
            "BATCH: 620/1074\n",
            " train loss item: 0.871477484703064\n",
            " f1:0.29778482\n",
            " avg_train_loss: 0.77747661 \n",
            "BATCH: 621/1074\n",
            " train loss item: 0.5690910816192627\n",
            " f1:0.29770733\n",
            " avg_train_loss: 0.77742239 \n",
            "BATCH: 622/1074\n",
            " train loss item: 0.7697568535804749\n",
            " f1:0.29762988\n",
            " avg_train_loss: 0.77742040 \n",
            "BATCH: 623/1074\n",
            " train loss item: 0.6889594793319702\n",
            " f1:0.29755247\n",
            " avg_train_loss: 0.77739739 \n",
            "BATCH: 624/1074\n",
            " train loss item: 0.6966679096221924\n",
            " f1:0.29747511\n",
            " avg_train_loss: 0.77737640 \n",
            "BATCH: 625/1074\n",
            " train loss item: 0.8358423709869385\n",
            " f1:0.29753957\n",
            " avg_train_loss: 0.77739160 \n",
            "BATCH: 626/1074\n",
            " train loss item: 0.6211254596710205\n",
            " f1:0.29767400\n",
            " avg_train_loss: 0.77735099 \n",
            "BATCH: 627/1074\n",
            " train loss item: 0.7367321848869324\n",
            " f1:0.29778372\n",
            " avg_train_loss: 0.77734043 \n",
            "BATCH: 628/1074\n",
            " train loss item: 0.8067454099655151\n",
            " f1:0.29786448\n",
            " avg_train_loss: 0.77734807 \n",
            "BATCH: 629/1074\n",
            " train loss item: 0.7176061868667603\n",
            " f1:0.29792877\n",
            " avg_train_loss: 0.77733256 \n",
            "BATCH: 630/1074\n",
            " train loss item: 0.623439610004425\n",
            " f1:0.29785142\n",
            " avg_train_loss: 0.77729261 \n",
            "BATCH: 631/1074\n",
            " train loss item: 0.9287285208702087\n",
            " f1:0.29777412\n",
            " avg_train_loss: 0.77733191 \n",
            "BATCH: 632/1074\n",
            " train loss item: 1.0824795961380005\n",
            " f1:0.29769686\n",
            " avg_train_loss: 0.77741109 \n",
            "BATCH: 633/1074\n",
            " train loss item: 0.9592690467834473\n",
            " f1:0.29761963\n",
            " avg_train_loss: 0.77745826 \n",
            "BATCH: 634/1074\n",
            " train loss item: 0.7023569345474243\n",
            " f1:0.29768391\n",
            " avg_train_loss: 0.77743879 \n",
            "BATCH: 635/1074\n",
            " train loss item: 0.882095217704773\n",
            " f1:0.29774814\n",
            " avg_train_loss: 0.77746592 \n",
            "BATCH: 636/1074\n",
            " train loss item: 0.8639270067214966\n",
            " f1:0.29782874\n",
            " avg_train_loss: 0.77748833 \n",
            "BATCH: 637/1074\n",
            " train loss item: 0.7334885597229004\n",
            " f1:0.29792432\n",
            " avg_train_loss: 0.77747693 \n",
            "BATCH: 638/1074\n",
            " train loss item: 0.6919329166412354\n",
            " f1:0.29784714\n",
            " avg_train_loss: 0.77745477 \n",
            "BATCH: 639/1074\n",
            " train loss item: 0.7048112154006958\n",
            " f1:0.29777000\n",
            " avg_train_loss: 0.77743595 \n",
            "BATCH: 640/1074\n",
            " train loss item: 0.7607811689376831\n",
            " f1:0.29769289\n",
            " avg_train_loss: 0.77743164 \n",
            "BATCH: 641/1074\n",
            " train loss item: 0.8388277292251587\n",
            " f1:0.29761583\n",
            " avg_train_loss: 0.77744753 \n",
            "BATCH: 642/1074\n",
            " train loss item: 0.6660958528518677\n",
            " f1:0.29753881\n",
            " avg_train_loss: 0.77741872 \n",
            "BATCH: 643/1074\n",
            " train loss item: 0.6853709816932678\n",
            " f1:0.29746183\n",
            " avg_train_loss: 0.77739490 \n",
            "BATCH: 644/1074\n",
            " train loss item: 0.7033324241638184\n",
            " f1:0.29754233\n",
            " avg_train_loss: 0.77737574 \n",
            "BATCH: 645/1074\n",
            " train loss item: 0.6859866380691528\n",
            " f1:0.29765158\n",
            " avg_train_loss: 0.77735211 \n",
            "BATCH: 646/1074\n",
            " train loss item: 0.6617597341537476\n",
            " f1:0.29777350\n",
            " avg_train_loss: 0.77732222 \n",
            "BATCH: 647/1074\n",
            " train loss item: 0.8171319961547852\n",
            " f1:0.29783751\n",
            " avg_train_loss: 0.77733251 \n",
            "BATCH: 648/1074\n",
            " train loss item: 0.7187872529029846\n",
            " f1:0.29791784\n",
            " avg_train_loss: 0.77731739 \n",
            "BATCH: 649/1074\n",
            " train loss item: 0.7195754051208496\n",
            " f1:0.29784088\n",
            " avg_train_loss: 0.77730247 \n",
            "BATCH: 650/1074\n",
            " train loss item: 0.7280662059783936\n",
            " f1:0.29776395\n",
            " avg_train_loss: 0.77728975 \n",
            "BATCH: 651/1074\n",
            " train loss item: 0.7620913982391357\n",
            " f1:0.29768707\n",
            " avg_train_loss: 0.77728583 \n",
            "BATCH: 652/1074\n",
            " train loss item: 0.685641348361969\n",
            " f1:0.29761023\n",
            " avg_train_loss: 0.77726217 \n",
            "BATCH: 653/1074\n",
            " train loss item: 0.6939542293548584\n",
            " f1:0.29770547\n",
            " avg_train_loss: 0.77724067 \n",
            "BATCH: 654/1074\n",
            " train loss item: 0.7413220405578613\n",
            " f1:0.29776939\n",
            " avg_train_loss: 0.77723141 \n",
            "BATCH: 655/1074\n",
            " train loss item: 0.6867655515670776\n",
            " f1:0.29787830\n",
            " avg_train_loss: 0.77720807 \n",
            "BATCH: 656/1074\n",
            " train loss item: 0.68708336353302\n",
            " f1:0.29799984\n",
            " avg_train_loss: 0.77718483 \n",
            "BATCH: 657/1074\n",
            " train loss item: 0.6965894103050232\n",
            " f1:0.29809488\n",
            " avg_train_loss: 0.77716406 \n",
            "BATCH: 658/1074\n",
            " train loss item: 0.6856838464736938\n",
            " f1:0.29820362\n",
            " avg_train_loss: 0.77714048 \n",
            "BATCH: 659/1074\n",
            " train loss item: 0.7008076906204224\n",
            " f1:0.29829856\n",
            " avg_train_loss: 0.77712081 \n",
            "BATCH: 660/1074\n",
            " train loss item: 0.6974362134933472\n",
            " f1:0.29839345\n",
            " avg_train_loss: 0.77710028 \n",
            "BATCH: 661/1074\n",
            " train loss item: 0.6964031457901001\n",
            " f1:0.29847337\n",
            " avg_train_loss: 0.77707950 \n",
            "BATCH: 662/1074\n",
            " train loss item: 0.6595742106437683\n",
            " f1:0.29839652\n",
            " avg_train_loss: 0.77704925 \n",
            "BATCH: 663/1074\n",
            " train loss item: 0.664763331413269\n",
            " f1:0.29831971\n",
            " avg_train_loss: 0.77702035 \n",
            "BATCH: 664/1074\n",
            " train loss item: 0.8209294080734253\n",
            " f1:0.29824294\n",
            " avg_train_loss: 0.77703164 \n",
            "BATCH: 665/1074\n",
            " train loss item: 0.7982769012451172\n",
            " f1:0.29816622\n",
            " avg_train_loss: 0.77703711 \n",
            "BATCH: 666/1074\n",
            " train loss item: 0.6905744671821594\n",
            " f1:0.29808953\n",
            " avg_train_loss: 0.77701487 \n",
            "BATCH: 667/1074\n",
            " train loss item: 0.6935479044914246\n",
            " f1:0.29818430\n",
            " avg_train_loss: 0.77699341 \n",
            "BATCH: 668/1074\n",
            " train loss item: 0.6925914287567139\n",
            " f1:0.29829274\n",
            " avg_train_loss: 0.77697171 \n",
            "BATCH: 669/1074\n",
            " train loss item: 0.8671174049377441\n",
            " f1:0.29835626\n",
            " avg_train_loss: 0.77699488 \n",
            "BATCH: 670/1074\n",
            " train loss item: 0.7548340559005737\n",
            " f1:0.29843600\n",
            " avg_train_loss: 0.77698919 \n",
            "BATCH: 671/1074\n",
            " train loss item: 0.7109436988830566\n",
            " f1:0.29835934\n",
            " avg_train_loss: 0.77697222 \n",
            "BATCH: 672/1074\n",
            " train loss item: 0.7624199390411377\n",
            " f1:0.29828272\n",
            " avg_train_loss: 0.77696848 \n",
            "BATCH: 673/1074\n",
            " train loss item: 0.7268251180648804\n",
            " f1:0.29832839\n",
            " avg_train_loss: 0.77695561 \n",
            "BATCH: 674/1074\n",
            " train loss item: 0.7269940972328186\n",
            " f1:0.29835449\n",
            " avg_train_loss: 0.77694279 \n",
            "BATCH: 675/1074\n",
            " train loss item: 0.6880794763565063\n",
            " f1:0.29827793\n",
            " avg_train_loss: 0.77691998 \n",
            "BATCH: 676/1074\n",
            " train loss item: 0.7736755013465881\n",
            " f1:0.29820141\n",
            " avg_train_loss: 0.77691915 \n",
            "BATCH: 677/1074\n",
            " train loss item: 0.884895384311676\n",
            " f1:0.29812493\n",
            " avg_train_loss: 0.77694684 \n",
            "BATCH: 678/1074\n",
            " train loss item: 0.6855746507644653\n",
            " f1:0.29804848\n",
            " avg_train_loss: 0.77692342 \n",
            "BATCH: 679/1074\n",
            " train loss item: 0.6988646984100342\n",
            " f1:0.29814298\n",
            " avg_train_loss: 0.77690341 \n",
            "BATCH: 680/1074\n",
            " train loss item: 0.6983959674835205\n",
            " f1:0.29825109\n",
            " avg_train_loss: 0.77688329 \n",
            "BATCH: 681/1074\n",
            " train loss item: 0.765679121017456\n",
            " f1:0.29834548\n",
            " avg_train_loss: 0.77688042 \n",
            "BATCH: 682/1074\n",
            " train loss item: 0.8558740615844727\n",
            " f1:0.29839104\n",
            " avg_train_loss: 0.77690065 \n",
            "BATCH: 683/1074\n",
            " train loss item: 0.6802825927734375\n",
            " f1:0.29831463\n",
            " avg_train_loss: 0.77687591 \n",
            "BATCH: 684/1074\n",
            " train loss item: 0.9196743965148926\n",
            " f1:0.29823825\n",
            " avg_train_loss: 0.77691247 \n",
            "BATCH: 685/1074\n",
            " train loss item: 0.9008948802947998\n",
            " f1:0.29816192\n",
            " avg_train_loss: 0.77694420 \n",
            "BATCH: 686/1074\n",
            " train loss item: 0.6641042828559875\n",
            " f1:0.29808562\n",
            " avg_train_loss: 0.77691533 \n",
            "BATCH: 687/1074\n",
            " train loss item: 0.6448012590408325\n",
            " f1:0.29800937\n",
            " avg_train_loss: 0.77688153 \n",
            "BATCH: 688/1074\n",
            " train loss item: 0.6965559124946594\n",
            " f1:0.29793315\n",
            " avg_train_loss: 0.77686098 \n",
            "BATCH: 689/1074\n",
            " train loss item: 0.6876758337020874\n",
            " f1:0.29785697\n",
            " avg_train_loss: 0.77683818 \n",
            "BATCH: 690/1074\n",
            " train loss item: 0.7042717933654785\n",
            " f1:0.29778083\n",
            " avg_train_loss: 0.77681963 \n",
            "BATCH: 691/1074\n",
            " train loss item: 0.6934748888015747\n",
            " f1:0.29770473\n",
            " avg_train_loss: 0.77679833 \n",
            "BATCH: 692/1074\n",
            " train loss item: 0.6934688091278076\n",
            " f1:0.29779900\n",
            " avg_train_loss: 0.77677704 \n",
            "BATCH: 693/1074\n",
            " train loss item: 0.7024514079093933\n",
            " f1:0.29787841\n",
            " avg_train_loss: 0.77675806 \n",
            "BATCH: 694/1074\n",
            " train loss item: 0.695513129234314\n",
            " f1:0.29794163\n",
            " avg_train_loss: 0.77673731 \n",
            "BATCH: 695/1074\n",
            " train loss item: 0.647799015045166\n",
            " f1:0.29786557\n",
            " avg_train_loss: 0.77670439 \n",
            "BATCH: 696/1074\n",
            " train loss item: 0.6210881471633911\n",
            " f1:0.29778954\n",
            " avg_train_loss: 0.77666467 \n",
            "BATCH: 697/1074\n",
            " train loss item: 0.7945190668106079\n",
            " f1:0.29771356\n",
            " avg_train_loss: 0.77666923 \n",
            "BATCH: 698/1074\n",
            " train loss item: 0.8764966130256653\n",
            " f1:0.29763761\n",
            " avg_train_loss: 0.77669470 \n",
            "BATCH: 699/1074\n",
            " train loss item: 0.6694687604904175\n",
            " f1:0.29756170\n",
            " avg_train_loss: 0.77666735 \n",
            "BATCH: 700/1074\n",
            " train loss item: 0.6853271722793579\n",
            " f1:0.29748583\n",
            " avg_train_loss: 0.77664406 \n",
            "BATCH: 701/1074\n",
            " train loss item: 0.7146204710006714\n",
            " f1:0.29756516\n",
            " avg_train_loss: 0.77662825 \n",
            "BATCH: 702/1074\n",
            " train loss item: 0.8018709421157837\n",
            " f1:0.29761068\n",
            " avg_train_loss: 0.77663468 \n",
            "BATCH: 703/1074\n",
            " train loss item: 0.715530276298523\n",
            " f1:0.29765618\n",
            " avg_train_loss: 0.77661911 \n",
            "BATCH: 704/1074\n",
            " train loss item: 0.8054501414299011\n",
            " f1:0.29758036\n",
            " avg_train_loss: 0.77662646 \n",
            "BATCH: 705/1074\n",
            " train loss item: 0.5765748620033264\n",
            " f1:0.29750459\n",
            " avg_train_loss: 0.77657552 \n",
            "BATCH: 706/1074\n",
            " train loss item: 0.7490068674087524\n",
            " f1:0.29742885\n",
            " avg_train_loss: 0.77656850 \n",
            "BATCH: 707/1074\n",
            " train loss item: 0.7989290356636047\n",
            " f1:0.29735314\n",
            " avg_train_loss: 0.77657419 \n",
            "BATCH: 708/1074\n",
            " train loss item: 0.7866951823234558\n",
            " f1:0.29727748\n",
            " avg_train_loss: 0.77657676 \n",
            "BATCH: 709/1074\n",
            " train loss item: 0.706801176071167\n",
            " f1:0.29737145\n",
            " avg_train_loss: 0.77655901 \n",
            "BATCH: 710/1074\n",
            " train loss item: 0.7985206246376038\n",
            " f1:0.29746537\n",
            " avg_train_loss: 0.77656460 \n",
            "BATCH: 711/1074\n",
            " train loss item: 1.0391874313354492\n",
            " f1:0.29751081\n",
            " avg_train_loss: 0.77663137 \n",
            "BATCH: 712/1074\n",
            " train loss item: 0.7189873456954956\n",
            " f1:0.29760465\n",
            " avg_train_loss: 0.77661672 \n",
            "BATCH: 713/1074\n",
            " train loss item: 0.7248607873916626\n",
            " f1:0.29752902\n",
            " avg_train_loss: 0.77660357 \n",
            "BATCH: 714/1074\n",
            " train loss item: 0.6211951375007629\n",
            " f1:0.29745343\n",
            " avg_train_loss: 0.77656408 \n",
            "BATCH: 715/1074\n",
            " train loss item: 1.0010557174682617\n",
            " f1:0.29737788\n",
            " avg_train_loss: 0.77662110 \n",
            "BATCH: 716/1074\n",
            " train loss item: 0.8342535495758057\n",
            " f1:0.29730236\n",
            " avg_train_loss: 0.77663574 \n",
            "BATCH: 717/1074\n",
            " train loss item: 0.7250505089759827\n",
            " f1:0.29722688\n",
            " avg_train_loss: 0.77662264 \n",
            "BATCH: 718/1074\n",
            " train loss item: 0.930402934551239\n",
            " f1:0.29730594\n",
            " avg_train_loss: 0.77666167 \n",
            "BATCH: 719/1074\n",
            " train loss item: 1.4688174724578857\n",
            " f1:0.29733199\n",
            " avg_train_loss: 0.77683730 \n",
            "BATCH: 720/1074\n",
            " train loss item: 0.7865424156188965\n",
            " f1:0.29743922\n",
            " avg_train_loss: 0.77683977 \n",
            "BATCH: 721/1074\n",
            " train loss item: 0.7070044279098511\n",
            " f1:0.29753286\n",
            " avg_train_loss: 0.77682205 \n",
            "BATCH: 722/1074\n",
            " train loss item: 0.7718126773834229\n",
            " f1:0.29745742\n",
            " avg_train_loss: 0.77682078 \n",
            "BATCH: 723/1074\n",
            " train loss item: 0.7593973875045776\n",
            " f1:0.29738202\n",
            " avg_train_loss: 0.77681637 \n",
            "BATCH: 724/1074\n",
            " train loss item: 0.7068314552307129\n",
            " f1:0.29730665\n",
            " avg_train_loss: 0.77679863 \n",
            "BATCH: 725/1074\n",
            " train loss item: 0.5631093978881836\n",
            " f1:0.29723133\n",
            " avg_train_loss: 0.77674449 \n",
            "BATCH: 726/1074\n",
            " train loss item: 0.792877197265625\n",
            " f1:0.29715604\n",
            " avg_train_loss: 0.77674858 \n",
            "BATCH: 727/1074\n",
            " train loss item: 0.7450006008148193\n",
            " f1:0.29708079\n",
            " avg_train_loss: 0.77674054 \n",
            "BATCH: 728/1074\n",
            " train loss item: 0.7905420660972595\n",
            " f1:0.29708553\n",
            " avg_train_loss: 0.77674403 \n",
            "BATCH: 729/1074\n",
            " train loss item: 0.7012429237365723\n",
            " f1:0.29717907\n",
            " avg_train_loss: 0.77672492 \n",
            "BATCH: 730/1074\n",
            " train loss item: 0.7099316120147705\n",
            " f1:0.29724190\n",
            " avg_train_loss: 0.77670802 \n",
            "BATCH: 731/1074\n",
            " train loss item: 0.702599287033081\n",
            " f1:0.29716670\n",
            " avg_train_loss: 0.77668928 \n",
            "BATCH: 732/1074\n",
            " train loss item: 0.6615633368492126\n",
            " f1:0.29709155\n",
            " avg_train_loss: 0.77666016 \n",
            "BATCH: 733/1074\n",
            " train loss item: 0.8472317457199097\n",
            " f1:0.29701643\n",
            " avg_train_loss: 0.77667800 \n",
            "BATCH: 734/1074\n",
            " train loss item: 0.7597985863685608\n",
            " f1:0.29694135\n",
            " avg_train_loss: 0.77667374 \n",
            "BATCH: 735/1074\n",
            " train loss item: 0.6281560659408569\n",
            " f1:0.29708292\n",
            " avg_train_loss: 0.77663620 \n",
            "BATCH: 736/1074\n",
            " train loss item: 0.7961649894714355\n",
            " f1:0.29718977\n",
            " avg_train_loss: 0.77664114 \n",
            "BATCH: 737/1074\n",
            " train loss item: 1.4423370361328125\n",
            " f1:0.29721574\n",
            " avg_train_loss: 0.77680928 \n",
            "BATCH: 738/1074\n",
            " train loss item: 0.7618803977966309\n",
            " f1:0.29732250\n",
            " avg_train_loss: 0.77680551 \n",
            "BATCH: 739/1074\n",
            " train loss item: 0.6968638896942139\n",
            " f1:0.29741575\n",
            " avg_train_loss: 0.77678533 \n",
            "BATCH: 740/1074\n",
            " train loss item: 0.8423718214035034\n",
            " f1:0.29734068\n",
            " avg_train_loss: 0.77680189 \n",
            "BATCH: 741/1074\n",
            " train loss item: 0.8079146146774292\n",
            " f1:0.29726565\n",
            " avg_train_loss: 0.77680974 \n",
            "BATCH: 742/1074\n",
            " train loss item: 0.7238433361053467\n",
            " f1:0.29719066\n",
            " avg_train_loss: 0.77679638 \n",
            "BATCH: 743/1074\n",
            " train loss item: 0.6900458335876465\n",
            " f1:0.29711571\n",
            " avg_train_loss: 0.77677450 \n",
            "BATCH: 744/1074\n",
            " train loss item: 0.6861363649368286\n",
            " f1:0.29704079\n",
            " avg_train_loss: 0.77675164 \n",
            "BATCH: 745/1074\n",
            " train loss item: 0.6889837980270386\n",
            " f1:0.29696591\n",
            " avg_train_loss: 0.77672952 \n",
            "BATCH: 746/1074\n",
            " train loss item: 0.6981488466262817\n",
            " f1:0.29689107\n",
            " avg_train_loss: 0.77670971 \n",
            "BATCH: 747/1074\n",
            " train loss item: 0.6854267120361328\n",
            " f1:0.29681627\n",
            " avg_train_loss: 0.77668672 \n",
            "BATCH: 748/1074\n",
            " train loss item: 0.7011330723762512\n",
            " f1:0.29674151\n",
            " avg_train_loss: 0.77666768 \n",
            "BATCH: 749/1074\n",
            " train loss item: 0.6647894382476807\n",
            " f1:0.29666678\n",
            " avg_train_loss: 0.77663951 \n",
            "BATCH: 750/1074\n",
            " train loss item: 0.7086178064346313\n",
            " f1:0.29659209\n",
            " avg_train_loss: 0.77662238 \n",
            "BATCH: 751/1074\n",
            " train loss item: 0.7547428011894226\n",
            " f1:0.29651744\n",
            " avg_train_loss: 0.77661688 \n",
            "BATCH: 752/1074\n",
            " train loss item: 0.6915056705474854\n",
            " f1:0.29664786\n",
            " avg_train_loss: 0.77659546 \n",
            "BATCH: 753/1074\n",
            " train loss item: 0.706432580947876\n",
            " f1:0.29675436\n",
            " avg_train_loss: 0.77657781 \n",
            "BATCH: 754/1074\n",
            " train loss item: 0.8351107835769653\n",
            " f1:0.29684740\n",
            " avg_train_loss: 0.77659253 \n",
            "BATCH: 755/1074\n",
            " train loss item: 1.0131633281707764\n",
            " f1:0.29689249\n",
            " avg_train_loss: 0.77665202 \n",
            "BATCH: 756/1074\n",
            " train loss item: 0.6854052543640137\n",
            " f1:0.29699886\n",
            " avg_train_loss: 0.77662908 \n",
            "BATCH: 757/1074\n",
            " train loss item: 0.7839069962501526\n",
            " f1:0.29692421\n",
            " avg_train_loss: 0.77663091 \n",
            "BATCH: 758/1074\n",
            " train loss item: 0.6219745874404907\n",
            " f1:0.29684961\n",
            " avg_train_loss: 0.77659205 \n",
            "BATCH: 759/1074\n",
            " train loss item: 0.7380737066268921\n",
            " f1:0.29677504\n",
            " avg_train_loss: 0.77658237 \n",
            "BATCH: 760/1074\n",
            " train loss item: 0.7279521226882935\n",
            " f1:0.29670051\n",
            " avg_train_loss: 0.77657016 \n",
            "BATCH: 761/1074\n",
            " train loss item: 0.7296943664550781\n",
            " f1:0.29662602\n",
            " avg_train_loss: 0.77655839 \n",
            "BATCH: 762/1074\n",
            " train loss item: 0.6892582178115845\n",
            " f1:0.29655157\n",
            " avg_train_loss: 0.77653648 \n",
            "BATCH: 763/1074\n",
            " train loss item: 0.7131174206733704\n",
            " f1:0.29662990\n",
            " avg_train_loss: 0.77652056 \n",
            "BATCH: 764/1074\n",
            " train loss item: 0.685326099395752\n",
            " f1:0.29673611\n",
            " avg_train_loss: 0.77649769 \n",
            "BATCH: 765/1074\n",
            " train loss item: 0.652365505695343\n",
            " f1:0.29686605\n",
            " avg_train_loss: 0.77646655 \n",
            "BATCH: 766/1074\n",
            " train loss item: 0.7553278803825378\n",
            " f1:0.29694425\n",
            " avg_train_loss: 0.77646125 \n",
            "BATCH: 767/1074\n",
            " train loss item: 0.6394071578979492\n",
            " f1:0.29707407\n",
            " avg_train_loss: 0.77642689 \n",
            "BATCH: 768/1074\n",
            " train loss item: 0.6064615249633789\n",
            " f1:0.29721444\n",
            " avg_train_loss: 0.77638430 \n",
            "BATCH: 769/1074\n",
            " train loss item: 0.9019976854324341\n",
            " f1:0.29725928\n",
            " avg_train_loss: 0.77641577 \n",
            "BATCH: 770/1074\n",
            " train loss item: 0.8033876419067383\n",
            " f1:0.29728502\n",
            " avg_train_loss: 0.77642253 \n",
            "BATCH: 771/1074\n",
            " train loss item: 0.6977782845497131\n",
            " f1:0.29721057\n",
            " avg_train_loss: 0.77640283 \n",
            "BATCH: 772/1074\n",
            " train loss item: 0.7263767719268799\n",
            " f1:0.29713616\n",
            " avg_train_loss: 0.77639031 \n",
            "BATCH: 773/1074\n",
            " train loss item: 0.6910722255706787\n",
            " f1:0.29706178\n",
            " avg_train_loss: 0.77636895 \n",
            "BATCH: 774/1074\n",
            " train loss item: 1.4464343786239624\n",
            " f1:0.29698744\n",
            " avg_train_loss: 0.77653663 \n",
            "BATCH: 775/1074\n",
            " train loss item: 0.6658505797386169\n",
            " f1:0.29691314\n",
            " avg_train_loss: 0.77650894 \n",
            "BATCH: 776/1074\n",
            " train loss item: 0.7372610569000244\n",
            " f1:0.29697530\n",
            " avg_train_loss: 0.77649912 \n",
            "BATCH: 777/1074\n",
            " train loss item: 0.7232300639152527\n",
            " f1:0.29708109\n",
            " avg_train_loss: 0.77648580 \n",
            "BATCH: 778/1074\n",
            " train loss item: 0.8970213532447815\n",
            " f1:0.29715899\n",
            " avg_train_loss: 0.77651594 \n",
            "BATCH: 779/1074\n",
            " train loss item: 0.6684630513191223\n",
            " f1:0.29727698\n",
            " avg_train_loss: 0.77648893 \n",
            "BATCH: 780/1074\n",
            " train loss item: 0.7808735966682434\n",
            " f1:0.29732168\n",
            " avg_train_loss: 0.77649003 \n",
            "BATCH: 781/1074\n",
            " train loss item: 0.6908105611801147\n",
            " f1:0.29724741\n",
            " avg_train_loss: 0.77646862 \n",
            "BATCH: 782/1074\n",
            " train loss item: 0.9148852825164795\n",
            " f1:0.29717317\n",
            " avg_train_loss: 0.77650319 \n",
            "BATCH: 783/1074\n",
            " train loss item: 0.9988506436347961\n",
            " f1:0.29709897\n",
            " avg_train_loss: 0.77655871 \n",
            "BATCH: 784/1074\n",
            " train loss item: 0.6615632772445679\n",
            " f1:0.29702481\n",
            " avg_train_loss: 0.77653000 \n",
            "BATCH: 785/1074\n",
            " train loss item: 0.703351616859436\n",
            " f1:0.29708681\n",
            " avg_train_loss: 0.77651174 \n",
            "BATCH: 786/1074\n",
            " train loss item: 0.7302446961402893\n",
            " f1:0.29716455\n",
            " avg_train_loss: 0.77650020 \n",
            "BATCH: 787/1074\n",
            " train loss item: 0.6861317157745361\n",
            " f1:0.29727002\n",
            " avg_train_loss: 0.77647766 \n",
            "BATCH: 788/1074\n",
            " train loss item: 0.6501363515853882\n",
            " f1:0.29739909\n",
            " avg_train_loss: 0.77644615 \n",
            "BATCH: 789/1074\n",
            " train loss item: 0.6620810627937317\n",
            " f1:0.29751672\n",
            " avg_train_loss: 0.77641764 \n",
            "BATCH: 790/1074\n",
            " train loss item: 0.701546311378479\n",
            " f1:0.29762203\n",
            " avg_train_loss: 0.77639897 \n",
            "BATCH: 791/1074\n",
            " train loss item: 0.5466920137405396\n",
            " f1:0.29777127\n",
            " avg_train_loss: 0.77634173 \n",
            "BATCH: 792/1074\n",
            " train loss item: 0.9067272543907166\n",
            " f1:0.29783298\n",
            " avg_train_loss: 0.77637422 \n",
            "BATCH: 793/1074\n",
            " train loss item: 0.6262978315353394\n",
            " f1:0.29796174\n",
            " avg_train_loss: 0.77633684 \n",
            "BATCH: 794/1074\n",
            " train loss item: 0.7306255102157593\n",
            " f1:0.29803912\n",
            " avg_train_loss: 0.77632546 \n",
            "BATCH: 795/1074\n",
            " train loss item: 0.687799870967865\n",
            " f1:0.29796492\n",
            " avg_train_loss: 0.77630342 \n",
            "BATCH: 796/1074\n",
            " train loss item: 0.7357362508773804\n",
            " f1:0.29789076\n",
            " avg_train_loss: 0.77629332 \n",
            "BATCH: 797/1074\n",
            " train loss item: 0.9080616235733032\n",
            " f1:0.29781664\n",
            " avg_train_loss: 0.77632611 \n",
            "BATCH: 798/1074\n",
            " train loss item: 0.6517184972763062\n",
            " f1:0.29774256\n",
            " avg_train_loss: 0.77629511 \n",
            "BATCH: 799/1074\n",
            " train loss item: 0.6933116316795349\n",
            " f1:0.29766851\n",
            " avg_train_loss: 0.77627447 \n",
            "BATCH: 800/1074\n",
            " train loss item: 0.7331451177597046\n",
            " f1:0.29771290\n",
            " avg_train_loss: 0.77626375 \n",
            "BATCH: 801/1074\n",
            " train loss item: 0.6932166814804077\n",
            " f1:0.29763890\n",
            " avg_train_loss: 0.77624311 \n",
            "BATCH: 802/1074\n",
            " train loss item: 0.6855793595314026\n",
            " f1:0.29756493\n",
            " avg_train_loss: 0.77622058 \n",
            "BATCH: 803/1074\n",
            " train loss item: 0.6390480995178223\n",
            " f1:0.29749100\n",
            " avg_train_loss: 0.77618650 \n",
            "BATCH: 804/1074\n",
            " train loss item: 0.7112037539482117\n",
            " f1:0.29741711\n",
            " avg_train_loss: 0.77617036 \n",
            "BATCH: 805/1074\n",
            " train loss item: 0.6722695827484131\n",
            " f1:0.29734325\n",
            " avg_train_loss: 0.77614455 \n",
            "BATCH: 806/1074\n",
            " train loss item: 0.915982186794281\n",
            " f1:0.29726943\n",
            " avg_train_loss: 0.77617927 \n",
            "BATCH: 807/1074\n",
            " train loss item: 0.6883944869041443\n",
            " f1:0.29719565\n",
            " avg_train_loss: 0.77615748 \n",
            "BATCH: 808/1074\n",
            " train loss item: 0.8346713781356812\n",
            " f1:0.29722116\n",
            " avg_train_loss: 0.77617200 \n",
            "BATCH: 809/1074\n",
            " train loss item: 0.6464544534683228\n",
            " f1:0.29734956\n",
            " avg_train_loss: 0.77613982 \n",
            "BATCH: 810/1074\n",
            " train loss item: 0.6872662305831909\n",
            " f1:0.29745439\n",
            " avg_train_loss: 0.77611778 \n",
            "BATCH: 811/1074\n",
            " train loss item: 0.6398879289627075\n",
            " f1:0.29758267\n",
            " avg_train_loss: 0.77608400 \n",
            "BATCH: 812/1074\n",
            " train loss item: 0.7644174098968506\n",
            " f1:0.29765979\n",
            " avg_train_loss: 0.77608111 \n",
            "BATCH: 813/1074\n",
            " train loss item: 0.7522767782211304\n",
            " f1:0.29772120\n",
            " avg_train_loss: 0.77607521 \n",
            "BATCH: 814/1074\n",
            " train loss item: 0.7236562967300415\n",
            " f1:0.29764744\n",
            " avg_train_loss: 0.77606222 \n",
            "BATCH: 815/1074\n",
            " train loss item: 0.7119734287261963\n",
            " f1:0.29757371\n",
            " avg_train_loss: 0.77604635 \n",
            "BATCH: 816/1074\n",
            " train loss item: 0.7116764783859253\n",
            " f1:0.29750001\n",
            " avg_train_loss: 0.77603041 \n",
            "BATCH: 817/1074\n",
            " train loss item: 0.7255724668502808\n",
            " f1:0.29742636\n",
            " avg_train_loss: 0.77601791 \n",
            "BATCH: 818/1074\n",
            " train loss item: 0.7176327705383301\n",
            " f1:0.29750340\n",
            " avg_train_loss: 0.77600346 \n",
            "BATCH: 819/1074\n",
            " train loss item: 0.7191541194915771\n",
            " f1:0.29759476\n",
            " avg_train_loss: 0.77598939 \n",
            "BATCH: 820/1074\n",
            " train loss item: 0.7442185878753662\n",
            " f1:0.29767173\n",
            " avg_train_loss: 0.77598153 \n",
            "BATCH: 821/1074\n",
            " train loss item: 0.7093104720115662\n",
            " f1:0.29773301\n",
            " avg_train_loss: 0.77596504 \n",
            "BATCH: 822/1074\n",
            " train loss item: 0.7180007100105286\n",
            " f1:0.29765939\n",
            " avg_train_loss: 0.77595071 \n",
            "BATCH: 823/1074\n",
            " train loss item: 0.8109415173530579\n",
            " f1:0.29758580\n",
            " avg_train_loss: 0.77595936 \n",
            "BATCH: 824/1074\n",
            " train loss item: 0.624775767326355\n",
            " f1:0.29751225\n",
            " avg_train_loss: 0.77592199 \n",
            "BATCH: 825/1074\n",
            " train loss item: 0.6953463554382324\n",
            " f1:0.29743874\n",
            " avg_train_loss: 0.77590208 \n",
            "BATCH: 826/1074\n",
            " train loss item: 0.7534767389297485\n",
            " f1:0.29736526\n",
            " avg_train_loss: 0.77589654 \n",
            "BATCH: 827/1074\n",
            " train loss item: 0.697567343711853\n",
            " f1:0.29745647\n",
            " avg_train_loss: 0.77587720 \n",
            "BATCH: 828/1074\n",
            " train loss item: 0.7739086151123047\n",
            " f1:0.29753332\n",
            " avg_train_loss: 0.77587671 \n",
            "BATCH: 829/1074\n",
            " train loss item: 0.6253501176834106\n",
            " f1:0.29766101\n",
            " avg_train_loss: 0.77583955 \n",
            "BATCH: 830/1074\n",
            " train loss item: 0.7060447335243225\n",
            " f1:0.29776524\n",
            " avg_train_loss: 0.77582233 \n",
            "BATCH: 831/1074\n",
            " train loss item: 0.696770429611206\n",
            " f1:0.29786942\n",
            " avg_train_loss: 0.77580282 \n",
            "BATCH: 832/1074\n",
            " train loss item: 0.6860784888267517\n",
            " f1:0.29797354\n",
            " avg_train_loss: 0.77578069 \n",
            "BATCH: 833/1074\n",
            " train loss item: 0.7053757905960083\n",
            " f1:0.29803458\n",
            " avg_train_loss: 0.77576333 \n",
            "BATCH: 834/1074\n",
            " train loss item: 0.6388879418373108\n",
            " f1:0.29796110\n",
            " avg_train_loss: 0.77572958 \n",
            "BATCH: 835/1074\n",
            " train loss item: 0.7483497858047485\n",
            " f1:0.29788765\n",
            " avg_train_loss: 0.77572283 \n",
            "BATCH: 836/1074\n",
            " train loss item: 1.0231645107269287\n",
            " f1:0.29781424\n",
            " avg_train_loss: 0.77578381 \n",
            "BATCH: 837/1074\n",
            " train loss item: 0.6670390367507935\n",
            " f1:0.29774087\n",
            " avg_train_loss: 0.77575702 \n",
            "BATCH: 838/1074\n",
            " train loss item: 0.661468505859375\n",
            " f1:0.29766754\n",
            " avg_train_loss: 0.77572887 \n",
            "BATCH: 839/1074\n",
            " train loss item: 0.6957713961601257\n",
            " f1:0.29774413\n",
            " avg_train_loss: 0.77570918 \n",
            "BATCH: 840/1074\n",
            " train loss item: 0.6874629259109497\n",
            " f1:0.29784808\n",
            " avg_train_loss: 0.77568746 \n",
            "BATCH: 841/1074\n",
            " train loss item: 0.7322757840156555\n",
            " f1:0.29790902\n",
            " avg_train_loss: 0.77567677 \n",
            "BATCH: 842/1074\n",
            " train loss item: 0.7015594244003296\n",
            " f1:0.29793414\n",
            " avg_train_loss: 0.77565853 \n",
            "BATCH: 843/1074\n",
            " train loss item: 0.6630675792694092\n",
            " f1:0.29786085\n",
            " avg_train_loss: 0.77563084 \n",
            "BATCH: 844/1074\n",
            " train loss item: 0.7090494632720947\n",
            " f1:0.29778759\n",
            " avg_train_loss: 0.77561446 \n",
            "BATCH: 845/1074\n",
            " train loss item: 0.8361445665359497\n",
            " f1:0.29771437\n",
            " avg_train_loss: 0.77562934 \n",
            "BATCH: 846/1074\n",
            " train loss item: 0.7132052183151245\n",
            " f1:0.29764119\n",
            " avg_train_loss: 0.77561400 \n",
            "BATCH: 847/1074\n",
            " train loss item: 0.668117105960846\n",
            " f1:0.29756804\n",
            " avg_train_loss: 0.77558758 \n",
            "BATCH: 848/1074\n",
            " train loss item: 0.6543046236038208\n",
            " f1:0.29749493\n",
            " avg_train_loss: 0.77555778 \n",
            "BATCH: 849/1074\n",
            " train loss item: 0.6918439865112305\n",
            " f1:0.29742185\n",
            " avg_train_loss: 0.77553722 \n",
            "BATCH: 850/1074\n",
            " train loss item: 0.6989161968231201\n",
            " f1:0.29749829\n",
            " avg_train_loss: 0.77551840 \n",
            "BATCH: 851/1074\n",
            " train loss item: 0.693213939666748\n",
            " f1:0.29758893\n",
            " avg_train_loss: 0.77549819 \n",
            "BATCH: 852/1074\n",
            " train loss item: 0.6957147121429443\n",
            " f1:0.29751589\n",
            " avg_train_loss: 0.77547861 \n",
            "BATCH: 853/1074\n",
            " train loss item: 0.7000710964202881\n",
            " f1:0.29755973\n",
            " avg_train_loss: 0.77546011 \n",
            "BATCH: 854/1074\n",
            " train loss item: 0.7478442788124084\n",
            " f1:0.29748673\n",
            " avg_train_loss: 0.77545333 \n",
            "BATCH: 855/1074\n",
            " train loss item: 0.689886212348938\n",
            " f1:0.29741376\n",
            " avg_train_loss: 0.77543234 \n",
            "BATCH: 856/1074\n",
            " train loss item: 0.6881711483001709\n",
            " f1:0.29754064\n",
            " avg_train_loss: 0.77541094 \n",
            "BATCH: 857/1074\n",
            " train loss item: 0.716023325920105\n",
            " f1:0.29763113\n",
            " avg_train_loss: 0.77539639 \n",
            "BATCH: 858/1074\n",
            " train loss item: 0.6625221967697144\n",
            " f1:0.29774672\n",
            " avg_train_loss: 0.77536872 \n",
            "BATCH: 859/1074\n",
            " train loss item: 0.8462616801261902\n",
            " f1:0.29780742\n",
            " avg_train_loss: 0.77538609 \n",
            "BATCH: 860/1074\n",
            " train loss item: 0.7036056518554688\n",
            " f1:0.29789778\n",
            " avg_train_loss: 0.77536851 \n",
            "BATCH: 861/1074\n",
            " train loss item: 0.6995190382003784\n",
            " f1:0.29782482\n",
            " avg_train_loss: 0.77534993 \n",
            "BATCH: 862/1074\n",
            " train loss item: 0.6622779965400696\n",
            " f1:0.29775190\n",
            " avg_train_loss: 0.77532224 \n",
            "BATCH: 863/1074\n",
            " train loss item: 0.732479453086853\n",
            " f1:0.29767901\n",
            " avg_train_loss: 0.77531176 \n",
            "BATCH: 864/1074\n",
            " train loss item: 0.8993732929229736\n",
            " f1:0.29760615\n",
            " avg_train_loss: 0.77534212 \n",
            "BATCH: 865/1074\n",
            " train loss item: 0.6855051517486572\n",
            " f1:0.29753333\n",
            " avg_train_loss: 0.77532014 \n",
            "BATCH: 866/1074\n",
            " train loss item: 0.7347912788391113\n",
            " f1:0.29759398\n",
            " avg_train_loss: 0.77531022 \n",
            "BATCH: 867/1074\n",
            " train loss item: 0.8199490308761597\n",
            " f1:0.29761903\n",
            " avg_train_loss: 0.77532114 \n",
            "BATCH: 868/1074\n",
            " train loss item: 0.6986047029495239\n",
            " f1:0.29754626\n",
            " avg_train_loss: 0.77530238 \n",
            "BATCH: 869/1074\n",
            " train loss item: 0.734129786491394\n",
            " f1:0.29747353\n",
            " avg_train_loss: 0.77529232 \n",
            "BATCH: 870/1074\n",
            " train loss item: 0.7554694414138794\n",
            " f1:0.29740083\n",
            " avg_train_loss: 0.77528747 \n",
            "BATCH: 871/1074\n",
            " train loss item: 0.6874573826789856\n",
            " f1:0.29752724\n",
            " avg_train_loss: 0.77526602 \n",
            "BATCH: 872/1074\n",
            " train loss item: 0.8441148400306702\n",
            " f1:0.29758780\n",
            " avg_train_loss: 0.77528283 \n",
            "BATCH: 873/1074\n",
            " train loss item: 0.868484377861023\n",
            " f1:0.29764833\n",
            " avg_train_loss: 0.77530559 \n",
            "BATCH: 874/1074\n",
            " train loss item: 0.7231521606445312\n",
            " f1:0.29772427\n",
            " avg_train_loss: 0.77529286 \n",
            "BATCH: 875/1074\n",
            " train loss item: 0.6633331775665283\n",
            " f1:0.29765160\n",
            " avg_train_loss: 0.77526553 \n",
            "BATCH: 876/1074\n",
            " train loss item: 0.8277539014816284\n",
            " f1:0.29757897\n",
            " avg_train_loss: 0.77527834 \n",
            "BATCH: 877/1074\n",
            " train loss item: 0.7175520658493042\n",
            " f1:0.29750637\n",
            " avg_train_loss: 0.77526426 \n",
            "BATCH: 878/1074\n",
            " train loss item: 0.9101217985153198\n",
            " f1:0.29743381\n",
            " avg_train_loss: 0.77529715 \n",
            "BATCH: 879/1074\n",
            " train loss item: 0.7724215388298035\n",
            " f1:0.29736128\n",
            " avg_train_loss: 0.77529645 \n",
            "BATCH: 880/1074\n",
            " train loss item: 0.7351107597351074\n",
            " f1:0.29745131\n",
            " avg_train_loss: 0.77528665 \n",
            "BATCH: 881/1074\n",
            " train loss item: 0.8770902156829834\n",
            " f1:0.29754130\n",
            " avg_train_loss: 0.77531146 \n",
            "BATCH: 882/1074\n",
            " train loss item: 0.9905140995979309\n",
            " f1:0.29761712\n",
            " avg_train_loss: 0.77536390 \n",
            "BATCH: 883/1074\n",
            " train loss item: 0.9215866327285767\n",
            " f1:0.29766062\n",
            " avg_train_loss: 0.77539952 \n",
            "BATCH: 884/1074\n",
            " train loss item: 0.7335659265518188\n",
            " f1:0.29758812\n",
            " avg_train_loss: 0.77538933 \n",
            "BATCH: 885/1074\n",
            " train loss item: 0.7064095735549927\n",
            " f1:0.29751566\n",
            " avg_train_loss: 0.77537254 \n",
            "BATCH: 886/1074\n",
            " train loss item: 0.7859543561935425\n",
            " f1:0.29744324\n",
            " avg_train_loss: 0.77537511 \n",
            "BATCH: 887/1074\n",
            " train loss item: 0.6911334991455078\n",
            " f1:0.29737085\n",
            " avg_train_loss: 0.77535461 \n",
            "BATCH: 888/1074\n",
            " train loss item: 0.7476019859313965\n",
            " f1:0.29729850\n",
            " avg_train_loss: 0.77534786 \n",
            "BATCH: 889/1074\n",
            " train loss item: 0.6767594814300537\n",
            " f1:0.29722618\n",
            " avg_train_loss: 0.77532388 \n",
            "BATCH: 890/1074\n",
            " train loss item: 0.669715166091919\n",
            " f1:0.29715390\n",
            " avg_train_loss: 0.77529819 \n",
            "BATCH: 891/1074\n",
            " train loss item: 0.7488784193992615\n",
            " f1:0.29717890\n",
            " avg_train_loss: 0.77529177 \n",
            "BATCH: 892/1074\n",
            " train loss item: 0.7023617029190063\n",
            " f1:0.29723925\n",
            " avg_train_loss: 0.77527404 \n",
            "BATCH: 893/1074\n",
            " train loss item: 0.781636118888855\n",
            " f1:0.29716702\n",
            " avg_train_loss: 0.77527559 \n",
            "BATCH: 894/1074\n",
            " train loss item: 0.6972705125808716\n",
            " f1:0.29709482\n",
            " avg_train_loss: 0.77525664 \n",
            "BATCH: 895/1074\n",
            " train loss item: 0.7235415577888489\n",
            " f1:0.29717051\n",
            " avg_train_loss: 0.77524408 \n",
            "BATCH: 896/1074\n",
            " train loss item: 0.6635413765907288\n",
            " f1:0.29728514\n",
            " avg_train_loss: 0.77521695 \n",
            "BATCH: 897/1074\n",
            " train loss item: 0.7249982953071594\n",
            " f1:0.29737482\n",
            " avg_train_loss: 0.77520476 \n",
            "BATCH: 898/1074\n",
            " train loss item: 0.787229061126709\n",
            " f1:0.29741822\n",
            " avg_train_loss: 0.77520768 \n",
            "BATCH: 899/1074\n",
            " train loss item: 0.6854710578918457\n",
            " f1:0.29734605\n",
            " avg_train_loss: 0.77518590 \n",
            "BATCH: 900/1074\n",
            " train loss item: 0.7190096974372864\n",
            " f1:0.29727391\n",
            " avg_train_loss: 0.77517227 \n",
            "BATCH: 901/1074\n",
            " train loss item: 0.964165210723877\n",
            " f1:0.29720181\n",
            " avg_train_loss: 0.77521811 \n",
            "BATCH: 902/1074\n",
            " train loss item: 0.7474254369735718\n",
            " f1:0.29712975\n",
            " avg_train_loss: 0.77521137 \n",
            "BATCH: 903/1074\n",
            " train loss item: 0.6938180923461914\n",
            " f1:0.29705771\n",
            " avg_train_loss: 0.77519164 \n",
            "BATCH: 904/1074\n",
            " train loss item: 0.7525111436843872\n",
            " f1:0.29714729\n",
            " avg_train_loss: 0.77518614 \n",
            "BATCH: 905/1074\n",
            " train loss item: 0.8204531669616699\n",
            " f1:0.29723683\n",
            " avg_train_loss: 0.77519711 \n",
            "BATCH: 906/1074\n",
            " train loss item: 0.6791030168533325\n",
            " f1:0.29735117\n",
            " avg_train_loss: 0.77517383 \n",
            "BATCH: 907/1074\n",
            " train loss item: 0.7030789256095886\n",
            " f1:0.29745353\n",
            " avg_train_loss: 0.77515637 \n",
            "BATCH: 908/1074\n",
            " train loss item: 0.6853799819946289\n",
            " f1:0.29755584\n",
            " avg_train_loss: 0.77513464 \n",
            "BATCH: 909/1074\n",
            " train loss item: 0.7032155990600586\n",
            " f1:0.29748381\n",
            " avg_train_loss: 0.77511723 \n",
            "BATCH: 910/1074\n",
            " train loss item: 0.6679874658584595\n",
            " f1:0.29741182\n",
            " avg_train_loss: 0.77509130 \n",
            "BATCH: 911/1074\n",
            " train loss item: 0.7259179949760437\n",
            " f1:0.29733986\n",
            " avg_train_loss: 0.77507940 \n",
            "BATCH: 912/1074\n",
            " train loss item: 0.6615768074989319\n",
            " f1:0.29726793\n",
            " avg_train_loss: 0.77505195 \n",
            "BATCH: 913/1074\n",
            " train loss item: 0.692318856716156\n",
            " f1:0.29719604\n",
            " avg_train_loss: 0.77503194 \n",
            "BATCH: 914/1074\n",
            " train loss item: 0.6636250615119934\n",
            " f1:0.29712419\n",
            " avg_train_loss: 0.77500500 \n",
            "BATCH: 915/1074\n",
            " train loss item: 0.64436936378479\n",
            " f1:0.29705237\n",
            " avg_train_loss: 0.77497343 \n",
            "BATCH: 916/1074\n",
            " train loss item: 0.6616508960723877\n",
            " f1:0.29698058\n",
            " avg_train_loss: 0.77494604 \n",
            "BATCH: 917/1074\n",
            " train loss item: 0.7812245488166809\n",
            " f1:0.29690883\n",
            " avg_train_loss: 0.77494756 \n",
            "BATCH: 918/1074\n",
            " train loss item: 0.7336127758026123\n",
            " f1:0.29683711\n",
            " avg_train_loss: 0.77493757 \n",
            "BATCH: 919/1074\n",
            " train loss item: 0.6781746745109558\n",
            " f1:0.29695119\n",
            " avg_train_loss: 0.77491421 \n",
            "BATCH: 920/1074\n",
            " train loss item: 0.8118683099746704\n",
            " f1:0.29702645\n",
            " avg_train_loss: 0.77492313 \n",
            "BATCH: 921/1074\n",
            " train loss item: 1.0628060102462769\n",
            " f1:0.29703098\n",
            " avg_train_loss: 0.77499261 \n",
            "BATCH: 922/1074\n",
            " train loss item: 0.6965829133987427\n",
            " f1:0.29710619\n",
            " avg_train_loss: 0.77497369 \n",
            "BATCH: 923/1074\n",
            " train loss item: 0.7255956530570984\n",
            " f1:0.29703451\n",
            " avg_train_loss: 0.77496178 \n",
            "BATCH: 924/1074\n",
            " train loss item: 1.176497220993042\n",
            " f1:0.29696287\n",
            " avg_train_loss: 0.77505863 \n",
            "BATCH: 925/1074\n",
            " train loss item: 0.5004388093948364\n",
            " f1:0.29689126\n",
            " avg_train_loss: 0.77499241 \n",
            "BATCH: 926/1074\n",
            " train loss item: 0.7720029354095459\n",
            " f1:0.29681968\n",
            " avg_train_loss: 0.77499169 \n",
            "BATCH: 927/1074\n",
            " train loss item: 0.7205294370651245\n",
            " f1:0.29674814\n",
            " avg_train_loss: 0.77497856 \n",
            "BATCH: 928/1074\n",
            " train loss item: 0.7428626418113708\n",
            " f1:0.29682331\n",
            " avg_train_loss: 0.77497082 \n",
            "BATCH: 929/1074\n",
            " train loss item: 0.9080837368965149\n",
            " f1:0.29686652\n",
            " avg_train_loss: 0.77500289 \n",
            "BATCH: 930/1074\n",
            " train loss item: 0.6380180716514587\n",
            " f1:0.29699127\n",
            " avg_train_loss: 0.77496990 \n",
            "BATCH: 931/1074\n",
            " train loss item: 0.6709034442901611\n",
            " f1:0.29710498\n",
            " avg_train_loss: 0.77494484 \n",
            "BATCH: 932/1074\n",
            " train loss item: 0.6478992700576782\n",
            " f1:0.29723980\n",
            " avg_train_loss: 0.77491425 \n",
            "BATCH: 933/1074\n",
            " train loss item: 0.7284746766090393\n",
            " f1:0.29732871\n",
            " avg_train_loss: 0.77490308 \n",
            "BATCH: 934/1074\n",
            " train loss item: 0.8122990131378174\n",
            " f1:0.29738841\n",
            " avg_train_loss: 0.77491208 \n",
            "BATCH: 935/1074\n",
            " train loss item: 0.6861463785171509\n",
            " f1:0.29749008\n",
            " avg_train_loss: 0.77489072 \n",
            "BATCH: 936/1074\n",
            " train loss item: 0.6405627131462097\n",
            " f1:0.29741853\n",
            " avg_train_loss: 0.77485842 \n",
            "BATCH: 937/1074\n",
            " train loss item: 0.808530330657959\n",
            " f1:0.29734702\n",
            " avg_train_loss: 0.77486651 \n",
            "BATCH: 938/1074\n",
            " train loss item: 0.8875145316123962\n",
            " f1:0.29727554\n",
            " avg_train_loss: 0.77489359 \n",
            "BATCH: 939/1074\n",
            " train loss item: 0.6941781640052795\n",
            " f1:0.29720410\n",
            " avg_train_loss: 0.77487419 \n",
            "BATCH: 940/1074\n",
            " train loss item: 0.8212515711784363\n",
            " f1:0.29713269\n",
            " avg_train_loss: 0.77488534 \n",
            "BATCH: 941/1074\n",
            " train loss item: 0.6542540192604065\n",
            " f1:0.29725704\n",
            " avg_train_loss: 0.77485636 \n",
            "BATCH: 942/1074\n",
            " train loss item: 1.0404483079910278\n",
            " f1:0.29731665\n",
            " avg_train_loss: 0.77492014 \n",
            "BATCH: 943/1074\n",
            " train loss item: 1.2094080448150635\n",
            " f1:0.29735959\n",
            " avg_train_loss: 0.77502446 \n",
            "BATCH: 944/1074\n",
            " train loss item: 0.8026901483535767\n",
            " f1:0.29743433\n",
            " avg_train_loss: 0.77503110 \n",
            "BATCH: 945/1074\n",
            " train loss item: 0.7534003853797913\n",
            " f1:0.29736295\n",
            " avg_train_loss: 0.77502591 \n",
            "BATCH: 946/1074\n",
            " train loss item: 0.6827670335769653\n",
            " f1:0.29729160\n",
            " avg_train_loss: 0.77500378 \n",
            "BATCH: 947/1074\n",
            " train loss item: 0.8924027681350708\n",
            " f1:0.29722029\n",
            " avg_train_loss: 0.77503194 \n",
            "BATCH: 948/1074\n",
            " train loss item: 0.8405503034591675\n",
            " f1:0.29714902\n",
            " avg_train_loss: 0.77504765 \n",
            "BATCH: 949/1074\n",
            " train loss item: 0.8437802791595459\n",
            " f1:0.29707777\n",
            " avg_train_loss: 0.77506413 \n",
            "BATCH: 950/1074\n",
            " train loss item: 0.9455149173736572\n",
            " f1:0.29710244\n",
            " avg_train_loss: 0.77510498 \n",
            "BATCH: 951/1074\n",
            " train loss item: 1.0336679220199585\n",
            " f1:0.29714536\n",
            " avg_train_loss: 0.77516694 \n",
            "BATCH: 952/1074\n",
            " train loss item: 0.6625564098358154\n",
            " f1:0.29725846\n",
            " avg_train_loss: 0.77513996 \n",
            "BATCH: 953/1074\n",
            " train loss item: 0.7041624784469604\n",
            " f1:0.29733306\n",
            " avg_train_loss: 0.77512296 \n",
            "BATCH: 954/1074\n",
            " train loss item: 0.7187454700469971\n",
            " f1:0.29726186\n",
            " avg_train_loss: 0.77510946 \n",
            "BATCH: 955/1074\n",
            " train loss item: 0.6210864782333374\n",
            " f1:0.29719069\n",
            " avg_train_loss: 0.77507259 \n",
            "BATCH: 956/1074\n",
            " train loss item: 0.9023786783218384\n",
            " f1:0.29711956\n",
            " avg_train_loss: 0.77510306 \n",
            "BATCH: 957/1074\n",
            " train loss item: 0.7213044762611389\n",
            " f1:0.29704846\n",
            " avg_train_loss: 0.77509019 \n",
            "BATCH: 958/1074\n",
            " train loss item: 0.6858031749725342\n",
            " f1:0.29697740\n",
            " avg_train_loss: 0.77506883 \n",
            "BATCH: 959/1074\n",
            " train loss item: 0.6770342588424683\n",
            " f1:0.29709035\n",
            " avg_train_loss: 0.77504538 \n",
            "BATCH: 960/1074\n",
            " train loss item: 0.7580223083496094\n",
            " f1:0.29717872\n",
            " avg_train_loss: 0.77504131 \n",
            "BATCH: 961/1074\n",
            " train loss item: 0.8567696809768677\n",
            " f1:0.29725319\n",
            " avg_train_loss: 0.77506085 \n",
            "BATCH: 962/1074\n",
            " train loss item: 0.8552365303039551\n",
            " f1:0.29729596\n",
            " avg_train_loss: 0.77508001 \n",
            "BATCH: 963/1074\n",
            " train loss item: 0.6682345867156982\n",
            " f1:0.29722492\n",
            " avg_train_loss: 0.77505448 \n",
            "BATCH: 964/1074\n",
            " train loss item: 1.0045429468154907\n",
            " f1:0.29715392\n",
            " avg_train_loss: 0.77510930 \n",
            "BATCH: 965/1074\n",
            " train loss item: 0.8079084157943726\n",
            " f1:0.29708295\n",
            " avg_train_loss: 0.77511713 \n",
            "BATCH: 966/1074\n",
            " train loss item: 0.5633273720741272\n",
            " f1:0.29701201\n",
            " avg_train_loss: 0.77506656 \n",
            "BATCH: 967/1074\n",
            " train loss item: 0.8126480579376221\n",
            " f1:0.29694111\n",
            " avg_train_loss: 0.77507553 \n",
            "BATCH: 968/1074\n",
            " train loss item: 0.6651495695114136\n",
            " f1:0.29687024\n",
            " avg_train_loss: 0.77504930 \n",
            "BATCH: 969/1074\n",
            " train loss item: 0.6937305927276611\n",
            " f1:0.29695847\n",
            " avg_train_loss: 0.77502990 \n",
            "BATCH: 970/1074\n",
            " train loss item: 0.659756600856781\n",
            " f1:0.29708201\n",
            " avg_train_loss: 0.77500240 \n",
            "BATCH: 971/1074\n",
            " train loss item: 0.697973370552063\n",
            " f1:0.29718287\n",
            " avg_train_loss: 0.77498403 \n",
            "BATCH: 972/1074\n",
            " train loss item: 0.5731476545333862\n",
            " f1:0.29731639\n",
            " avg_train_loss: 0.77493590 \n",
            "BATCH: 973/1074\n",
            " train loss item: 0.5624640583992004\n",
            " f1:0.29744984\n",
            " avg_train_loss: 0.77488525 \n",
            "BATCH: 974/1074\n",
            " train loss item: 0.9022372364997864\n",
            " f1:0.29753783\n",
            " avg_train_loss: 0.77491560 \n",
            "BATCH: 975/1074\n",
            " train loss item: 1.0276598930358887\n",
            " f1:0.29758040\n",
            " avg_train_loss: 0.77497582 \n",
            "BATCH: 976/1074\n",
            " train loss item: 0.7015009522438049\n",
            " f1:0.29762294\n",
            " avg_train_loss: 0.77495832 \n",
            "BATCH: 977/1074\n",
            " train loss item: 0.7946306467056274\n",
            " f1:0.29755206\n",
            " avg_train_loss: 0.77496301 \n",
            "BATCH: 978/1074\n",
            " train loss item: 0.8763465881347656\n",
            " f1:0.29748122\n",
            " avg_train_loss: 0.77498715 \n",
            "BATCH: 979/1074\n",
            " train loss item: 1.4958794116973877\n",
            " f1:0.29741041\n",
            " avg_train_loss: 0.77515875 \n",
            "BATCH: 980/1074\n",
            " train loss item: 1.2298845052719116\n",
            " f1:0.29733963\n",
            " avg_train_loss: 0.77526696 \n",
            "BATCH: 981/1074\n",
            " train loss item: 0.6977845430374146\n",
            " f1:0.29741371\n",
            " avg_train_loss: 0.77524853 \n",
            "BATCH: 982/1074\n",
            " train loss item: 0.7333190441131592\n",
            " f1:0.29752594\n",
            " avg_train_loss: 0.77523855 \n",
            "BATCH: 983/1074\n",
            " train loss item: 1.329319953918457\n",
            " f1:0.29759994\n",
            " avg_train_loss: 0.77537032 \n",
            "BATCH: 984/1074\n",
            " train loss item: 0.9050388336181641\n",
            " f1:0.29771207\n",
            " avg_train_loss: 0.77540115 \n",
            "BATCH: 985/1074\n",
            " train loss item: 0.680374026298523\n",
            " f1:0.29783498\n",
            " avg_train_loss: 0.77537856 \n",
            "BATCH: 986/1074\n",
            " train loss item: 1.0438112020492554\n",
            " f1:0.29783925\n",
            " avg_train_loss: 0.77544235 \n",
            "BATCH: 987/1074\n",
            " train loss item: 0.7568339705467224\n",
            " f1:0.29776849\n",
            " avg_train_loss: 0.77543793 \n",
            "BATCH: 988/1074\n",
            " train loss item: 0.6055776476860046\n",
            " f1:0.29769776\n",
            " avg_train_loss: 0.77539759 \n",
            "BATCH: 989/1074\n",
            " train loss item: 1.707062005996704\n",
            " f1:0.29762707\n",
            " avg_train_loss: 0.77561883 \n",
            "BATCH: 990/1074\n",
            " train loss item: 0.970624566078186\n",
            " f1:0.29755640\n",
            " avg_train_loss: 0.77566513 \n",
            "BATCH: 991/1074\n",
            " train loss item: 0.9796596765518188\n",
            " f1:0.29748578\n",
            " avg_train_loss: 0.77571355 \n",
            "BATCH: 992/1074\n",
            " train loss item: 0.7059999108314514\n",
            " f1:0.29741518\n",
            " avg_train_loss: 0.77569701 \n",
            "BATCH: 993/1074\n",
            " train loss item: 0.5641883611679077\n",
            " f1:0.29754797\n",
            " avg_train_loss: 0.77564683 \n",
            "BATCH: 994/1074\n",
            " train loss item: 1.0544087886810303\n",
            " f1:0.29764818\n",
            " avg_train_loss: 0.77571295 \n",
            "BATCH: 995/1074\n",
            " train loss item: 0.7305586934089661\n",
            " f1:0.29778085\n",
            " avg_train_loss: 0.77570224 \n",
            "BATCH: 996/1074\n",
            " train loss item: 1.2697033882141113\n",
            " f1:0.29788095\n",
            " avg_train_loss: 0.77581936 \n",
            "BATCH: 997/1074\n",
            " train loss item: 1.202995777130127\n",
            " f1:0.29796836\n",
            " avg_train_loss: 0.77592061 \n",
            "BATCH: 998/1074\n",
            " train loss item: 0.8081949949264526\n",
            " f1:0.29805573\n",
            " avg_train_loss: 0.77592825 \n",
            "BATCH: 999/1074\n",
            " train loss item: 0.7587558031082153\n",
            " f1:0.29798512\n",
            " avg_train_loss: 0.77592419 \n",
            "BATCH: 1000/1074\n",
            " train loss item: 1.2480309009552002\n",
            " f1:0.29791454\n",
            " avg_train_loss: 0.77603601 \n",
            "BATCH: 1001/1074\n",
            " train loss item: 1.3487458229064941\n",
            " f1:0.29784399\n",
            " avg_train_loss: 0.77617162 \n",
            "BATCH: 1002/1074\n",
            " train loss item: 0.6282653212547302\n",
            " f1:0.29777348\n",
            " avg_train_loss: 0.77613661 \n",
            "BATCH: 1003/1074\n",
            " train loss item: 0.7513521909713745\n",
            " f1:0.29781571\n",
            " avg_train_loss: 0.77613074 \n",
            "BATCH: 1004/1074\n",
            " train loss item: 0.6616395115852356\n",
            " f1:0.29792726\n",
            " avg_train_loss: 0.77610365 \n",
            "BATCH: 1005/1074\n",
            " train loss item: 0.8138787150382996\n",
            " f1:0.29800078\n",
            " avg_train_loss: 0.77611259 \n",
            "BATCH: 1006/1074\n",
            " train loss item: 0.7303134799003601\n",
            " f1:0.29808798\n",
            " avg_train_loss: 0.77610175 \n",
            "BATCH: 1007/1074\n",
            " train loss item: 0.7158790230751038\n",
            " f1:0.29813009\n",
            " avg_train_loss: 0.77608751 \n",
            "BATCH: 1008/1074\n",
            " train loss item: 0.7877519130706787\n",
            " f1:0.29805961\n",
            " avg_train_loss: 0.77609027 \n",
            "BATCH: 1009/1074\n",
            " train loss item: 0.8653171062469482\n",
            " f1:0.29798917\n",
            " avg_train_loss: 0.77611136 \n",
            "BATCH: 1010/1074\n",
            " train loss item: 0.7131277322769165\n",
            " f1:0.29791875\n",
            " avg_train_loss: 0.77609648 \n",
            "BATCH: 1011/1074\n",
            " train loss item: 0.7064703106880188\n",
            " f1:0.29784837\n",
            " avg_train_loss: 0.77608003 \n",
            "BATCH: 1012/1074\n",
            " train loss item: 0.6745550632476807\n",
            " f1:0.29795971\n",
            " avg_train_loss: 0.77605605 \n",
            "BATCH: 1013/1074\n",
            " train loss item: 0.9137504696846008\n",
            " f1:0.29800179\n",
            " avg_train_loss: 0.77608856 \n",
            "BATCH: 1014/1074\n",
            " train loss item: 0.6641839742660522\n",
            " f1:0.29811303\n",
            " avg_train_loss: 0.77606215 \n",
            "BATCH: 1015/1074\n",
            " train loss item: 0.7232585549354553\n",
            " f1:0.29820002\n",
            " avg_train_loss: 0.77604968 \n",
            "BATCH: 1016/1074\n",
            " train loss item: 0.7032763957977295\n",
            " f1:0.29827328\n",
            " avg_train_loss: 0.77603251 \n",
            "BATCH: 1017/1074\n",
            " train loss item: 0.6869285106658936\n",
            " f1:0.29820292\n",
            " avg_train_loss: 0.77601149 \n",
            "BATCH: 1018/1074\n",
            " train loss item: 0.668813169002533\n",
            " f1:0.29813259\n",
            " avg_train_loss: 0.77598621 \n",
            "BATCH: 1019/1074\n",
            " train loss item: 0.6903005838394165\n",
            " f1:0.29806229\n",
            " avg_train_loss: 0.77596600 \n",
            "BATCH: 1020/1074\n",
            " train loss item: 0.6285049915313721\n",
            " f1:0.29799203\n",
            " avg_train_loss: 0.77593124 \n",
            "BATCH: 1021/1074\n",
            " train loss item: 0.9358524084091187\n",
            " f1:0.29792180\n",
            " avg_train_loss: 0.77596893 \n",
            "BATCH: 1022/1074\n",
            " train loss item: 0.6857467889785767\n",
            " f1:0.29785160\n",
            " avg_train_loss: 0.77594767 \n",
            "BATCH: 1023/1074\n",
            " train loss item: 0.706666111946106\n",
            " f1:0.29793848\n",
            " avg_train_loss: 0.77593135 \n",
            "BATCH: 1024/1074\n",
            " train loss item: 0.8592376112937927\n",
            " f1:0.29799677\n",
            " avg_train_loss: 0.77595097 \n",
            "BATCH: 1025/1074\n",
            " train loss item: 0.7768687009811401\n",
            " f1:0.29806993\n",
            " avg_train_loss: 0.77595119 \n",
            "BATCH: 1026/1074\n",
            " train loss item: 0.6881385445594788\n",
            " f1:0.29816925\n",
            " avg_train_loss: 0.77593052 \n",
            "BATCH: 1027/1074\n",
            " train loss item: 0.7219783663749695\n",
            " f1:0.29809908\n",
            " avg_train_loss: 0.77591782 \n",
            "BATCH: 1028/1074\n",
            " train loss item: 0.6628586053848267\n",
            " f1:0.29802894\n",
            " avg_train_loss: 0.77589122 \n",
            "BATCH: 1029/1074\n",
            " train loss item: 0.5926381349563599\n",
            " f1:0.29795883\n",
            " avg_train_loss: 0.77584811 \n",
            "BATCH: 1030/1074\n",
            " train loss item: 0.6221596002578735\n",
            " f1:0.29788876\n",
            " avg_train_loss: 0.77581196 \n",
            "BATCH: 1031/1074\n",
            " train loss item: 0.7790125608444214\n",
            " f1:0.29781872\n",
            " avg_train_loss: 0.77581272 \n",
            "BATCH: 1032/1074\n",
            " train loss item: 0.8232889771461487\n",
            " f1:0.29774871\n",
            " avg_train_loss: 0.77582388 \n",
            "BATCH: 1033/1074\n",
            " train loss item: 0.726412832736969\n",
            " f1:0.29767873\n",
            " avg_train_loss: 0.77581227 \n",
            "BATCH: 1034/1074\n",
            " train loss item: 0.7303578853607178\n",
            " f1:0.29772067\n",
            " avg_train_loss: 0.77580159 \n",
            "BATCH: 1035/1074\n",
            " train loss item: 0.7370206117630005\n",
            " f1:0.29779372\n",
            " avg_train_loss: 0.77579248 \n",
            "BATCH: 1036/1074\n",
            " train loss item: 0.647456169128418\n",
            " f1:0.29791515\n",
            " avg_train_loss: 0.77576234 \n",
            "BATCH: 1037/1074\n",
            " train loss item: 0.7950336337089539\n",
            " f1:0.29795701\n",
            " avg_train_loss: 0.77576686 \n",
            "BATCH: 1038/1074\n",
            " train loss item: 0.6949074864387512\n",
            " f1:0.29801510\n",
            " avg_train_loss: 0.77574788 \n",
            "BATCH: 1039/1074\n",
            " train loss item: 0.6986166834831238\n",
            " f1:0.29794516\n",
            " avg_train_loss: 0.77572978 \n",
            "BATCH: 1040/1074\n",
            " train loss item: 0.6861980557441711\n",
            " f1:0.29787526\n",
            " avg_train_loss: 0.77570877 \n",
            "BATCH: 1041/1074\n",
            " train loss item: 0.9240907430648804\n",
            " f1:0.29780538\n",
            " avg_train_loss: 0.77574358 \n",
            "BATCH: 1042/1074\n",
            " train loss item: 0.8069981932640076\n",
            " f1:0.29773554\n",
            " avg_train_loss: 0.77575091 \n",
            "BATCH: 1043/1074\n",
            " train loss item: 0.6969605684280396\n",
            " f1:0.29780845\n",
            " avg_train_loss: 0.77573243 \n",
            "BATCH: 1044/1074\n",
            " train loss item: 0.5836747288703918\n",
            " f1:0.29793956\n",
            " avg_train_loss: 0.77568741 \n",
            "BATCH: 1045/1074\n",
            " train loss item: 1.0670677423477173\n",
            " f1:0.29799757\n",
            " avg_train_loss: 0.77575570 \n",
            "BATCH: 1046/1074\n",
            " train loss item: 0.8687256574630737\n",
            " f1:0.29808395\n",
            " avg_train_loss: 0.77577748 \n",
            "BATCH: 1047/1074\n",
            " train loss item: 0.8227246999740601\n",
            " f1:0.29814190\n",
            " avg_train_loss: 0.77578848 \n",
            "BATCH: 1048/1074\n",
            " train loss item: 0.6635570526123047\n",
            " f1:0.29807207\n",
            " avg_train_loss: 0.77576220 \n",
            "BATCH: 1049/1074\n",
            " train loss item: 1.2444361448287964\n",
            " f1:0.29800228\n",
            " avg_train_loss: 0.77587193 \n",
            "BATCH: 1050/1074\n",
            " train loss item: 1.0385525226593018\n",
            " f1:0.29793253\n",
            " avg_train_loss: 0.77593342 \n",
            "BATCH: 1051/1074\n",
            " train loss item: 0.6911152601242065\n",
            " f1:0.29786280\n",
            " avg_train_loss: 0.77591357 \n",
            "BATCH: 1052/1074\n",
            " train loss item: 0.6439697742462158\n",
            " f1:0.29798376\n",
            " avg_train_loss: 0.77588270 \n",
            "BATCH: 1053/1074\n",
            " train loss item: 0.7204577922821045\n",
            " f1:0.29809399\n",
            " avg_train_loss: 0.77586973 \n",
            "BATCH: 1054/1074\n",
            " train loss item: 1.1653592586517334\n",
            " f1:0.29816663\n",
            " avg_train_loss: 0.77596082 \n",
            "BATCH: 1055/1074\n",
            " train loss item: 0.677356481552124\n",
            " f1:0.29828742\n",
            " avg_train_loss: 0.77593777 \n",
            "BATCH: 1056/1074\n",
            " train loss item: 0.9026333093643188\n",
            " f1:0.29835998\n",
            " avg_train_loss: 0.77596738 \n",
            "BATCH: 1057/1074\n",
            " train loss item: 0.7027565240859985\n",
            " f1:0.29843251\n",
            " avg_train_loss: 0.77595027 \n",
            "BATCH: 1058/1074\n",
            " train loss item: 0.7933897376060486\n",
            " f1:0.29836278\n",
            " avg_train_loss: 0.77595435 \n",
            "BATCH: 1059/1074\n",
            " train loss item: 1.1236517429351807\n",
            " f1:0.29829309\n",
            " avg_train_loss: 0.77603557 \n",
            "BATCH: 1060/1074\n",
            " train loss item: 0.8616464734077454\n",
            " f1:0.29822342\n",
            " avg_train_loss: 0.77605556 \n",
            "BATCH: 1061/1074\n",
            " train loss item: 0.8113634586334229\n",
            " f1:0.29815379\n",
            " avg_train_loss: 0.77606380 \n",
            "BATCH: 1062/1074\n",
            " train loss item: 0.8408897519111633\n",
            " f1:0.29821152\n",
            " avg_train_loss: 0.77607894 \n",
            "BATCH: 1063/1074\n",
            " train loss item: 1.109509825706482\n",
            " f1:0.29825306\n",
            " avg_train_loss: 0.77615675 \n",
            "BATCH: 1064/1074\n",
            " train loss item: 0.7957319021224976\n",
            " f1:0.29833901\n",
            " avg_train_loss: 0.77616132 \n",
            "BATCH: 1065/1074\n",
            " train loss item: 0.7481334209442139\n",
            " f1:0.29838050\n",
            " avg_train_loss: 0.77615478 \n",
            "BATCH: 1066/1074\n",
            " train loss item: 0.621086835861206\n",
            " f1:0.29831092\n",
            " avg_train_loss: 0.77611861 \n",
            "BATCH: 1067/1074\n",
            " train loss item: 1.1354814767837524\n",
            " f1:0.29824136\n",
            " avg_train_loss: 0.77620240 \n",
            "BATCH: 1068/1074\n",
            " train loss item: 0.7238994836807251\n",
            " f1:0.29817184\n",
            " avg_train_loss: 0.77619021 \n",
            "BATCH: 1069/1074\n",
            " train loss item: 1.1028214693069458\n",
            " f1:0.29810235\n",
            " avg_train_loss: 0.77626633 \n",
            "BATCH: 1070/1074\n",
            " train loss item: 0.621149480342865\n",
            " f1:0.29803290\n",
            " avg_train_loss: 0.77623019 \n",
            "BATCH: 1071/1074\n",
            " train loss item: 0.6932075023651123\n",
            " f1:0.29811877\n",
            " avg_train_loss: 0.77621085 \n",
            "BATCH: 1072/1074\n",
            " train loss item: 0.7116259932518005\n",
            " f1:0.29821702\n",
            " avg_train_loss: 0.77619581 \n",
            "BATCH: 1073/1074\n",
            " train loss item: 0.9238048791885376\n",
            " f1:0.29828931\n",
            " avg_train_loss: 0.77623018 \n",
            "BATCH: 1074/1074\n",
            " train loss item: 0.9723953008651733\n",
            " f1:0.29833626\n",
            " avg_train_loss: 0.77627584 \n",
            "\n",
            "\n",
            "\n",
            "VALIDATION FOR EPOCH 4\n",
            "BATCH: 1/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.42298294\n",
            " avg_val_loss: 0.70140289 \n",
            "BATCH: 2/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.42338643\n",
            " avg_val_loss: 0.70136619 \n",
            "BATCH: 3/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.42349977\n",
            " avg_val_loss: 0.70137618 \n",
            "BATCH: 4/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.42377482\n",
            " avg_val_loss: 0.70136286 \n",
            "BATCH: 5/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.42404936\n",
            " avg_val_loss: 0.70134956 \n",
            "BATCH: 6/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.42427400\n",
            " avg_val_loss: 0.70134404 \n",
            "BATCH: 7/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.42444461\n",
            " avg_val_loss: 0.70134627 \n",
            "BATCH: 8/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.42455644\n",
            " avg_val_loss: 0.70135623 \n",
            "BATCH: 9/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.42472647\n",
            " avg_val_loss: 0.70135844 \n",
            "BATCH: 10/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.42489618\n",
            " avg_val_loss: 0.70136065 \n",
            "BATCH: 11/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.42500730\n",
            " avg_val_loss: 0.70137056 \n",
            "BATCH: 12/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.42527893\n",
            " avg_val_loss: 0.70135735 \n",
            "BATCH: 13/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.42538949\n",
            " avg_val_loss: 0.70136725 \n",
            "BATCH: 14/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.42555797\n",
            " avg_val_loss: 0.70136944 \n",
            "BATCH: 15/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.42572614\n",
            " avg_val_loss: 0.70137163 \n",
            "BATCH: 16/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.42577244\n",
            " avg_val_loss: 0.70138917 \n",
            "BATCH: 17/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.42604212\n",
            " avg_val_loss: 0.70137600 \n",
            "BATCH: 18/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.42639814\n",
            " avg_val_loss: 0.70134753 \n",
            "BATCH: 19/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.42666676\n",
            " avg_val_loss: 0.70133442 \n",
            "BATCH: 20/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.42702156\n",
            " avg_val_loss: 0.70130603 \n",
            "BATCH: 21/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.42724041\n",
            " avg_val_loss: 0.70130063 \n",
            "BATCH: 22/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.42750753\n",
            " avg_val_loss: 0.70128760 \n",
            "BATCH: 23/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.42772554\n",
            " avg_val_loss: 0.70128222 \n",
            "BATCH: 24/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.42807808\n",
            " avg_val_loss: 0.70125399 \n",
            "BATCH: 25/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.42838850\n",
            " avg_val_loss: 0.70123342 \n",
            "BATCH: 26/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.42849493\n",
            " avg_val_loss: 0.70124332 \n",
            "BATCH: 27/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.42880440\n",
            " avg_val_loss: 0.70122280 \n",
            "BATCH: 28/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.42884740\n",
            " avg_val_loss: 0.70124028 \n",
            "BATCH: 29/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.42895312\n",
            " avg_val_loss: 0.70125014 \n",
            "BATCH: 30/358\n",
            "val loss item: 0.7455884218215942\n",
            " f1_val:0.42876587\n",
            " avg_val_loss: 0.70129030 \n",
            "BATCH: 31/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.42902943\n",
            " avg_val_loss: 0.70127739 \n",
            "BATCH: 32/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.42913469\n",
            " avg_val_loss: 0.70128719 \n",
            "BATCH: 33/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.42929690\n",
            " avg_val_loss: 0.70128942 \n",
            "BATCH: 34/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.42940173\n",
            " avg_val_loss: 0.70129919 \n",
            "BATCH: 35/358\n",
            "val loss item: 0.6786516308784485\n",
            " f1_val:0.42970816\n",
            " avg_val_loss: 0.70127877 \n",
            "BATCH: 36/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.43005511\n",
            " avg_val_loss: 0.70125085 \n",
            "BATCH: 37/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43026808\n",
            " avg_val_loss: 0.70124557 \n",
            "BATCH: 38/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.43037166\n",
            " avg_val_loss: 0.70125535 \n",
            "BATCH: 39/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43058397\n",
            " avg_val_loss: 0.70125008 \n",
            "BATCH: 40/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.43055651\n",
            " avg_val_loss: 0.70127485 \n",
            "BATCH: 41/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.43086026\n",
            " avg_val_loss: 0.70125456 \n",
            "BATCH: 42/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.43101961\n",
            " avg_val_loss: 0.70125680 \n",
            "BATCH: 43/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.43127832\n",
            " avg_val_loss: 0.70124406 \n",
            "BATCH: 44/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.43143701\n",
            " avg_val_loss: 0.70124630 \n",
            "BATCH: 45/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.43153890\n",
            " avg_val_loss: 0.70125602 \n",
            "BATCH: 46/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.43184041\n",
            " avg_val_loss: 0.70123584 \n",
            "BATCH: 47/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.43218205\n",
            " avg_val_loss: 0.70120823 \n",
            "BATCH: 48/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.43243857\n",
            " avg_val_loss: 0.70119558 \n",
            "BATCH: 49/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43264714\n",
            " avg_val_loss: 0.70119041 \n",
            "BATCH: 50/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43285535\n",
            " avg_val_loss: 0.70118524 \n",
            "BATCH: 51/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.43295544\n",
            " avg_val_loss: 0.70119496 \n",
            "BATCH: 52/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.43305535\n",
            " avg_val_loss: 0.70120466 \n",
            "BATCH: 53/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43326263\n",
            " avg_val_loss: 0.70119950 \n",
            "BATCH: 54/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.43351683\n",
            " avg_val_loss: 0.70118693 \n",
            "BATCH: 55/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.43361598\n",
            " avg_val_loss: 0.70119661 \n",
            "BATCH: 56/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.43377092\n",
            " avg_val_loss: 0.70119887 \n",
            "BATCH: 57/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.43402399\n",
            " avg_val_loss: 0.70118634 \n",
            "BATCH: 58/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43422951\n",
            " avg_val_loss: 0.70118121 \n",
            "BATCH: 59/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.43432768\n",
            " avg_val_loss: 0.70119087 \n",
            "BATCH: 60/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.43466320\n",
            " avg_val_loss: 0.70116361 \n",
            "BATCH: 61/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.43481654\n",
            " avg_val_loss: 0.70116589 \n",
            "BATCH: 62/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.43491393\n",
            " avg_val_loss: 0.70117554 \n",
            "BATCH: 63/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.43495023\n",
            " avg_val_loss: 0.70119252 \n",
            "BATCH: 64/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43515385\n",
            " avg_val_loss: 0.70118742 \n",
            "BATCH: 65/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.43518988\n",
            " avg_val_loss: 0.70120436 \n",
            "BATCH: 66/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.43548290\n",
            " avg_val_loss: 0.70118458 \n",
            "BATCH: 67/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.43563470\n",
            " avg_val_loss: 0.70118683 \n",
            "BATCH: 68/358\n",
            "val loss item: 0.737221360206604\n",
            " f1_val:0.43552976\n",
            " avg_val_loss: 0.70121839 \n",
            "BATCH: 69/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43573198\n",
            " avg_val_loss: 0.70121328 \n",
            "BATCH: 70/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.43602350\n",
            " avg_val_loss: 0.70119356 \n",
            "BATCH: 71/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.43617430\n",
            " avg_val_loss: 0.70119580 \n",
            "BATCH: 72/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.43632485\n",
            " avg_val_loss: 0.70119803 \n",
            "BATCH: 73/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.43669173\n",
            " avg_val_loss: 0.70116378 \n",
            "BATCH: 74/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.43693852\n",
            " avg_val_loss: 0.70115146 \n",
            "BATCH: 75/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.43718487\n",
            " avg_val_loss: 0.70113916 \n",
            "BATCH: 76/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.43727902\n",
            " avg_val_loss: 0.70114871 \n",
            "BATCH: 77/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.43737300\n",
            " avg_val_loss: 0.70115824 \n",
            "BATCH: 78/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.43766107\n",
            " avg_val_loss: 0.70113870 \n",
            "BATCH: 79/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43785969\n",
            " avg_val_loss: 0.70113371 \n",
            "BATCH: 80/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.43814684\n",
            " avg_val_loss: 0.70111423 \n",
            "BATCH: 81/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.43839087\n",
            " avg_val_loss: 0.70110203 \n",
            "BATCH: 82/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43858834\n",
            " avg_val_loss: 0.70109708 \n",
            "BATCH: 83/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43878547\n",
            " avg_val_loss: 0.70109215 \n",
            "BATCH: 84/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.43907083\n",
            " avg_val_loss: 0.70107277 \n",
            "BATCH: 85/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.43921718\n",
            " avg_val_loss: 0.70107508 \n",
            "BATCH: 86/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.43930877\n",
            " avg_val_loss: 0.70108460 \n",
            "BATCH: 87/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.43955053\n",
            " avg_val_loss: 0.70107249 \n",
            "BATCH: 88/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43974599\n",
            " avg_val_loss: 0.70106759 \n",
            "BATCH: 89/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.43994110\n",
            " avg_val_loss: 0.70106271 \n",
            "BATCH: 90/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44018170\n",
            " avg_val_loss: 0.70105064 \n",
            "BATCH: 91/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.44046415\n",
            " avg_val_loss: 0.70103142 \n",
            "BATCH: 92/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.44042945\n",
            " avg_val_loss: 0.70105528 \n",
            "BATCH: 93/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44062331\n",
            " avg_val_loss: 0.70105042 \n",
            "BATCH: 94/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44076720\n",
            " avg_val_loss: 0.70105273 \n",
            "BATCH: 95/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44091086\n",
            " avg_val_loss: 0.70105504 \n",
            "BATCH: 96/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.44094101\n",
            " avg_val_loss: 0.70107165 \n",
            "BATCH: 97/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.44108427\n",
            " avg_val_loss: 0.70107394 \n",
            "BATCH: 98/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.44117332\n",
            " avg_val_loss: 0.70108337 \n",
            "BATCH: 99/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.44113822\n",
            " avg_val_loss: 0.70110704 \n",
            "BATCH: 100/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44137575\n",
            " avg_val_loss: 0.70109504 \n",
            "BATCH: 101/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44151815\n",
            " avg_val_loss: 0.70109730 \n",
            "BATCH: 102/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44170960\n",
            " avg_val_loss: 0.70109245 \n",
            "BATCH: 103/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44190073\n",
            " avg_val_loss: 0.70108760 \n",
            "BATCH: 104/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44213681\n",
            " avg_val_loss: 0.70107565 \n",
            "BATCH: 105/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.44222444\n",
            " avg_val_loss: 0.70108502 \n",
            "BATCH: 106/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44245984\n",
            " avg_val_loss: 0.70107310 \n",
            "BATCH: 107/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44269485\n",
            " avg_val_loss: 0.70106120 \n",
            "BATCH: 108/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.44292945\n",
            " avg_val_loss: 0.70104932 \n",
            "BATCH: 109/358\n",
            "val loss item: 0.6786516308784485\n",
            " f1_val:0.44320528\n",
            " avg_val_loss: 0.70103039 \n",
            "BATCH: 110/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.44323314\n",
            " avg_val_loss: 0.70104682 \n",
            "BATCH: 111/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.44337277\n",
            " avg_val_loss: 0.70104910 \n",
            "BATCH: 112/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.44340044\n",
            " avg_val_loss: 0.70106549 \n",
            "BATCH: 113/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44353970\n",
            " avg_val_loss: 0.70106776 \n",
            "BATCH: 114/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44372751\n",
            " avg_val_loss: 0.70106298 \n",
            "BATCH: 115/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44386626\n",
            " avg_val_loss: 0.70106524 \n",
            "BATCH: 116/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.44382940\n",
            " avg_val_loss: 0.70108859 \n",
            "BATCH: 117/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44401650\n",
            " avg_val_loss: 0.70108380 \n",
            "BATCH: 118/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.44415465\n",
            " avg_val_loss: 0.70108604 \n",
            "BATCH: 119/358\n",
            "val loss item: 0.6786516308784485\n",
            " f1_val:0.44442714\n",
            " avg_val_loss: 0.70106724 \n",
            "BATCH: 120/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44461327\n",
            " avg_val_loss: 0.70106248 \n",
            "BATCH: 121/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44475057\n",
            " avg_val_loss: 0.70106473 \n",
            "BATCH: 122/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44498072\n",
            " avg_val_loss: 0.70105298 \n",
            "BATCH: 123/358\n",
            "val loss item: 0.6786516308784485\n",
            " f1_val:0.44525160\n",
            " avg_val_loss: 0.70103427 \n",
            "BATCH: 124/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44548094\n",
            " avg_val_loss: 0.70102257 \n",
            "BATCH: 125/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.44561707\n",
            " avg_val_loss: 0.70102485 \n",
            "BATCH: 126/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44580127\n",
            " avg_val_loss: 0.70102015 \n",
            "BATCH: 127/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.44582658\n",
            " avg_val_loss: 0.70103636 \n",
            "BATCH: 128/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.44609563\n",
            " avg_val_loss: 0.70101773 \n",
            "BATCH: 129/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.44612065\n",
            " avg_val_loss: 0.70103392 \n",
            "BATCH: 130/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.44620315\n",
            " avg_val_loss: 0.70104313 \n",
            "BATCH: 131/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.44650905\n",
            " avg_val_loss: 0.70101760 \n",
            "BATCH: 132/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.44664354\n",
            " avg_val_loss: 0.70101987 \n",
            "BATCH: 133/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.44691080\n",
            " avg_val_loss: 0.70100134 \n",
            "BATCH: 134/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.44693504\n",
            " avg_val_loss: 0.70101747 \n",
            "BATCH: 135/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44711679\n",
            " avg_val_loss: 0.70101281 \n",
            "BATCH: 136/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44729823\n",
            " avg_val_loss: 0.70100816 \n",
            "BATCH: 137/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.44760171\n",
            " avg_val_loss: 0.70098279 \n",
            "BATCH: 138/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.44773463\n",
            " avg_val_loss: 0.70098507 \n",
            "BATCH: 139/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.44781519\n",
            " avg_val_loss: 0.70099425 \n",
            "BATCH: 140/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44794771\n",
            " avg_val_loss: 0.70099653 \n",
            "BATCH: 141/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44817162\n",
            " avg_val_loss: 0.70098502 \n",
            "BATCH: 142/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.44847314\n",
            " avg_val_loss: 0.70095977 \n",
            "BATCH: 143/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44865243\n",
            " avg_val_loss: 0.70095519 \n",
            "BATCH: 144/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.44895305\n",
            " avg_val_loss: 0.70093001 \n",
            "BATCH: 145/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44913165\n",
            " avg_val_loss: 0.70092546 \n",
            "BATCH: 146/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.44926244\n",
            " avg_val_loss: 0.70092778 \n",
            "BATCH: 147/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.44934122\n",
            " avg_val_loss: 0.70093695 \n",
            "BATCH: 148/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.44956271\n",
            " avg_val_loss: 0.70092556 \n",
            "BATCH: 149/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.44974023\n",
            " avg_val_loss: 0.70092103 \n",
            "BATCH: 150/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45000125\n",
            " avg_val_loss: 0.70090284 \n",
            "BATCH: 151/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.45022166\n",
            " avg_val_loss: 0.70089150 \n",
            "BATCH: 152/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45029934\n",
            " avg_val_loss: 0.70090066 \n",
            "BATCH: 153/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.45059641\n",
            " avg_val_loss: 0.70087571 \n",
            "BATCH: 154/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.45055521\n",
            " avg_val_loss: 0.70089849 \n",
            "BATCH: 155/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.45051408\n",
            " avg_val_loss: 0.70092124 \n",
            "BATCH: 156/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.45047301\n",
            " avg_val_loss: 0.70094395 \n",
            "BATCH: 157/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45064863\n",
            " avg_val_loss: 0.70093943 \n",
            "BATCH: 158/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45082397\n",
            " avg_val_loss: 0.70093493 \n",
            "BATCH: 159/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45095201\n",
            " avg_val_loss: 0.70093721 \n",
            "BATCH: 160/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.45117004\n",
            " avg_val_loss: 0.70092593 \n",
            "BATCH: 161/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.45138772\n",
            " avg_val_loss: 0.70091467 \n",
            "BATCH: 162/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.45140778\n",
            " avg_val_loss: 0.70093051 \n",
            "BATCH: 163/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.45162492\n",
            " avg_val_loss: 0.70091926 \n",
            "BATCH: 164/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45179862\n",
            " avg_val_loss: 0.70091479 \n",
            "BATCH: 165/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45187421\n",
            " avg_val_loss: 0.70092383 \n",
            "BATCH: 166/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45204743\n",
            " avg_val_loss: 0.70091937 \n",
            "BATCH: 167/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.45226334\n",
            " avg_val_loss: 0.70090817 \n",
            "BATCH: 168/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.45255525\n",
            " avg_val_loss: 0.70088351 \n",
            "BATCH: 169/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45268087\n",
            " avg_val_loss: 0.70088582 \n",
            "BATCH: 170/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45280628\n",
            " avg_val_loss: 0.70088812 \n",
            "BATCH: 171/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45288070\n",
            " avg_val_loss: 0.70089714 \n",
            "BATCH: 172/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45295499\n",
            " avg_val_loss: 0.70090615 \n",
            "BATCH: 173/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.45324518\n",
            " avg_val_loss: 0.70088160 \n",
            "BATCH: 174/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.45326356\n",
            " avg_val_loss: 0.70089731 \n",
            "BATCH: 175/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45351654\n",
            " avg_val_loss: 0.70087949 \n",
            "BATCH: 176/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.45347373\n",
            " avg_val_loss: 0.70090187 \n",
            "BATCH: 177/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45359781\n",
            " avg_val_loss: 0.70090415 \n",
            "BATCH: 178/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45384991\n",
            " avg_val_loss: 0.70088638 \n",
            "BATCH: 179/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.45406232\n",
            " avg_val_loss: 0.70087531 \n",
            "BATCH: 180/358\n",
            "val loss item: 0.737221360206604\n",
            " f1_val:0.45395205\n",
            " avg_val_loss: 0.70090430 \n",
            "BATCH: 181/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45407536\n",
            " avg_val_loss: 0.70090657 \n",
            "BATCH: 182/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.45409296\n",
            " avg_val_loss: 0.70092216 \n",
            "BATCH: 183/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.45430450\n",
            " avg_val_loss: 0.70091109 \n",
            "BATCH: 184/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.45459108\n",
            " avg_val_loss: 0.70088675 \n",
            "BATCH: 185/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.45454772\n",
            " avg_val_loss: 0.70090896 \n",
            "BATCH: 186/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45479747\n",
            " avg_val_loss: 0.70089130 \n",
            "BATCH: 187/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45496548\n",
            " avg_val_loss: 0.70088693 \n",
            "BATCH: 188/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45513323\n",
            " avg_val_loss: 0.70088257 \n",
            "BATCH: 189/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45530072\n",
            " avg_val_loss: 0.70087822 \n",
            "BATCH: 190/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.45551013\n",
            " avg_val_loss: 0.70086726 \n",
            "BATCH: 191/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.45571922\n",
            " avg_val_loss: 0.70085631 \n",
            "BATCH: 192/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45584005\n",
            " avg_val_loss: 0.70085860 \n",
            "BATCH: 193/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.45612338\n",
            " avg_val_loss: 0.70083447 \n",
            "BATCH: 194/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45628942\n",
            " avg_val_loss: 0.70083017 \n",
            "BATCH: 195/358\n",
            "val loss item: 0.6535502672195435\n",
            " f1_val:0.45663636\n",
            " avg_val_loss: 0.70079291 \n",
            "BATCH: 196/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45680173\n",
            " avg_val_loss: 0.70078865 \n",
            "BATCH: 197/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45692124\n",
            " avg_val_loss: 0.70079099 \n",
            "BATCH: 198/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45699084\n",
            " avg_val_loss: 0.70079989 \n",
            "BATCH: 199/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45723612\n",
            " avg_val_loss: 0.70078249 \n",
            "BATCH: 200/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.45735501\n",
            " avg_val_loss: 0.70078483 \n",
            "BATCH: 201/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45747371\n",
            " avg_val_loss: 0.70078715 \n",
            "BATCH: 202/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45771803\n",
            " avg_val_loss: 0.70076981 \n",
            "BATCH: 203/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45796197\n",
            " avg_val_loss: 0.70075249 \n",
            "BATCH: 204/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45803043\n",
            " avg_val_loss: 0.70076138 \n",
            "BATCH: 205/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.45823525\n",
            " avg_val_loss: 0.70075064 \n",
            "BATCH: 206/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.45843976\n",
            " avg_val_loss: 0.70073991 \n",
            "BATCH: 207/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.45868237\n",
            " avg_val_loss: 0.70072266 \n",
            "BATCH: 208/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.45888621\n",
            " avg_val_loss: 0.70071198 \n",
            "BATCH: 209/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45904816\n",
            " avg_val_loss: 0.70070782 \n",
            "BATCH: 210/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.45932524\n",
            " avg_val_loss: 0.70068413 \n",
            "BATCH: 211/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45939226\n",
            " avg_val_loss: 0.70069303 \n",
            "BATCH: 212/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.45940533\n",
            " avg_val_loss: 0.70070842 \n",
            "BATCH: 213/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.45947219\n",
            " avg_val_loss: 0.70071729 \n",
            "BATCH: 214/358\n",
            "val loss item: 0.712119996547699\n",
            " f1_val:0.45953894\n",
            " avg_val_loss: 0.70072614 \n",
            "BATCH: 215/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.45960559\n",
            " avg_val_loss: 0.70073498 \n",
            "BATCH: 216/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.45976611\n",
            " avg_val_loss: 0.70073083 \n",
            "BATCH: 217/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.45977883\n",
            " avg_val_loss: 0.70074614 \n",
            "BATCH: 218/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.45989409\n",
            " avg_val_loss: 0.70074846 \n",
            "BATCH: 219/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46005401\n",
            " avg_val_loss: 0.70074432 \n",
            "BATCH: 220/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46021368\n",
            " avg_val_loss: 0.70074018 \n",
            "BATCH: 221/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.46045230\n",
            " avg_val_loss: 0.70072312 \n",
            "BATCH: 222/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.46069056\n",
            " avg_val_loss: 0.70070609 \n",
            "BATCH: 223/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46075591\n",
            " avg_val_loss: 0.70071489 \n",
            "BATCH: 224/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.46102868\n",
            " avg_val_loss: 0.70069144 \n",
            "BATCH: 225/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46118699\n",
            " avg_val_loss: 0.70068736 \n",
            "BATCH: 226/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.46113992\n",
            " avg_val_loss: 0.70070903 \n",
            "BATCH: 227/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.46109293\n",
            " avg_val_loss: 0.70073066 \n",
            "BATCH: 228/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.46129178\n",
            " avg_val_loss: 0.70072013 \n",
            "BATCH: 229/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46144940\n",
            " avg_val_loss: 0.70071604 \n",
            "BATCH: 230/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46164767\n",
            " avg_val_loss: 0.70070553 \n",
            "BATCH: 231/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46180477\n",
            " avg_val_loss: 0.70070145 \n",
            "BATCH: 232/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46200247\n",
            " avg_val_loss: 0.70069098 \n",
            "BATCH: 233/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46201333\n",
            " avg_val_loss: 0.70070612 \n",
            "BATCH: 234/358\n",
            "val loss item: 0.6786516308784485\n",
            " f1_val:0.46224820\n",
            " avg_val_loss: 0.70068926 \n",
            "BATCH: 235/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46240437\n",
            " avg_val_loss: 0.70068521 \n",
            "BATCH: 236/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46251604\n",
            " avg_val_loss: 0.70068755 \n",
            "BATCH: 237/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46267176\n",
            " avg_val_loss: 0.70068351 \n",
            "BATCH: 238/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46282725\n",
            " avg_val_loss: 0.70067947 \n",
            "BATCH: 239/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46289018\n",
            " avg_val_loss: 0.70068818 \n",
            "BATCH: 240/358\n",
            "val loss item: 0.737221360206604\n",
            " f1_val:0.46277823\n",
            " avg_val_loss: 0.70071599 \n",
            "BATCH: 241/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46297383\n",
            " avg_val_loss: 0.70070557 \n",
            "BATCH: 242/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46303651\n",
            " avg_val_loss: 0.70071424 \n",
            "BATCH: 243/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46319112\n",
            " avg_val_loss: 0.70071020 \n",
            "BATCH: 244/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46320099\n",
            " avg_val_loss: 0.70072520 \n",
            "BATCH: 245/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46331129\n",
            " avg_val_loss: 0.70072750 \n",
            "BATCH: 246/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46346535\n",
            " avg_val_loss: 0.70072345 \n",
            "BATCH: 247/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46347498\n",
            " avg_val_loss: 0.70073841 \n",
            "BATCH: 248/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46362868\n",
            " avg_val_loss: 0.70073437 \n",
            "BATCH: 249/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46369053\n",
            " avg_val_loss: 0.70074297 \n",
            "BATCH: 250/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46384384\n",
            " avg_val_loss: 0.70073893 \n",
            "BATCH: 251/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46395316\n",
            " avg_val_loss: 0.70074120 \n",
            "BATCH: 252/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.46390493\n",
            " avg_val_loss: 0.70076240 \n",
            "BATCH: 253/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.46409792\n",
            " avg_val_loss: 0.70075204 \n",
            "BATCH: 254/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46410702\n",
            " avg_val_loss: 0.70076690 \n",
            "BATCH: 255/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.46421582\n",
            " avg_val_loss: 0.70076915 \n",
            "BATCH: 256/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46427690\n",
            " avg_val_loss: 0.70077769 \n",
            "BATCH: 257/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46442896\n",
            " avg_val_loss: 0.70077364 \n",
            "BATCH: 258/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46458079\n",
            " avg_val_loss: 0.70076959 \n",
            "BATCH: 259/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.46487528\n",
            " avg_val_loss: 0.70074044 \n",
            "BATCH: 260/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.46498310\n",
            " avg_val_loss: 0.70074270 \n",
            "BATCH: 261/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.46524514\n",
            " avg_val_loss: 0.70071989 \n",
            "BATCH: 262/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.46519631\n",
            " avg_val_loss: 0.70074095 \n",
            "BATCH: 263/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.46514754\n",
            " avg_val_loss: 0.70076197 \n",
            "BATCH: 264/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46520756\n",
            " avg_val_loss: 0.70077046 \n",
            "BATCH: 265/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.46543462\n",
            " avg_val_loss: 0.70075394 \n",
            "BATCH: 266/358\n",
            "val loss item: 0.737221360206604\n",
            " f1_val:0.46532294\n",
            " avg_val_loss: 0.70078116 \n",
            "BATCH: 267/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46547308\n",
            " avg_val_loss: 0.70077713 \n",
            "BATCH: 268/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46553268\n",
            " avg_val_loss: 0.70078559 \n",
            "BATCH: 269/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.46582428\n",
            " avg_val_loss: 0.70075664 \n",
            "BATCH: 270/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46593058\n",
            " avg_val_loss: 0.70075887 \n",
            "BATCH: 271/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.46588156\n",
            " avg_val_loss: 0.70077976 \n",
            "BATCH: 272/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.46614080\n",
            " avg_val_loss: 0.70075711 \n",
            "BATCH: 273/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46619968\n",
            " avg_val_loss: 0.70076554 \n",
            "BATCH: 274/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46620709\n",
            " avg_val_loss: 0.70078017 \n",
            "BATCH: 275/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.46649689\n",
            " avg_val_loss: 0.70075136 \n",
            "BATCH: 276/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46655538\n",
            " avg_val_loss: 0.70075979 \n",
            "BATCH: 277/358\n",
            "val loss item: 0.6870187520980835\n",
            " f1_val:0.46674297\n",
            " avg_val_loss: 0.70074961 \n",
            "BATCH: 278/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46689085\n",
            " avg_val_loss: 0.70074565 \n",
            "BATCH: 279/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46703850\n",
            " avg_val_loss: 0.70074169 \n",
            "BATCH: 280/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46718594\n",
            " avg_val_loss: 0.70073773 \n",
            "BATCH: 281/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46729037\n",
            " avg_val_loss: 0.70073996 \n",
            "BATCH: 282/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.46739465\n",
            " avg_val_loss: 0.70074218 \n",
            "BATCH: 283/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.46768187\n",
            " avg_val_loss: 0.70071357 \n",
            "BATCH: 284/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46778570\n",
            " avg_val_loss: 0.70071580 \n",
            "BATCH: 285/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46779189\n",
            " avg_val_loss: 0.70073035 \n",
            "BATCH: 286/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46793812\n",
            " avg_val_loss: 0.70072642 \n",
            "BATCH: 287/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46812332\n",
            " avg_val_loss: 0.70071635 \n",
            "BATCH: 288/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46812925\n",
            " avg_val_loss: 0.70073087 \n",
            "BATCH: 289/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46827491\n",
            " avg_val_loss: 0.70072695 \n",
            "BATCH: 290/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46833149\n",
            " avg_val_loss: 0.70073530 \n",
            "BATCH: 291/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46838799\n",
            " avg_val_loss: 0.70074364 \n",
            "BATCH: 292/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46857219\n",
            " avg_val_loss: 0.70073359 \n",
            "BATCH: 293/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.46885644\n",
            " avg_val_loss: 0.70070520 \n",
            "BATCH: 294/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46900104\n",
            " avg_val_loss: 0.70070131 \n",
            "BATCH: 295/358\n",
            "val loss item: 0.737221360206604\n",
            " f1_val:0.46888912\n",
            " avg_val_loss: 0.70072798 \n",
            "BATCH: 296/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46899117\n",
            " avg_val_loss: 0.70073019 \n",
            "BATCH: 297/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46917426\n",
            " avg_val_loss: 0.70072019 \n",
            "BATCH: 298/358\n",
            "val loss item: 0.737221360206604\n",
            " f1_val:0.46906246\n",
            " avg_val_loss: 0.70074680 \n",
            "BATCH: 299/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.46906765\n",
            " avg_val_loss: 0.70076117 \n",
            "BATCH: 300/358\n",
            "val loss item: 0.737221360206604\n",
            " f1_val:0.46895609\n",
            " avg_val_loss: 0.70078771 \n",
            "BATCH: 301/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46901173\n",
            " avg_val_loss: 0.70079595 \n",
            "BATCH: 302/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.46896158\n",
            " avg_val_loss: 0.70081634 \n",
            "BATCH: 303/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46914388\n",
            " avg_val_loss: 0.70080632 \n",
            "BATCH: 304/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46919926\n",
            " avg_val_loss: 0.70081453 \n",
            "BATCH: 305/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.46925456\n",
            " avg_val_loss: 0.70082273 \n",
            "BATCH: 306/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.46950497\n",
            " avg_val_loss: 0.70080060 \n",
            "BATCH: 307/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.46964773\n",
            " avg_val_loss: 0.70079668 \n",
            "BATCH: 308/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.46974835\n",
            " avg_val_loss: 0.70079882 \n",
            "BATCH: 309/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.46992930\n",
            " avg_val_loss: 0.70078886 \n",
            "BATCH: 310/358\n",
            "val loss item: 0.6535502672195435\n",
            " f1_val:0.47023755\n",
            " avg_val_loss: 0.70075472 \n",
            "BATCH: 311/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.47045343\n",
            " avg_val_loss: 0.70073876 \n",
            "BATCH: 312/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.47050754\n",
            " avg_val_loss: 0.70074698 \n",
            "BATCH: 313/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.47072291\n",
            " avg_val_loss: 0.70073105 \n",
            "BATCH: 314/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47072685\n",
            " avg_val_loss: 0.70074528 \n",
            "BATCH: 315/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47086792\n",
            " avg_val_loss: 0.70074142 \n",
            "BATCH: 316/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.47092158\n",
            " avg_val_loss: 0.70074961 \n",
            "BATCH: 317/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.47097516\n",
            " avg_val_loss: 0.70075778 \n",
            "BATCH: 318/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47097891\n",
            " avg_val_loss: 0.70077195 \n",
            "BATCH: 319/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.47092795\n",
            " avg_val_loss: 0.70079211 \n",
            "BATCH: 320/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47093173\n",
            " avg_val_loss: 0.70080624 \n",
            "BATCH: 321/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.47103048\n",
            " avg_val_loss: 0.70080835 \n",
            "BATCH: 322/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.47130707\n",
            " avg_val_loss: 0.70078050 \n",
            "BATCH: 323/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47144691\n",
            " avg_val_loss: 0.70077663 \n",
            "BATCH: 324/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47145030\n",
            " avg_val_loss: 0.70079073 \n",
            "BATCH: 325/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.47139923\n",
            " avg_val_loss: 0.70081079 \n",
            "BATCH: 326/358\n",
            "val loss item: 0.6619173884391785\n",
            " f1_val:0.47167476\n",
            " avg_val_loss: 0.70078301 \n",
            "BATCH: 327/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47167799\n",
            " avg_val_loss: 0.70079708 \n",
            "BATCH: 328/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.47177572\n",
            " avg_val_loss: 0.70079918 \n",
            "BATCH: 329/358\n",
            "val loss item: 0.6786516308784485\n",
            " f1_val:0.47198773\n",
            " avg_val_loss: 0.70078340 \n",
            "BATCH: 330/358\n",
            "val loss item: 0.728854238986969\n",
            " f1_val:0.47193646\n",
            " avg_val_loss: 0.70080339 \n",
            "BATCH: 331/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.47203379\n",
            " avg_val_loss: 0.70080549 \n",
            "BATCH: 332/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47217222\n",
            " avg_val_loss: 0.70080164 \n",
            "BATCH: 333/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.47234836\n",
            " avg_val_loss: 0.70079184 \n",
            "BATCH: 334/358\n",
            "val loss item: 0.6786515712738037\n",
            " f1_val:0.47255922\n",
            " avg_val_loss: 0.70077612 \n",
            "BATCH: 335/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.47280212\n",
            " avg_val_loss: 0.70075448 \n",
            "BATCH: 336/358\n",
            "val loss item: 0.703752875328064\n",
            " f1_val:0.47289850\n",
            " avg_val_loss: 0.70075660 \n",
            "BATCH: 337/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.47294992\n",
            " avg_val_loss: 0.70076466 \n",
            "BATCH: 338/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47308712\n",
            " avg_val_loss: 0.70076085 \n",
            "BATCH: 339/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.47326186\n",
            " avg_val_loss: 0.70075112 \n",
            "BATCH: 340/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.47343636\n",
            " avg_val_loss: 0.70074141 \n",
            "BATCH: 341/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47357292\n",
            " avg_val_loss: 0.70073762 \n",
            "BATCH: 342/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47357477\n",
            " avg_val_loss: 0.70075157 \n",
            "BATCH: 343/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.47362549\n",
            " avg_val_loss: 0.70075959 \n",
            "BATCH: 344/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47362730\n",
            " avg_val_loss: 0.70077351 \n",
            "BATCH: 345/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47376334\n",
            " avg_val_loss: 0.70076971 \n",
            "BATCH: 346/358\n",
            "val loss item: 0.7121200561523438\n",
            " f1_val:0.47381383\n",
            " avg_val_loss: 0.70077770 \n",
            "BATCH: 347/358\n",
            "val loss item: 0.720487117767334\n",
            " f1_val:0.47381550\n",
            " avg_val_loss: 0.70079157 \n",
            "BATCH: 348/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.47405530\n",
            " avg_val_loss: 0.70077012 \n",
            "BATCH: 349/358\n",
            "val loss item: 0.7037529349327087\n",
            " f1_val:0.47414992\n",
            " avg_val_loss: 0.70077222 \n",
            "BATCH: 350/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.47438915\n",
            " avg_val_loss: 0.70075081 \n",
            "BATCH: 351/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.47456151\n",
            " avg_val_loss: 0.70074117 \n",
            "BATCH: 352/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47469622\n",
            " avg_val_loss: 0.70073741 \n",
            "BATCH: 353/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47483075\n",
            " avg_val_loss: 0.70073366 \n",
            "BATCH: 354/358\n",
            "val loss item: 0.6702845096588135\n",
            " f1_val:0.47506884\n",
            " avg_val_loss: 0.70071234 \n",
            "BATCH: 355/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.47524024\n",
            " avg_val_loss: 0.70070276 \n",
            "BATCH: 356/358\n",
            "val loss item: 0.6953858137130737\n",
            " f1_val:0.47537410\n",
            " avg_val_loss: 0.70069904 \n",
            "BATCH: 357/358\n",
            "val loss item: 0.6870186924934387\n",
            " f1_val:0.47554505\n",
            " avg_val_loss: 0.70068948 \n",
            "BATCH: 358/358\n",
            "val loss item: 0.6842297315597534\n",
            " f1_val:0.47572752\n",
            " avg_val_loss: 0.70067799 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss_data_BERTBUPO, model_BERTBUPO = bert_train(BERT_EPOCHS, bert_bupo, bert_trainLoader, bert_valLoader,\n",
        "                                                optimizer_fn(bert_bupo,bupo_LR,weight_decay=0.000),\n",
        "                                                bert_bupo_ckpt, bert_bupo_best,\n",
        "                                                BERT_BUPO_ATTEMPT[bert_bupo_attempt],\n",
        "                                                save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoIJfmWUq8Fs",
        "outputId": "996ef28b-662c-4eab-d71c-fb2fb161db60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69305285 \n",
            "BATCH: 184/358\n",
            "val loss item: 0.7255589962005615\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69322951 \n",
            "BATCH: 185/358\n",
            "val loss item: 0.657071590423584\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69303407 \n",
            "BATCH: 186/358\n",
            "val loss item: 0.7157750129699707\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69315633 \n",
            "BATCH: 187/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69317264 \n",
            "BATCH: 188/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69318879 \n",
            "BATCH: 189/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69320476 \n",
            "BATCH: 190/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69327205 \n",
            "BATCH: 191/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69333865 \n",
            "BATCH: 192/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69330263 \n",
            "BATCH: 193/358\n",
            "val loss item: 0.7255590558052063\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69346976 \n",
            "BATCH: 194/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69348387 \n",
            "BATCH: 195/358\n",
            "val loss item: 0.7451268434524536\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69374871 \n",
            "BATCH: 196/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69376125 \n",
            "BATCH: 197/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69372400 \n",
            "BATCH: 198/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69363772 \n",
            "BATCH: 199/358\n",
            "val loss item: 0.7157751321792603\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69374896 \n",
            "BATCH: 200/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69371233 \n",
            "BATCH: 201/358\n",
            "val loss item: 0.6864232420921326\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69367607 \n",
            "BATCH: 202/358\n",
            "val loss item: 0.7157751321792603\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69378547 \n",
            "BATCH: 203/358\n",
            "val loss item: 0.7157750129699707\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69389379 \n",
            "BATCH: 204/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69380921 \n",
            "BATCH: 205/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69386863 \n",
            "BATCH: 206/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69392748 \n",
            "BATCH: 207/358\n",
            "val loss item: 0.7157750725746155\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69403303 \n",
            "BATCH: 208/358\n",
            "val loss item: 0.7059910893440247\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69409052 \n",
            "BATCH: 209/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69410064 \n",
            "BATCH: 210/358\n",
            "val loss item: 0.7255589365959167\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69425045 \n",
            "BATCH: 211/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69416698 \n",
            "BATCH: 212/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69403815 \n",
            "BATCH: 213/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69395647 \n",
            "BATCH: 214/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69387555 \n",
            "BATCH: 215/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69379538 \n",
            "BATCH: 216/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69380654 \n",
            "BATCH: 217/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69368235 \n",
            "BATCH: 218/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69364905 \n",
            "BATCH: 219/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69366073 \n",
            "BATCH: 220/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69367230 \n",
            "BATCH: 221/358\n",
            "val loss item: 0.7157750129699707\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69377232 \n",
            "BATCH: 222/358\n",
            "val loss item: 0.7157751321792603\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69387143 \n",
            "BATCH: 223/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69379415 \n",
            "BATCH: 224/358\n",
            "val loss item: 0.7255589962005615\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69393596 \n",
            "BATCH: 225/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69394606 \n",
            "BATCH: 226/358\n",
            "val loss item: 0.657071590423584\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69378290 \n",
            "BATCH: 227/358\n",
            "val loss item: 0.6570715308189392\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69362117 \n",
            "BATCH: 228/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69367543 \n",
            "BATCH: 229/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69368648 \n",
            "BATCH: 230/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69373998 \n",
            "BATCH: 231/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69375066 \n",
            "BATCH: 232/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69380342 \n",
            "BATCH: 233/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69368776 \n",
            "BATCH: 234/358\n",
            "val loss item: 0.7157751321792603\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69378215 \n",
            "BATCH: 235/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69379247 \n",
            "BATCH: 236/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69376125 \n",
            "BATCH: 237/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69377157 \n",
            "BATCH: 238/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69378180 \n",
            "BATCH: 239/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69371008 \n",
            "BATCH: 240/358\n",
            "val loss item: 0.6472876071929932\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69351665 \n",
            "BATCH: 241/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69356841 \n",
            "BATCH: 242/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69349846 \n",
            "BATCH: 243/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69350960 \n",
            "BATCH: 244/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69340037 \n",
            "BATCH: 245/358\n",
            "val loss item: 0.6864233613014221\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69337189 \n",
            "BATCH: 246/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69338341 \n",
            "BATCH: 247/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69327601 \n",
            "BATCH: 248/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69328783 \n",
            "BATCH: 249/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69322097 \n",
            "BATCH: 250/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69323292 \n",
            "BATCH: 251/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69320579 \n",
            "BATCH: 252/358\n",
            "val loss item: 0.6570715308189392\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69306240 \n",
            "BATCH: 253/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69311350 \n",
            "BATCH: 254/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69301012 \n",
            "BATCH: 255/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69298429 \n",
            "BATCH: 256/358\n",
            "val loss item: 0.6766393184661865\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69292044 \n",
            "BATCH: 257/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69293323 \n",
            "BATCH: 258/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69294592 \n",
            "BATCH: 259/358\n",
            "val loss item: 0.7353428602218628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69310962 \n",
            "BATCH: 260/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69308390 \n",
            "BATCH: 261/358\n",
            "val loss item: 0.7255589962005615\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69320833 \n",
            "BATCH: 262/358\n",
            "val loss item: 0.6570715308189392\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69307040 \n",
            "BATCH: 263/358\n",
            "val loss item: 0.657071590423584\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69293352 \n",
            "BATCH: 264/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69287180 \n",
            "BATCH: 265/358\n",
            "val loss item: 0.7157751321792603\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69295823 \n",
            "BATCH: 266/358\n",
            "val loss item: 0.6472876667976379\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278653 \n",
            "BATCH: 267/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279935 \n",
            "BATCH: 268/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273905 \n",
            "BATCH: 269/358\n",
            "val loss item: 0.7353429198265076\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69289743 \n",
            "BATCH: 270/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69287345 \n",
            "BATCH: 271/358\n",
            "val loss item: 0.657071590423584\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274134 \n",
            "BATCH: 272/358\n",
            "val loss item: 0.7255589962005615\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69286199 \n",
            "BATCH: 273/358\n",
            "val loss item: 0.6766393184661865\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69280257 \n",
            "BATCH: 274/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270787 \n",
            "BATCH: 275/358\n",
            "val loss item: 0.7353429198265076\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69286291 \n",
            "BATCH: 276/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69280413 \n",
            "BATCH: 277/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69285173 \n",
            "BATCH: 278/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69286380 \n",
            "BATCH: 279/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69287579 \n",
            "BATCH: 280/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69288768 \n",
            "BATCH: 281/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69286468 \n",
            "BATCH: 282/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69284184 \n",
            "BATCH: 283/358\n",
            "val loss item: 0.7353428602218628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69299202 \n",
            "BATCH: 284/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69296889 \n",
            "BATCH: 285/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69287726 \n",
            "BATCH: 286/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69288891 \n",
            "BATCH: 287/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69293456 \n",
            "BATCH: 288/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69284401 \n",
            "BATCH: 289/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69285564 \n",
            "BATCH: 290/358\n",
            "val loss item: 0.6766393184661865\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279973 \n",
            "BATCH: 291/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274419 \n",
            "BATCH: 292/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278956 \n",
            "BATCH: 293/358\n",
            "val loss item: 0.7353429198265076\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69293479 \n",
            "BATCH: 294/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69294592 \n",
            "BATCH: 295/358\n",
            "val loss item: 0.6472876071929932\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279115 \n",
            "BATCH: 296/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276964 \n",
            "BATCH: 297/358\n",
            "val loss item: 0.7059910893440247\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69281415 \n",
            "BATCH: 298/358\n",
            "val loss item: 0.6472876667976379\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69266138 \n",
            "BATCH: 299/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69257507 \n",
            "BATCH: 300/358\n",
            "val loss item: 0.6472876667976379\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69242411 \n",
            "BATCH: 301/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69237167 \n",
            "BATCH: 302/358\n",
            "val loss item: 0.6570715308189392\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69225478 \n",
            "BATCH: 303/358\n",
            "val loss item: 0.7059910893440247\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69230012 \n",
            "BATCH: 304/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69224860 \n",
            "BATCH: 305/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69219743 \n",
            "BATCH: 306/358\n",
            "val loss item: 0.7255589365959167\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69230645 \n",
            "BATCH: 307/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69231916 \n",
            "BATCH: 308/358\n",
            "val loss item: 0.6864233613014221\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69230001 \n",
            "BATCH: 309/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69234432 \n",
            "BATCH: 310/358\n",
            "val loss item: 0.7451267838478088\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69251459 \n",
            "BATCH: 311/358\n",
            "val loss item: 0.7157750725746155\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69258938 \n",
            "BATCH: 312/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69253826 \n",
            "BATCH: 313/358\n",
            "val loss item: 0.7157750725746155\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69261250 \n",
            "BATCH: 314/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69253047 \n",
            "BATCH: 315/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69254214 \n",
            "BATCH: 316/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69249182 \n",
            "BATCH: 317/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69244181 \n",
            "BATCH: 318/358\n",
            "val loss item: 0.66685551404953\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69236135 \n",
            "BATCH: 319/358\n",
            "val loss item: 0.657071590423584\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69225072 \n",
            "BATCH: 320/358\n",
            "val loss item: 0.66685551404953\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69217136 \n",
            "BATCH: 321/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69215346 \n",
            "BATCH: 322/358\n",
            "val loss item: 0.7353429198265076\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69228758 \n",
            "BATCH: 323/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69229972 \n",
            "BATCH: 324/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69222119 \n",
            "BATCH: 325/358\n",
            "val loss item: 0.6570715308189392\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69211304 \n",
            "BATCH: 326/358\n",
            "val loss item: 0.7353429198265076\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69224564 \n",
            "BATCH: 327/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69216800 \n",
            "BATCH: 328/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69215048 \n",
            "BATCH: 329/358\n",
            "val loss item: 0.7157750725746155\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69222229 \n",
            "BATCH: 330/358\n",
            "val loss item: 0.657071590423584\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69211577 \n",
            "BATCH: 331/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69209857 \n",
            "BATCH: 332/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69211095 \n",
            "BATCH: 333/358\n",
            "val loss item: 0.7059910893440247\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69215263 \n",
            "BATCH: 334/358\n",
            "val loss item: 0.7157750725746155\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69222336 \n",
            "BATCH: 335/358\n",
            "val loss item: 0.7255589962005615\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69232287 \n",
            "BATCH: 336/358\n",
            "val loss item: 0.6864233016967773\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69230531 \n",
            "BATCH: 337/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69225882 \n",
            "BATCH: 338/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69227050 \n",
            "BATCH: 339/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69231098 \n",
            "BATCH: 340/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69235121 \n",
            "BATCH: 341/358\n",
            "val loss item: 0.6962072253227234\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69236252 \n",
            "BATCH: 342/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69228794 \n",
            "BATCH: 343/358\n",
            "val loss item: 0.6766394376754761\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69224232 \n",
            "BATCH: 344/358\n",
            "val loss item: 0.6668553948402405\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69216852 \n",
            "BATCH: 345/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69218022 \n",
            "BATCH: 346/358\n",
            "val loss item: 0.6766393780708313\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69213531 \n",
            "BATCH: 347/358\n",
            "val loss item: 0.6668554544448853\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69206246 \n",
            "BATCH: 348/358\n",
            "val loss item: 0.7255589365959167\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69215871 \n",
            "BATCH: 349/358\n",
            "val loss item: 0.6864233613014221\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69214228 \n",
            "BATCH: 350/358\n",
            "val loss item: 0.7255589365959167\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69223775 \n",
            "BATCH: 351/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69227694 \n",
            "BATCH: 352/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69228810 \n",
            "BATCH: 353/358\n",
            "val loss item: 0.6962072849273682\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69229921 \n",
            "BATCH: 354/358\n",
            "val loss item: 0.7255589962005615\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69239316 \n",
            "BATCH: 355/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69243146 \n",
            "BATCH: 356/358\n",
            "val loss item: 0.6962071657180786\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69244207 \n",
            "BATCH: 357/358\n",
            "val loss item: 0.7059911489486694\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69248002 \n",
            "BATCH: 358/358\n",
            "val loss item: 0.7092527151107788\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69252687 \n",
            "SAVED\n",
            "\n",
            "\n",
            "EPOCH 2\n",
            "====================\n",
            "BATCH: 1/1074\n",
            " train loss item: 0.7059911489486694\n",
            " f1:0.36040873\n",
            " avg_train_loss: 0.70686875 \n",
            "BATCH: 2/1074\n",
            " train loss item: 7.143495082855225\n",
            " f1:0.36063948\n",
            " avg_train_loss: 0.71285074 \n",
            "BATCH: 3/1074\n",
            " train loss item: 2.6874618530273438\n",
            " f1:0.36030462\n",
            " avg_train_loss: 0.71468418 \n",
            "BATCH: 4/1074\n",
            " train loss item: 2.0265674591064453\n",
            " f1:0.35997039\n",
            " avg_train_loss: 0.71590114 \n",
            "BATCH: 5/1074\n",
            " train loss item: 3.1699352264404297\n",
            " f1:0.36025463\n",
            " avg_train_loss: 0.71817550 \n",
            "BATCH: 6/1074\n",
            " train loss item: 3.4271109104156494\n",
            " f1:0.36048467\n",
            " avg_train_loss: 0.72068377 \n",
            "BATCH: 7/1074\n",
            " train loss item: 0.70313560962677\n",
            " f1:0.36081725\n",
            " avg_train_loss: 0.72066754 \n",
            "BATCH: 8/1074\n",
            " train loss item: 2.1219449043273926\n",
            " f1:0.36048377\n",
            " avg_train_loss: 0.72196262 \n",
            "BATCH: 9/1074\n",
            " train loss item: 1.442509412765503\n",
            " f1:0.36015092\n",
            " avg_train_loss: 0.72262794 \n",
            "BATCH: 10/1074\n",
            " train loss item: 0.5712769031524658\n",
            " f1:0.36060940\n",
            " avg_train_loss: 0.72248832 \n",
            "BATCH: 11/1074\n",
            " train loss item: 2.7881979942321777\n",
            " f1:0.36083805\n",
            " avg_train_loss: 0.72439220 \n",
            "BATCH: 12/1074\n",
            " train loss item: 1.4004476070404053\n",
            " f1:0.36121410\n",
            " avg_train_loss: 0.72501472 \n",
            "BATCH: 13/1074\n",
            " train loss item: 0.662727952003479\n",
            " f1:0.36158946\n",
            " avg_train_loss: 0.72495742 \n",
            "BATCH: 14/1074\n",
            " train loss item: 1.1984153985977173\n",
            " f1:0.36125712\n",
            " avg_train_loss: 0.72539258 \n",
            "BATCH: 15/1074\n",
            " train loss item: 1.358516812324524\n",
            " f1:0.36092538\n",
            " avg_train_loss: 0.72597396 \n",
            "BATCH: 16/1074\n",
            " train loss item: 0.826469898223877\n",
            " f1:0.36103113\n",
            " avg_train_loss: 0.72606616 \n",
            "BATCH: 17/1074\n",
            " train loss item: 0.9182214140892029\n",
            " f1:0.36131127\n",
            " avg_train_loss: 0.72624229 \n",
            "BATCH: 18/1074\n",
            " train loss item: 0.7724684476852417\n",
            " f1:0.36163974\n",
            " avg_train_loss: 0.72628462 \n",
            "BATCH: 19/1074\n",
            " train loss item: 0.7666960954666138\n",
            " f1:0.36174455\n",
            " avg_train_loss: 0.72632159 \n",
            "BATCH: 20/1074\n",
            " train loss item: 0.8328814506530762\n",
            " f1:0.36141389\n",
            " avg_train_loss: 0.72641900 \n",
            "BATCH: 21/1074\n",
            " train loss item: 1.0721642971038818\n",
            " f1:0.36108383\n",
            " avg_train_loss: 0.72673475 \n",
            "BATCH: 22/1074\n",
            " train loss item: 0.7815032601356506\n",
            " f1:0.36075437\n",
            " avg_train_loss: 0.72678472 \n",
            "BATCH: 23/1074\n",
            " train loss item: 0.7693508863449097\n",
            " f1:0.36079015\n",
            " avg_train_loss: 0.72682352 \n",
            "BATCH: 24/1074\n",
            " train loss item: 0.7262448072433472\n",
            " f1:0.36106872\n",
            " avg_train_loss: 0.72682299 \n",
            "BATCH: 25/1074\n",
            " train loss item: 0.7195690870285034\n",
            " f1:0.36134679\n",
            " avg_train_loss: 0.72681639 \n",
            "BATCH: 26/1074\n",
            " train loss item: 0.6859247088432312\n",
            " f1:0.36167284\n",
            " avg_train_loss: 0.72677922 \n",
            "BATCH: 27/1074\n",
            " train loss item: 0.6932475566864014\n",
            " f1:0.36134435\n",
            " avg_train_loss: 0.72674876 \n",
            "BATCH: 28/1074\n",
            " train loss item: 0.648875892162323\n",
            " f1:0.36101645\n",
            " avg_train_loss: 0.72667810 \n",
            "BATCH: 29/1074\n",
            " train loss item: 0.667733907699585\n",
            " f1:0.36068914\n",
            " avg_train_loss: 0.72662466 \n",
            "BATCH: 30/1074\n",
            " train loss item: 0.8435379266738892\n",
            " f1:0.36036243\n",
            " avg_train_loss: 0.72673056 \n",
            "BATCH: 31/1074\n",
            " train loss item: 0.7960556745529175\n",
            " f1:0.36003631\n",
            " avg_train_loss: 0.72679330 \n",
            "BATCH: 32/1074\n",
            " train loss item: 0.6459051370620728\n",
            " f1:0.35971078\n",
            " avg_train_loss: 0.72672016 \n",
            "BATCH: 33/1074\n",
            " train loss item: 0.693374752998352\n",
            " f1:0.35993570\n",
            " avg_train_loss: 0.72669004 \n",
            "BATCH: 34/1074\n",
            " train loss item: 0.7188277840614319\n",
            " f1:0.36010314\n",
            " avg_train_loss: 0.72668294 \n",
            "BATCH: 35/1074\n",
            " train loss item: 0.6958459615707397\n",
            " f1:0.36032729\n",
            " avg_train_loss: 0.72665514 \n",
            "BATCH: 36/1074\n",
            " train loss item: 0.6979238390922546\n",
            " f1:0.36000268\n",
            " avg_train_loss: 0.72662925 \n",
            "BATCH: 37/1074\n",
            " train loss item: 0.7264148592948914\n",
            " f1:0.35967864\n",
            " avg_train_loss: 0.72662906 \n",
            "BATCH: 38/1074\n",
            " train loss item: 0.6721806526184082\n",
            " f1:0.35935519\n",
            " avg_train_loss: 0.72658009 \n",
            "BATCH: 39/1074\n",
            " train loss item: 0.7414074540138245\n",
            " f1:0.35903232\n",
            " avg_train_loss: 0.72659342 \n",
            "BATCH: 40/1074\n",
            " train loss item: 0.687285304069519\n",
            " f1:0.35935635\n",
            " avg_train_loss: 0.72655813 \n",
            "BATCH: 41/1074\n",
            " train loss item: 0.7511250376701355\n",
            " f1:0.35957997\n",
            " avg_train_loss: 0.72658016 \n",
            "BATCH: 42/1074\n",
            " train loss item: 0.7277239561080933\n",
            " f1:0.35985514\n",
            " avg_train_loss: 0.72658119 \n",
            "BATCH: 43/1074\n",
            " train loss item: 0.687872052192688\n",
            " f1:0.36017756\n",
            " avg_train_loss: 0.72654653 \n",
            "BATCH: 44/1074\n",
            " train loss item: 0.7440935969352722\n",
            " f1:0.36028133\n",
            " avg_train_loss: 0.72656223 \n",
            "BATCH: 45/1074\n",
            " train loss item: 0.6600052714347839\n",
            " f1:0.35995936\n",
            " avg_train_loss: 0.72650275 \n",
            "BATCH: 46/1074\n",
            " train loss item: 0.6213499307632446\n",
            " f1:0.35963797\n",
            " avg_train_loss: 0.72640886 \n",
            "BATCH: 47/1074\n",
            " train loss item: 0.6392713189125061\n",
            " f1:0.35931715\n",
            " avg_train_loss: 0.72633113 \n",
            "BATCH: 48/1074\n",
            " train loss item: 1.0514004230499268\n",
            " f1:0.35899690\n",
            " avg_train_loss: 0.72662085 \n",
            "BATCH: 49/1074\n",
            " train loss item: 0.8913226127624512\n",
            " f1:0.35867722\n",
            " avg_train_loss: 0.72676752 \n",
            "BATCH: 50/1074\n",
            " train loss item: 0.8322317600250244\n",
            " f1:0.35835812\n",
            " avg_train_loss: 0.72686135 \n",
            "BATCH: 51/1074\n",
            " train loss item: 0.666084885597229\n",
            " f1:0.35803958\n",
            " avg_train_loss: 0.72680732 \n",
            "BATCH: 52/1074\n",
            " train loss item: 0.6970245838165283\n",
            " f1:0.35820602\n",
            " avg_train_loss: 0.72678087 \n",
            "BATCH: 53/1074\n",
            " train loss item: 0.7235573530197144\n",
            " f1:0.35837217\n",
            " avg_train_loss: 0.72677801 \n",
            "BATCH: 54/1074\n",
            " train loss item: 0.6854033470153809\n",
            " f1:0.35869276\n",
            " avg_train_loss: 0.72674133 \n",
            "BATCH: 55/1074\n",
            " train loss item: 0.6727468371391296\n",
            " f1:0.35905639\n",
            " avg_train_loss: 0.72669351 \n",
            "BATCH: 56/1074\n",
            " train loss item: 0.662102460861206\n",
            " f1:0.35945971\n",
            " avg_train_loss: 0.72663635 \n",
            "BATCH: 57/1074\n",
            " train loss item: 0.6995168924331665\n",
            " f1:0.35973134\n",
            " avg_train_loss: 0.72661237 \n",
            "BATCH: 58/1074\n",
            " train loss item: 0.7251983880996704\n",
            " f1:0.35989541\n",
            " avg_train_loss: 0.72661112 \n",
            "BATCH: 59/1074\n",
            " train loss item: 0.6856833100318909\n",
            " f1:0.36025669\n",
            " avg_train_loss: 0.72657500 \n",
            "BATCH: 60/1074\n",
            " train loss item: 0.6922868490219116\n",
            " f1:0.35993900\n",
            " avg_train_loss: 0.72654476 \n",
            "BATCH: 61/1074\n",
            " train loss item: 0.7090688347816467\n",
            " f1:0.35962188\n",
            " avg_train_loss: 0.72652936 \n",
            "BATCH: 62/1074\n",
            " train loss item: 0.6853220462799072\n",
            " f1:0.35930531\n",
            " avg_train_loss: 0.72649309 \n",
            "BATCH: 63/1074\n",
            " train loss item: 0.7272193431854248\n",
            " f1:0.35898930\n",
            " avg_train_loss: 0.72649373 \n",
            "BATCH: 64/1074\n",
            " train loss item: 0.6853223443031311\n",
            " f1:0.35867384\n",
            " avg_train_loss: 0.72645755 \n",
            "BATCH: 65/1074\n",
            " train loss item: 0.6966902017593384\n",
            " f1:0.35835894\n",
            " avg_train_loss: 0.72643141 \n",
            "BATCH: 66/1074\n",
            " train loss item: 0.6971952319145203\n",
            " f1:0.35804459\n",
            " avg_train_loss: 0.72640577 \n",
            "BATCH: 67/1074\n",
            " train loss item: 0.6949310302734375\n",
            " f1:0.35831507\n",
            " avg_train_loss: 0.72637818 \n",
            "BATCH: 68/1074\n",
            " train loss item: 0.7162613272666931\n",
            " f1:0.35853432\n",
            " avg_train_loss: 0.72636932 \n",
            "BATCH: 69/1074\n",
            " train loss item: 0.729573130607605\n",
            " f1:0.35869785\n",
            " avg_train_loss: 0.72637213 \n",
            "BATCH: 70/1074\n",
            " train loss item: 0.6933408379554749\n",
            " f1:0.35896706\n",
            " avg_train_loss: 0.72634325 \n",
            "BATCH: 71/1074\n",
            " train loss item: 0.6866347789764404\n",
            " f1:0.35865355\n",
            " avg_train_loss: 0.72630857 \n",
            "BATCH: 72/1074\n",
            " train loss item: 0.6648851633071899\n",
            " f1:0.35834059\n",
            " avg_train_loss: 0.72625498 \n",
            "BATCH: 73/1074\n",
            " train loss item: 0.8028829097747803\n",
            " f1:0.35802817\n",
            " avg_train_loss: 0.72632178 \n",
            "BATCH: 74/1074\n",
            " train loss item: 0.7499721646308899\n",
            " f1:0.35771630\n",
            " avg_train_loss: 0.72634238 \n",
            "BATCH: 75/1074\n",
            " train loss item: 0.7527933716773987\n",
            " f1:0.35740497\n",
            " avg_train_loss: 0.72636541 \n",
            "BATCH: 76/1074\n",
            " train loss item: 0.6853275895118713\n",
            " f1:0.35772027\n",
            " avg_train_loss: 0.72632972 \n",
            "BATCH: 77/1074\n",
            " train loss item: 0.6251310110092163\n",
            " f1:0.35811740\n",
            " avg_train_loss: 0.72624180 \n",
            "BATCH: 78/1074\n",
            " train loss item: 0.7349241375923157\n",
            " f1:0.35843153\n",
            " avg_train_loss: 0.72624933 \n",
            "BATCH: 79/1074\n",
            " train loss item: 0.9478825330734253\n",
            " f1:0.35859374\n",
            " avg_train_loss: 0.72644156 \n",
            "BATCH: 80/1074\n",
            " train loss item: 0.8332347869873047\n",
            " f1:0.35881047\n",
            " avg_train_loss: 0.72653410 \n",
            "BATCH: 81/1074\n",
            " train loss item: 0.6935616135597229\n",
            " f1:0.35912318\n",
            " avg_train_loss: 0.72650555 \n",
            "BATCH: 82/1074\n",
            " train loss item: 0.674482524394989\n",
            " f1:0.35947795\n",
            " avg_train_loss: 0.72646055 \n",
            "BATCH: 83/1074\n",
            " train loss item: 0.6935731172561646\n",
            " f1:0.35916725\n",
            " avg_train_loss: 0.72643212 \n",
            "BATCH: 84/1074\n",
            " train loss item: 0.7013503909111023\n",
            " f1:0.35885709\n",
            " avg_train_loss: 0.72641046 \n",
            "BATCH: 85/1074\n",
            " train loss item: 0.6872403621673584\n",
            " f1:0.35854746\n",
            " avg_train_loss: 0.72637667 \n",
            "BATCH: 86/1074\n",
            " train loss item: 0.6900154948234558\n",
            " f1:0.35823837\n",
            " avg_train_loss: 0.72634532 \n",
            "BATCH: 87/1074\n",
            " train loss item: 0.7190377712249756\n",
            " f1:0.35792981\n",
            " avg_train_loss: 0.72633903 \n",
            "BATCH: 88/1074\n",
            " train loss item: 0.5990745425224304\n",
            " f1:0.35762178\n",
            " avg_train_loss: 0.72622951 \n",
            "BATCH: 89/1074\n",
            " train loss item: 0.661942720413208\n",
            " f1:0.35731428\n",
            " avg_train_loss: 0.72617423 \n",
            "BATCH: 90/1074\n",
            " train loss item: 0.661899745464325\n",
            " f1:0.35700731\n",
            " avg_train_loss: 0.72611901 \n",
            "BATCH: 91/1074\n",
            " train loss item: 0.8269072771072388\n",
            " f1:0.35670087\n",
            " avg_train_loss: 0.72620552 \n",
            "BATCH: 92/1074\n",
            " train loss item: 0.7844460010528564\n",
            " f1:0.35639495\n",
            " avg_train_loss: 0.72625547 \n",
            "BATCH: 93/1074\n",
            " train loss item: 0.6954243183135986\n",
            " f1:0.35666082\n",
            " avg_train_loss: 0.72622905 \n",
            "BATCH: 94/1074\n",
            " train loss item: 0.8634735345840454\n",
            " f1:0.35669792\n",
            " avg_train_loss: 0.72634656 \n",
            "BATCH: 95/1074\n",
            " train loss item: 0.69283527135849\n",
            " f1:0.35700870\n",
            " avg_train_loss: 0.72631789 \n",
            "BATCH: 96/1074\n",
            " train loss item: 0.6626455783843994\n",
            " f1:0.35736103\n",
            " avg_train_loss: 0.72626347 \n",
            "BATCH: 97/1074\n",
            " train loss item: 0.6644312739372253\n",
            " f1:0.35771276\n",
            " avg_train_loss: 0.72621067 \n",
            "BATCH: 98/1074\n",
            " train loss item: 0.645733654499054\n",
            " f1:0.35810277\n",
            " avg_train_loss: 0.72614200 \n",
            "BATCH: 99/1074\n",
            " train loss item: 0.7290592193603516\n",
            " f1:0.35831641\n",
            " avg_train_loss: 0.72614449 \n",
            "BATCH: 100/1074\n",
            " train loss item: 0.721001148223877\n",
            " f1:0.35852968\n",
            " avg_train_loss: 0.72614011 \n",
            "BATCH: 101/1074\n",
            " train loss item: 0.6971042156219482\n",
            " f1:0.35879192\n",
            " avg_train_loss: 0.72611540 \n",
            "BATCH: 102/1074\n",
            " train loss item: 0.7123163342475891\n",
            " f1:0.35882696\n",
            " avg_train_loss: 0.72610366 \n",
            "BATCH: 103/1074\n",
            " train loss item: 0.6884499788284302\n",
            " f1:0.35852210\n",
            " avg_train_loss: 0.72607167 \n",
            "BATCH: 104/1074\n",
            " train loss item: 0.700206995010376\n",
            " f1:0.35821775\n",
            " avg_train_loss: 0.72604971 \n",
            "BATCH: 105/1074\n",
            " train loss item: 0.6861920952796936\n",
            " f1:0.35791392\n",
            " avg_train_loss: 0.72601591 \n",
            "BATCH: 106/1074\n",
            " train loss item: 0.6124507188796997\n",
            " f1:0.35761060\n",
            " avg_train_loss: 0.72591967 \n",
            "BATCH: 107/1074\n",
            " train loss item: 0.7604436874389648\n",
            " f1:0.35730780\n",
            " avg_train_loss: 0.72594890 \n",
            "BATCH: 108/1074\n",
            " train loss item: 0.7591164708137512\n",
            " f1:0.35700551\n",
            " avg_train_loss: 0.72597696 \n",
            "BATCH: 109/1074\n",
            " train loss item: 0.6876450181007385\n",
            " f1:0.35670373\n",
            " avg_train_loss: 0.72594456 \n",
            "BATCH: 110/1074\n",
            " train loss item: 0.6698500514030457\n",
            " f1:0.35640246\n",
            " avg_train_loss: 0.72589718 \n",
            "BATCH: 111/1074\n",
            " train loss item: 0.7235663533210754\n",
            " f1:0.35610170\n",
            " avg_train_loss: 0.72589521 \n",
            "BATCH: 112/1074\n",
            " train loss item: 0.6825304627418518\n",
            " f1:0.35648847\n",
            " avg_train_loss: 0.72585865 \n",
            "BATCH: 113/1074\n",
            " train loss item: 0.6862995624542236\n",
            " f1:0.35679471\n",
            " avg_train_loss: 0.72582532 \n",
            "BATCH: 114/1074\n",
            " train loss item: 0.8465890288352966\n",
            " f1:0.35689521\n",
            " avg_train_loss: 0.72592698 \n",
            "BATCH: 115/1074\n",
            " train loss item: 0.836294412612915\n",
            " f1:0.35699555\n",
            " avg_train_loss: 0.72601980 \n",
            "BATCH: 116/1074\n",
            " train loss item: 0.70684814453125\n",
            " f1:0.35725577\n",
            " avg_train_loss: 0.72600369 \n",
            "BATCH: 117/1074\n",
            " train loss item: 0.6869797706604004\n",
            " f1:0.35756035\n",
            " avg_train_loss: 0.72597092 \n",
            "BATCH: 118/1074\n",
            " train loss item: 0.692753791809082\n",
            " f1:0.35786441\n",
            " avg_train_loss: 0.72594306 \n",
            "BATCH: 119/1074\n",
            " train loss item: 0.6850267648696899\n",
            " f1:0.35756444\n",
            " avg_train_loss: 0.72590876 \n",
            "BATCH: 120/1074\n",
            " train loss item: 0.6708166599273682\n",
            " f1:0.35726497\n",
            " avg_train_loss: 0.72586262 \n",
            "BATCH: 121/1074\n",
            " train loss item: 0.6974718570709229\n",
            " f1:0.35696600\n",
            " avg_train_loss: 0.72583886 \n",
            "BATCH: 122/1074\n",
            " train loss item: 0.6587664484977722\n",
            " f1:0.35666754\n",
            " avg_train_loss: 0.72578278 \n",
            "BATCH: 123/1074\n",
            " train loss item: 0.6529138088226318\n",
            " f1:0.35636957\n",
            " avg_train_loss: 0.72572190 \n",
            "BATCH: 124/1074\n",
            " train loss item: 0.6435331106185913\n",
            " f1:0.35607210\n",
            " avg_train_loss: 0.72565330 \n",
            "BATCH: 125/1074\n",
            " train loss item: 0.6917159557342529\n",
            " f1:0.35577512\n",
            " avg_train_loss: 0.72562499 \n",
            "BATCH: 126/1074\n",
            " train loss item: 0.5890722274780273\n",
            " f1:0.35547864\n",
            " avg_train_loss: 0.72551120 \n",
            "BATCH: 127/1074\n",
            " train loss item: 0.6700870394706726\n",
            " f1:0.35518266\n",
            " avg_train_loss: 0.72546505 \n",
            "BATCH: 128/1074\n",
            " train loss item: 0.7367685437202454\n",
            " f1:0.35488716\n",
            " avg_train_loss: 0.72547446 \n",
            "BATCH: 129/1074\n",
            " train loss item: 0.6737966537475586\n",
            " f1:0.35459216\n",
            " avg_train_loss: 0.72543150 \n",
            "BATCH: 130/1074\n",
            " train loss item: 0.7076690793037415\n",
            " f1:0.35429765\n",
            " avg_train_loss: 0.72541675 \n",
            "BATCH: 131/1074\n",
            " train loss item: 0.7974773049354553\n",
            " f1:0.35400363\n",
            " avg_train_loss: 0.72547655 \n",
            "BATCH: 132/1074\n",
            " train loss item: 0.6934124231338501\n",
            " f1:0.35426288\n",
            " avg_train_loss: 0.72544996 \n",
            "BATCH: 133/1074\n",
            " train loss item: 0.690311074256897\n",
            " f1:0.35456590\n",
            " avg_train_loss: 0.72542085 \n",
            "BATCH: 134/1074\n",
            " train loss item: 0.9066515564918518\n",
            " f1:0.35466658\n",
            " avg_train_loss: 0.72557087 \n",
            "BATCH: 135/1074\n",
            " train loss item: 0.7824214696884155\n",
            " f1:0.35487670\n",
            " avg_train_loss: 0.72561790 \n",
            "BATCH: 136/1074\n",
            " train loss item: 0.6868628263473511\n",
            " f1:0.35517845\n",
            " avg_train_loss: 0.72558587 \n",
            "BATCH: 137/1074\n",
            " train loss item: 0.6676731109619141\n",
            " f1:0.35555800\n",
            " avg_train_loss: 0.72553804 \n",
            "BATCH: 138/1074\n",
            " train loss item: 0.6999975442886353\n",
            " f1:0.35571468\n",
            " avg_train_loss: 0.72551697 \n",
            "BATCH: 139/1074\n",
            " train loss item: 0.6970893144607544\n",
            " f1:0.35542143\n",
            " avg_train_loss: 0.72549354 \n",
            "BATCH: 140/1074\n",
            " train loss item: 0.7021653056144714\n",
            " f1:0.35512866\n",
            " avg_train_loss: 0.72547432 \n",
            "BATCH: 141/1074\n",
            " train loss item: 0.677800178527832\n",
            " f1:0.35483637\n",
            " avg_train_loss: 0.72543508 \n",
            "BATCH: 142/1074\n",
            " train loss item: 0.7283680438995361\n",
            " f1:0.35454457\n",
            " avg_train_loss: 0.72543749 \n",
            "BATCH: 143/1074\n",
            " train loss item: 0.6863918304443359\n",
            " f1:0.35425324\n",
            " avg_train_loss: 0.72540541 \n",
            "BATCH: 144/1074\n",
            " train loss item: 0.6866939663887024\n",
            " f1:0.35396239\n",
            " avg_train_loss: 0.72537363 \n",
            "BATCH: 145/1074\n",
            " train loss item: 0.6787042021751404\n",
            " f1:0.35367202\n",
            " avg_train_loss: 0.72533534 \n",
            "BATCH: 146/1074\n",
            " train loss item: 0.695220947265625\n",
            " f1:0.35338213\n",
            " avg_train_loss: 0.72531066 \n",
            "BATCH: 147/1074\n",
            " train loss item: 0.6800446510314941\n",
            " f1:0.35309271\n",
            " avg_train_loss: 0.72527358 \n",
            "BATCH: 148/1074\n",
            " train loss item: 0.687447190284729\n",
            " f1:0.35280376\n",
            " avg_train_loss: 0.72524263 \n",
            "BATCH: 149/1074\n",
            " train loss item: 0.6873081922531128\n",
            " f1:0.35251529\n",
            " avg_train_loss: 0.72521161 \n",
            "BATCH: 150/1074\n",
            " train loss item: 0.7118539810180664\n",
            " f1:0.35222728\n",
            " avg_train_loss: 0.72520070 \n",
            "BATCH: 151/1074\n",
            " train loss item: 0.6803958415985107\n",
            " f1:0.35193975\n",
            " avg_train_loss: 0.72516412 \n",
            "BATCH: 152/1074\n",
            " train loss item: 0.6733536720275879\n",
            " f1:0.35165269\n",
            " avg_train_loss: 0.72512186 \n",
            "BATCH: 153/1074\n",
            " train loss item: 0.6955838799476624\n",
            " f1:0.35136609\n",
            " avg_train_loss: 0.72509779 \n",
            "BATCH: 154/1074\n",
            " train loss item: 0.6765968203544617\n",
            " f1:0.35107996\n",
            " avg_train_loss: 0.72505830 \n",
            "BATCH: 155/1074\n",
            " train loss item: 0.7210926413536072\n",
            " f1:0.35079430\n",
            " avg_train_loss: 0.72505507 \n",
            "BATCH: 156/1074\n",
            " train loss item: 0.6742919683456421\n",
            " f1:0.35050910\n",
            " avg_train_loss: 0.72501380 \n",
            "BATCH: 157/1074\n",
            " train loss item: 0.6979728937149048\n",
            " f1:0.35022437\n",
            " avg_train_loss: 0.72499183 \n",
            "BATCH: 158/1074\n",
            " train loss item: 0.7227504253387451\n",
            " f1:0.34994009\n",
            " avg_train_loss: 0.72499001 \n",
            "BATCH: 159/1074\n",
            " train loss item: 0.686225414276123\n",
            " f1:0.34965628\n",
            " avg_train_loss: 0.72495857 \n",
            "BATCH: 160/1074\n",
            " train loss item: 0.6517597436904907\n",
            " f1:0.34937293\n",
            " avg_train_loss: 0.72489925 \n",
            "BATCH: 161/1074\n",
            " train loss item: 0.6861417293548584\n",
            " f1:0.34909004\n",
            " avg_train_loss: 0.72486787 \n",
            "BATCH: 162/1074\n",
            " train loss item: 0.7105387449264526\n",
            " f1:0.34880760\n",
            " avg_train_loss: 0.72485628 \n",
            "BATCH: 163/1074\n",
            " train loss item: 0.6727827191352844\n",
            " f1:0.34852562\n",
            " avg_train_loss: 0.72481418 \n",
            "BATCH: 164/1074\n",
            " train loss item: 0.6854111552238464\n",
            " f1:0.34824410\n",
            " avg_train_loss: 0.72478235 \n",
            "BATCH: 165/1074\n",
            " train loss item: 0.7455827593803406\n",
            " f1:0.34796303\n",
            " avg_train_loss: 0.72479914 \n",
            "BATCH: 166/1074\n",
            " train loss item: 0.7119564414024353\n",
            " f1:0.34768241\n",
            " avg_train_loss: 0.72478879 \n",
            "BATCH: 167/1074\n",
            " train loss item: 0.7593879103660583\n",
            " f1:0.34740225\n",
            " avg_train_loss: 0.72481667 \n",
            "BATCH: 168/1074\n",
            " train loss item: 0.6768290996551514\n",
            " f1:0.34712254\n",
            " avg_train_loss: 0.72477803 \n",
            "BATCH: 169/1074\n",
            " train loss item: 0.6893986463546753\n",
            " f1:0.34684328\n",
            " avg_train_loss: 0.72474956 \n",
            "BATCH: 170/1074\n",
            " train loss item: 0.6931614875793457\n",
            " f1:0.34656446\n",
            " avg_train_loss: 0.72472417 \n",
            "BATCH: 171/1074\n",
            " train loss item: 0.6921612024307251\n",
            " f1:0.34628610\n",
            " avg_train_loss: 0.72469802 \n",
            "BATCH: 172/1074\n",
            " train loss item: 0.6915343999862671\n",
            " f1:0.34600818\n",
            " avg_train_loss: 0.72467140 \n",
            "BATCH: 173/1074\n",
            " train loss item: 0.6868388056755066\n",
            " f1:0.34573071\n",
            " avg_train_loss: 0.72464106 \n",
            "BATCH: 174/1074\n",
            " train loss item: 0.6940768957138062\n",
            " f1:0.34545368\n",
            " avg_train_loss: 0.72461657 \n",
            "BATCH: 175/1074\n",
            " train loss item: 0.7017316818237305\n",
            " f1:0.34517710\n",
            " avg_train_loss: 0.72459825 \n",
            "BATCH: 176/1074\n",
            " train loss item: 0.6951688528060913\n",
            " f1:0.34490096\n",
            " avg_train_loss: 0.72457471 \n",
            "BATCH: 177/1074\n",
            " train loss item: 0.686913788318634\n",
            " f1:0.34462526\n",
            " avg_train_loss: 0.72454460 \n",
            "BATCH: 178/1074\n",
            " train loss item: 0.6958913803100586\n",
            " f1:0.34435000\n",
            " avg_train_loss: 0.72452172 \n",
            "BATCH: 179/1074\n",
            " train loss item: 0.6574902534484863\n",
            " f1:0.34407517\n",
            " avg_train_loss: 0.72446822 \n",
            "BATCH: 180/1074\n",
            " train loss item: 0.6749911904335022\n",
            " f1:0.34380079\n",
            " avg_train_loss: 0.72442876 \n",
            "BATCH: 181/1074\n",
            " train loss item: 0.6727985143661499\n",
            " f1:0.34352685\n",
            " avg_train_loss: 0.72438762 \n",
            "BATCH: 182/1074\n",
            " train loss item: 0.7305455803871155\n",
            " f1:0.34325334\n",
            " avg_train_loss: 0.72439253 \n",
            "BATCH: 183/1074\n",
            " train loss item: 0.6697929501533508\n",
            " f1:0.34298027\n",
            " avg_train_loss: 0.72434909 \n",
            "BATCH: 184/1074\n",
            " train loss item: 0.718360185623169\n",
            " f1:0.34270763\n",
            " avg_train_loss: 0.72434433 \n",
            "BATCH: 185/1074\n",
            " train loss item: 0.6371952295303345\n",
            " f1:0.34243542\n",
            " avg_train_loss: 0.72427511 \n",
            "BATCH: 186/1074\n",
            " train loss item: 0.7380138039588928\n",
            " f1:0.34216365\n",
            " avg_train_loss: 0.72428601 \n",
            "BATCH: 187/1074\n",
            " train loss item: 0.6853416562080383\n",
            " f1:0.34189230\n",
            " avg_train_loss: 0.72425513 \n",
            "BATCH: 188/1074\n",
            " train loss item: 0.6695210337638855\n",
            " f1:0.34162139\n",
            " avg_train_loss: 0.72421176 \n",
            "BATCH: 189/1074\n",
            " train loss item: 0.7323237061500549\n",
            " f1:0.34135091\n",
            " avg_train_loss: 0.72421818 \n",
            "BATCH: 190/1074\n",
            " train loss item: 0.6717914342880249\n",
            " f1:0.34108085\n",
            " avg_train_loss: 0.72417670 \n",
            "BATCH: 191/1074\n",
            " train loss item: 0.6856285333633423\n",
            " f1:0.34081122\n",
            " avg_train_loss: 0.72414623 \n",
            "BATCH: 192/1074\n",
            " train loss item: 0.6975110769271851\n",
            " f1:0.34054202\n",
            " avg_train_loss: 0.72412519 \n",
            "BATCH: 193/1074\n",
            " train loss item: 0.6861501932144165\n",
            " f1:0.34027324\n",
            " avg_train_loss: 0.72409522 \n",
            "BATCH: 194/1074\n",
            " train loss item: 0.6864430904388428\n",
            " f1:0.34000489\n",
            " avg_train_loss: 0.72406553 \n",
            "BATCH: 195/1074\n",
            " train loss item: 0.7049981951713562\n",
            " f1:0.33973695\n",
            " avg_train_loss: 0.72405050 \n",
            "BATCH: 196/1074\n",
            " train loss item: 0.6951944828033447\n",
            " f1:0.33946944\n",
            " avg_train_loss: 0.72402778 \n",
            "BATCH: 197/1074\n",
            " train loss item: 0.6674960851669312\n",
            " f1:0.33920236\n",
            " avg_train_loss: 0.72398330 \n",
            "BATCH: 198/1074\n",
            " train loss item: 0.6805574893951416\n",
            " f1:0.33893569\n",
            " avg_train_loss: 0.72394916 \n",
            "BATCH: 199/1074\n",
            " train loss item: 0.6872128844261169\n",
            " f1:0.33866944\n",
            " avg_train_loss: 0.72392030 \n",
            "BATCH: 200/1074\n",
            " train loss item: 0.7223522663116455\n",
            " f1:0.33840361\n",
            " avg_train_loss: 0.72391907 \n",
            "BATCH: 201/1074\n",
            " train loss item: 0.6954196691513062\n",
            " f1:0.33813819\n",
            " avg_train_loss: 0.72389672 \n",
            "BATCH: 202/1074\n",
            " train loss item: 0.6872657537460327\n",
            " f1:0.33787319\n",
            " avg_train_loss: 0.72386801 \n",
            "BATCH: 203/1074\n",
            " train loss item: 0.7024721503257751\n",
            " f1:0.33760861\n",
            " avg_train_loss: 0.72385126 \n",
            "BATCH: 204/1074\n",
            " train loss item: 0.6878377199172974\n",
            " f1:0.33734444\n",
            " avg_train_loss: 0.72382308 \n",
            "BATCH: 205/1074\n",
            " train loss item: 0.6818031668663025\n",
            " f1:0.33708068\n",
            " avg_train_loss: 0.72379022 \n",
            "BATCH: 206/1074\n",
            " train loss item: 0.6944385766983032\n",
            " f1:0.33681734\n",
            " avg_train_loss: 0.72376729 \n",
            "BATCH: 207/1074\n",
            " train loss item: 0.7195515632629395\n",
            " f1:0.33655441\n",
            " avg_train_loss: 0.72376400 \n",
            "BATCH: 208/1074\n",
            " train loss item: 0.6838443279266357\n",
            " f1:0.33629188\n",
            " avg_train_loss: 0.72373286 \n",
            "BATCH: 209/1074\n",
            " train loss item: 0.6937768459320068\n",
            " f1:0.33602977\n",
            " avg_train_loss: 0.72370951 \n",
            "BATCH: 210/1074\n",
            " train loss item: 0.6898007392883301\n",
            " f1:0.33576806\n",
            " avg_train_loss: 0.72368311 \n",
            "BATCH: 211/1074\n",
            " train loss item: 0.6935348510742188\n",
            " f1:0.33550677\n",
            " avg_train_loss: 0.72365964 \n",
            "BATCH: 212/1074\n",
            " train loss item: 0.68719881772995\n",
            " f1:0.33524587\n",
            " avg_train_loss: 0.72363129 \n",
            "BATCH: 213/1074\n",
            " train loss item: 0.6901557445526123\n",
            " f1:0.33498539\n",
            " avg_train_loss: 0.72360528 \n",
            "BATCH: 214/1074\n",
            " train loss item: 0.6898437738418579\n",
            " f1:0.33472531\n",
            " avg_train_loss: 0.72357907 \n",
            "BATCH: 215/1074\n",
            " train loss item: 0.6980162858963013\n",
            " f1:0.33446563\n",
            " avg_train_loss: 0.72355924 \n",
            "BATCH: 216/1074\n",
            " train loss item: 0.7071520090103149\n",
            " f1:0.33420635\n",
            " avg_train_loss: 0.72354652 \n",
            "BATCH: 217/1074\n",
            " train loss item: 0.6897202730178833\n",
            " f1:0.33394748\n",
            " avg_train_loss: 0.72352032 \n",
            "BATCH: 218/1074\n",
            " train loss item: 0.6935614347457886\n",
            " f1:0.33368900\n",
            " avg_train_loss: 0.72349713 \n",
            "BATCH: 219/1074\n",
            " train loss item: 0.690231442451477\n",
            " f1:0.33343093\n",
            " avg_train_loss: 0.72347140 \n",
            "BATCH: 220/1074\n",
            " train loss item: 0.7028905153274536\n",
            " f1:0.33317326\n",
            " avg_train_loss: 0.72345550 \n",
            "BATCH: 221/1074\n",
            " train loss item: 0.6888822913169861\n",
            " f1:0.33291598\n",
            " avg_train_loss: 0.72342880 \n",
            "BATCH: 222/1074\n",
            " train loss item: 0.698919951915741\n",
            " f1:0.33265910\n",
            " avg_train_loss: 0.72340989 \n",
            "BATCH: 223/1074\n",
            " train loss item: 0.6925805807113647\n",
            " f1:0.33240262\n",
            " avg_train_loss: 0.72338612 \n",
            "BATCH: 224/1074\n",
            " train loss item: 0.6947360038757324\n",
            " f1:0.33238982\n",
            " avg_train_loss: 0.72336405 \n",
            "BATCH: 225/1074\n",
            " train loss item: 0.6931734085083008\n",
            " f1:0.33213394\n",
            " avg_train_loss: 0.72334080 \n",
            "BATCH: 226/1074\n",
            " train loss item: 0.6913776993751526\n",
            " f1:0.33187845\n",
            " avg_train_loss: 0.72331622 \n",
            "BATCH: 227/1074\n",
            " train loss item: 0.6875945329666138\n",
            " f1:0.33162335\n",
            " avg_train_loss: 0.72328876 \n",
            "BATCH: 228/1074\n",
            " train loss item: 0.685495138168335\n",
            " f1:0.33136865\n",
            " avg_train_loss: 0.72325973 \n",
            "BATCH: 229/1074\n",
            " train loss item: 0.6887986660003662\n",
            " f1:0.33111434\n",
            " avg_train_loss: 0.72323329 \n",
            "BATCH: 230/1074\n",
            " train loss item: 0.7064560651779175\n",
            " f1:0.33086042\n",
            " avg_train_loss: 0.72322042 \n",
            "BATCH: 231/1074\n",
            " train loss item: 0.6621980667114258\n",
            " f1:0.33060688\n",
            " avg_train_loss: 0.72317366 \n",
            "BATCH: 232/1074\n",
            " train loss item: 0.6808017492294312\n",
            " f1:0.33035374\n",
            " avg_train_loss: 0.72314122 \n",
            "BATCH: 233/1074\n",
            " train loss item: 0.6948685646057129\n",
            " f1:0.33010098\n",
            " avg_train_loss: 0.72311958 \n",
            "BATCH: 234/1074\n",
            " train loss item: 0.7025136947631836\n",
            " f1:0.32984861\n",
            " avg_train_loss: 0.72310383 \n",
            "BATCH: 235/1074\n",
            " train loss item: 0.6949446201324463\n",
            " f1:0.32959663\n",
            " avg_train_loss: 0.72308232 \n",
            "BATCH: 236/1074\n",
            " train loss item: 0.6729644536972046\n",
            " f1:0.32934503\n",
            " avg_train_loss: 0.72304406 \n",
            "BATCH: 237/1074\n",
            " train loss item: 0.687498927116394\n",
            " f1:0.32909381\n",
            " avg_train_loss: 0.72301695 \n",
            "BATCH: 238/1074\n",
            " train loss item: 0.6874101161956787\n",
            " f1:0.32884298\n",
            " avg_train_loss: 0.72298981 \n",
            "BATCH: 239/1074\n",
            " train loss item: 0.6951056718826294\n",
            " f1:0.32859252\n",
            " avg_train_loss: 0.72296857 \n",
            "BATCH: 240/1074\n",
            " train loss item: 0.6872174739837646\n",
            " f1:0.32834245\n",
            " avg_train_loss: 0.72294136 \n",
            "BATCH: 241/1074\n",
            " train loss item: 0.7116654515266418\n",
            " f1:0.32809276\n",
            " avg_train_loss: 0.72293279 \n",
            "BATCH: 242/1074\n",
            " train loss item: 0.7027541995048523\n",
            " f1:0.32784345\n",
            " avg_train_loss: 0.72291745 \n",
            "BATCH: 243/1074\n",
            " train loss item: 0.6810669898986816\n",
            " f1:0.32759452\n",
            " avg_train_loss: 0.72288568 \n",
            "BATCH: 244/1074\n",
            " train loss item: 0.6816657781600952\n",
            " f1:0.32734597\n",
            " avg_train_loss: 0.72285440 \n",
            "BATCH: 245/1074\n",
            " train loss item: 0.7077426910400391\n",
            " f1:0.32709779\n",
            " avg_train_loss: 0.72284295 \n",
            "BATCH: 246/1074\n",
            " train loss item: 0.7119696140289307\n",
            " f1:0.32684999\n",
            " avg_train_loss: 0.72283471 \n",
            "BATCH: 247/1074\n",
            " train loss item: 0.697480320930481\n",
            " f1:0.32660256\n",
            " avg_train_loss: 0.72281552 \n",
            "BATCH: 248/1074\n",
            " train loss item: 0.6907454133033752\n",
            " f1:0.32635551\n",
            " avg_train_loss: 0.72279126 \n",
            "BATCH: 249/1074\n",
            " train loss item: 0.6933329105377197\n",
            " f1:0.32656892\n",
            " avg_train_loss: 0.72276899 \n",
            "BATCH: 250/1074\n",
            " train loss item: 0.693169355392456\n",
            " f1:0.32682579\n",
            " avg_train_loss: 0.72274663 \n",
            "BATCH: 251/1074\n",
            " train loss item: 0.6918283700942993\n",
            " f1:0.32712253\n",
            " avg_train_loss: 0.72272330 \n",
            "BATCH: 252/1074\n",
            " train loss item: 0.6933484077453613\n",
            " f1:0.32737859\n",
            " avg_train_loss: 0.72270115 \n",
            "BATCH: 253/1074\n",
            " train loss item: 0.6969526410102844\n",
            " f1:0.32759059\n",
            " avg_train_loss: 0.72268174 \n",
            "BATCH: 254/1074\n",
            " train loss item: 0.6934966444969177\n",
            " f1:0.32784591\n",
            " avg_train_loss: 0.72265977 \n",
            "BATCH: 255/1074\n",
            " train loss item: 0.6934403777122498\n",
            " f1:0.32810086\n",
            " avg_train_loss: 0.72263778 \n",
            "BATCH: 256/1074\n",
            " train loss item: 0.6907368898391724\n",
            " f1:0.32839552\n",
            " avg_train_loss: 0.72261379 \n",
            "BATCH: 257/1074\n",
            " train loss item: 0.6962977647781372\n",
            " f1:0.32860611\n",
            " avg_train_loss: 0.72259402 \n",
            "BATCH: 258/1074\n",
            " train loss item: 0.6910682916641235\n",
            " f1:0.32889995\n",
            " avg_train_loss: 0.72257036 \n",
            "BATCH: 259/1074\n",
            " train loss item: 0.6819747686386108\n",
            " f1:0.32932580\n",
            " avg_train_loss: 0.72253990 \n",
            "BATCH: 260/1074\n",
            " train loss item: 0.6942746639251709\n",
            " f1:0.32957868\n",
            " avg_train_loss: 0.72251871 \n",
            "BATCH: 261/1074\n",
            " train loss item: 0.7051618099212646\n",
            " f1:0.32978775\n",
            " avg_train_loss: 0.72250571 \n",
            "BATCH: 262/1074\n",
            " train loss item: 0.7393948435783386\n",
            " f1:0.32984031\n",
            " avg_train_loss: 0.72251835 \n",
            "BATCH: 263/1074\n",
            " train loss item: 0.6813352108001709\n",
            " f1:0.33016895\n",
            " avg_train_loss: 0.72248755 \n",
            "BATCH: 264/1074\n",
            " train loss item: 0.7080249786376953\n",
            " f1:0.33027808\n",
            " avg_train_loss: 0.72247674 \n",
            "BATCH: 265/1074\n",
            " train loss item: 0.6924073100090027\n",
            " f1:0.33056914\n",
            " avg_train_loss: 0.72245428 \n",
            "BATCH: 266/1074\n",
            " train loss item: 0.6932440996170044\n",
            " f1:0.33032244\n",
            " avg_train_loss: 0.72243249 \n",
            "BATCH: 267/1074\n",
            " train loss item: 0.6935627460479736\n",
            " f1:0.33007612\n",
            " avg_train_loss: 0.72241096 \n",
            "BATCH: 268/1074\n",
            " train loss item: 0.6642276048660278\n",
            " f1:0.32983016\n",
            " avg_train_loss: 0.72236760 \n",
            "BATCH: 269/1074\n",
            " train loss item: 0.6876789331436157\n",
            " f1:0.32958457\n",
            " avg_train_loss: 0.72234177 \n",
            "BATCH: 270/1074\n",
            " train loss item: 0.6869933009147644\n",
            " f1:0.32933934\n",
            " avg_train_loss: 0.72231547 \n",
            "BATCH: 271/1074\n",
            " train loss item: 0.7233196496963501\n",
            " f1:0.32909448\n",
            " avg_train_loss: 0.72231622 \n",
            "BATCH: 272/1074\n",
            " train loss item: 0.69576495885849\n",
            " f1:0.32884998\n",
            " avg_train_loss: 0.72229649 \n",
            "BATCH: 273/1074\n",
            " train loss item: 0.6954582929611206\n",
            " f1:0.32860584\n",
            " avg_train_loss: 0.72227657 \n",
            "BATCH: 274/1074\n",
            " train loss item: 0.6873941421508789\n",
            " f1:0.32836207\n",
            " avg_train_loss: 0.72225069 \n",
            "BATCH: 275/1074\n",
            " train loss item: 0.6879206895828247\n",
            " f1:0.32811866\n",
            " avg_train_loss: 0.72222524 \n",
            "BATCH: 276/1074\n",
            " train loss item: 0.6942015886306763\n",
            " f1:0.32787561\n",
            " avg_train_loss: 0.72220448 \n",
            "BATCH: 277/1074\n",
            " train loss item: 0.6985176801681519\n",
            " f1:0.32763292\n",
            " avg_train_loss: 0.72218695 \n",
            "BATCH: 278/1074\n",
            " train loss item: 0.6874366998672485\n",
            " f1:0.32739059\n",
            " avg_train_loss: 0.72216125 \n",
            "BATCH: 279/1074\n",
            " train loss item: 0.6841471195220947\n",
            " f1:0.32714861\n",
            " avg_train_loss: 0.72213315 \n",
            "BATCH: 280/1074\n",
            " train loss item: 0.6822855472564697\n",
            " f1:0.32690700\n",
            " avg_train_loss: 0.72210372 \n",
            "BATCH: 281/1074\n",
            " train loss item: 0.6880995631217957\n",
            " f1:0.32666574\n",
            " avg_train_loss: 0.72207863 \n",
            "BATCH: 282/1074\n",
            " train loss item: 0.6610748171806335\n",
            " f1:0.32642483\n",
            " avg_train_loss: 0.72203364 \n",
            "BATCH: 283/1074\n",
            " train loss item: 0.662981390953064\n",
            " f1:0.32618428\n",
            " avg_train_loss: 0.72199012 \n",
            "BATCH: 284/1074\n",
            " train loss item: 0.6711517572402954\n",
            " f1:0.32594409\n",
            " avg_train_loss: 0.72195269 \n",
            "BATCH: 285/1074\n",
            " train loss item: 0.7020679712295532\n",
            " f1:0.32570425\n",
            " avg_train_loss: 0.72193805 \n",
            "BATCH: 286/1074\n",
            " train loss item: 0.70381098985672\n",
            " f1:0.32546476\n",
            " avg_train_loss: 0.72192472 \n",
            "BATCH: 287/1074\n",
            " train loss item: 0.7232965230941772\n",
            " f1:0.32522562\n",
            " avg_train_loss: 0.72192573 \n",
            "BATCH: 288/1074\n",
            " train loss item: 0.7740871906280518\n",
            " f1:0.32498684\n",
            " avg_train_loss: 0.72196403 \n",
            "BATCH: 289/1074\n",
            " train loss item: 0.6855204105377197\n",
            " f1:0.32474840\n",
            " avg_train_loss: 0.72193729 \n",
            "BATCH: 290/1074\n",
            " train loss item: 0.6609166860580444\n",
            " f1:0.32451032\n",
            " avg_train_loss: 0.72189256 \n",
            "BATCH: 291/1074\n",
            " train loss item: 0.7229911088943481\n",
            " f1:0.32427258\n",
            " avg_train_loss: 0.72189336 \n",
            "BATCH: 292/1074\n",
            " train loss item: 0.6949101686477661\n",
            " f1:0.32403519\n",
            " avg_train_loss: 0.72187361 \n",
            "BATCH: 293/1074\n",
            " train loss item: 0.7039483189582825\n",
            " f1:0.32419717\n",
            " avg_train_loss: 0.72186049 \n",
            "BATCH: 294/1074\n",
            " train loss item: 0.6463980674743652\n",
            " f1:0.32464244\n",
            " avg_train_loss: 0.72180533 \n",
            "BATCH: 295/1074\n",
            " train loss item: 0.6860335469245911\n",
            " f1:0.32493123\n",
            " avg_train_loss: 0.72177920 \n",
            "BATCH: 296/1074\n",
            " train loss item: 0.7926223278045654\n",
            " f1:0.32509220\n",
            " avg_train_loss: 0.72183091 \n",
            "BATCH: 297/1074\n",
            " train loss item: 0.8220914602279663\n",
            " f1:0.32520241\n",
            " avg_train_loss: 0.72190404 \n",
            "BATCH: 298/1074\n",
            " train loss item: 0.685491681098938\n",
            " f1:0.32549016\n",
            " avg_train_loss: 0.72187750 \n",
            "BATCH: 299/1074\n",
            " train loss item: 0.723575234413147\n",
            " f1:0.32554443\n",
            " avg_train_loss: 0.72187874 \n",
            "BATCH: 300/1074\n",
            " train loss item: 0.6883718967437744\n",
            " f1:0.32530750\n",
            " avg_train_loss: 0.72185435 \n",
            "BATCH: 301/1074\n",
            " train loss item: 0.714002788066864\n",
            " f1:0.32507091\n",
            " avg_train_loss: 0.72184864 \n",
            "BATCH: 302/1074\n",
            " train loss item: 0.7032780647277832\n",
            " f1:0.32483467\n",
            " avg_train_loss: 0.72183515 \n",
            "BATCH: 303/1074\n",
            " train loss item: 0.7397704124450684\n",
            " f1:0.32459877\n",
            " avg_train_loss: 0.72184817 \n",
            "BATCH: 304/1074\n",
            " train loss item: 0.685357928276062\n",
            " f1:0.32436321\n",
            " avg_train_loss: 0.72182169 \n",
            "BATCH: 305/1074\n",
            " train loss item: 0.6769009828567505\n",
            " f1:0.32412800\n",
            " avg_train_loss: 0.72178912 \n",
            "BATCH: 306/1074\n",
            " train loss item: 0.7148120999336243\n",
            " f1:0.32389312\n",
            " avg_train_loss: 0.72178406 \n",
            "BATCH: 307/1074\n",
            " train loss item: 0.7104542851448059\n",
            " f1:0.32400340\n",
            " avg_train_loss: 0.72177586 \n",
            "BATCH: 308/1074\n",
            " train loss item: 0.7073928117752075\n",
            " f1:0.32420940\n",
            " avg_train_loss: 0.72176545 \n",
            "BATCH: 309/1074\n",
            " train loss item: 0.6856887340545654\n",
            " f1:0.32449558\n",
            " avg_train_loss: 0.72173936 \n",
            "BATCH: 310/1074\n",
            " train loss item: 0.6853806376457214\n",
            " f1:0.32478135\n",
            " avg_train_loss: 0.72171309 \n",
            "BATCH: 311/1074\n",
            " train loss item: 0.685326337814331\n",
            " f1:0.32506671\n",
            " avg_train_loss: 0.72168682 \n",
            "BATCH: 312/1074\n",
            " train loss item: 0.6855049133300781\n",
            " f1:0.32535165\n",
            " avg_train_loss: 0.72166071 \n",
            "BATCH: 313/1074\n",
            " train loss item: 0.7637178897857666\n",
            " f1:0.32546041\n",
            " avg_train_loss: 0.72169104 \n",
            "BATCH: 314/1074\n",
            " train loss item: 0.7279916405677795\n",
            " f1:0.32556900\n",
            " avg_train_loss: 0.72169558 \n",
            "BATCH: 315/1074\n",
            " train loss item: 0.6899540424346924\n",
            " f1:0.32533461\n",
            " avg_train_loss: 0.72167272 \n",
            "BATCH: 316/1074\n",
            " train loss item: 0.7459704279899597\n",
            " f1:0.32510056\n",
            " avg_train_loss: 0.72169020 \n",
            "BATCH: 317/1074\n",
            " train loss item: 0.6471168994903564\n",
            " f1:0.32486684\n",
            " avg_train_loss: 0.72163659 \n",
            "BATCH: 318/1074\n",
            " train loss item: 0.6397703886032104\n",
            " f1:0.32463346\n",
            " avg_train_loss: 0.72157778 \n",
            "BATCH: 319/1074\n",
            " train loss item: 0.7751872539520264\n",
            " f1:0.32440041\n",
            " avg_train_loss: 0.72161627 \n",
            "BATCH: 320/1074\n",
            " train loss item: 0.7742487192153931\n",
            " f1:0.32416770\n",
            " avg_train_loss: 0.72165402 \n",
            "BATCH: 321/1074\n",
            " train loss item: 0.7843307852745056\n",
            " f1:0.32393532\n",
            " avg_train_loss: 0.72169895 \n",
            "BATCH: 322/1074\n",
            " train loss item: 0.6854372024536133\n",
            " f1:0.32370328\n",
            " avg_train_loss: 0.72167298 \n",
            "BATCH: 323/1074\n",
            " train loss item: 0.6978909373283386\n",
            " f1:0.32347156\n",
            " avg_train_loss: 0.72165595 \n",
            "BATCH: 324/1074\n",
            " train loss item: 0.7090315818786621\n",
            " f1:0.32324018\n",
            " avg_train_loss: 0.72164692 \n",
            "BATCH: 325/1074\n",
            " train loss item: 0.6942761540412903\n",
            " f1:0.32300913\n",
            " avg_train_loss: 0.72162736 \n",
            "BATCH: 326/1074\n",
            " train loss item: 0.6941298842430115\n",
            " f1:0.32325460\n",
            " avg_train_loss: 0.72160772 \n",
            "BATCH: 327/1074\n",
            " train loss item: 0.7314051985740662\n",
            " f1:0.32336376\n",
            " avg_train_loss: 0.72161471 \n",
            "BATCH: 328/1074\n",
            " train loss item: 0.7068755626678467\n",
            " f1:0.32356728\n",
            " avg_train_loss: 0.72160420 \n",
            "BATCH: 329/1074\n",
            " train loss item: 0.6943249702453613\n",
            " f1:0.32381183\n",
            " avg_train_loss: 0.72158475 \n",
            "BATCH: 330/1074\n",
            " train loss item: 0.6956003308296204\n",
            " f1:0.32396969\n",
            " avg_train_loss: 0.72156625 \n",
            "BATCH: 331/1074\n",
            " train loss item: 0.6811832189559937\n",
            " f1:0.32373911\n",
            " avg_train_loss: 0.72153750 \n",
            "BATCH: 332/1074\n",
            " train loss item: 0.7016168832778931\n",
            " f1:0.32350885\n",
            " avg_train_loss: 0.72152334 \n",
            "BATCH: 333/1074\n",
            " train loss item: 0.687155544757843\n",
            " f1:0.32327892\n",
            " avg_train_loss: 0.72149891 \n",
            "BATCH: 334/1074\n",
            " train loss item: 0.6328675746917725\n",
            " f1:0.32304932\n",
            " avg_train_loss: 0.72143596 \n",
            "BATCH: 335/1074\n",
            " train loss item: 0.6260181665420532\n",
            " f1:0.32282005\n",
            " avg_train_loss: 0.72136824 \n",
            "BATCH: 336/1074\n",
            " train loss item: 0.5776886940002441\n",
            " f1:0.32259110\n",
            " avg_train_loss: 0.72126634 \n",
            "BATCH: 337/1074\n",
            " train loss item: 0.7866309881210327\n",
            " f1:0.32236247\n",
            " avg_train_loss: 0.72131267 \n",
            "BATCH: 338/1074\n",
            " train loss item: 0.7327986359596252\n",
            " f1:0.32213417\n",
            " avg_train_loss: 0.72132080 \n",
            "BATCH: 339/1074\n",
            " train loss item: 0.8193308115005493\n",
            " f1:0.32190619\n",
            " avg_train_loss: 0.72139016 \n",
            "BATCH: 340/1074\n",
            " train loss item: 0.6992523670196533\n",
            " f1:0.32167853\n",
            " avg_train_loss: 0.72137451 \n",
            "BATCH: 341/1074\n",
            " train loss item: 0.740736186504364\n",
            " f1:0.32145120\n",
            " avg_train_loss: 0.72138819 \n",
            "BATCH: 342/1074\n",
            " train loss item: 0.6853420734405518\n",
            " f1:0.32122419\n",
            " avg_train_loss: 0.72136273 \n",
            "BATCH: 343/1074\n",
            " train loss item: 0.6751152276992798\n",
            " f1:0.32099749\n",
            " avg_train_loss: 0.72133010 \n",
            "BATCH: 344/1074\n",
            " train loss item: 0.68706876039505\n",
            " f1:0.32077112\n",
            " avg_train_loss: 0.72130593 \n",
            "BATCH: 345/1074\n",
            " train loss item: 0.6798490285873413\n",
            " f1:0.32054506\n",
            " avg_train_loss: 0.72127672 \n",
            "BATCH: 346/1074\n",
            " train loss item: 0.6955602169036865\n",
            " f1:0.32031933\n",
            " avg_train_loss: 0.72125861 \n",
            "BATCH: 347/1074\n",
            " train loss item: 0.6966845989227295\n",
            " f1:0.32009391\n",
            " avg_train_loss: 0.72124132 \n",
            "BATCH: 348/1074\n",
            " train loss item: 0.6856305599212646\n",
            " f1:0.31986881\n",
            " avg_train_loss: 0.72121627 \n",
            "BATCH: 349/1074\n",
            " train loss item: 0.7305284142494202\n",
            " f1:0.31964402\n",
            " avg_train_loss: 0.72122282 \n",
            "BATCH: 350/1074\n",
            " train loss item: 0.7299388647079468\n",
            " f1:0.31941956\n",
            " avg_train_loss: 0.72122894 \n",
            "BATCH: 351/1074\n",
            " train loss item: 0.6974452137947083\n",
            " f1:0.31919540\n",
            " avg_train_loss: 0.72121225 \n",
            "BATCH: 352/1074\n",
            " train loss item: 0.687352180480957\n",
            " f1:0.31897156\n",
            " avg_train_loss: 0.72118850 \n",
            "BATCH: 353/1074\n",
            " train loss item: 0.689236044883728\n",
            " f1:0.31874804\n",
            " avg_train_loss: 0.72116611 \n",
            "BATCH: 354/1074\n",
            " train loss item: 0.6907835602760315\n",
            " f1:0.31852482\n",
            " avg_train_loss: 0.72114483 \n",
            "BATCH: 355/1074\n",
            " train loss item: 0.680375874042511\n",
            " f1:0.31830192\n",
            " avg_train_loss: 0.72111631 \n",
            "BATCH: 356/1074\n",
            " train loss item: 0.7062479257583618\n",
            " f1:0.31807933\n",
            " avg_train_loss: 0.72110591 \n",
            "BATCH: 357/1074\n",
            " train loss item: 0.6702978610992432\n",
            " f1:0.31785706\n",
            " avg_train_loss: 0.72107040 \n",
            "BATCH: 358/1074\n",
            " train loss item: 0.7480331659317017\n",
            " f1:0.31763509\n",
            " avg_train_loss: 0.72108923 \n",
            "BATCH: 359/1074\n",
            " train loss item: 0.751350998878479\n",
            " f1:0.31741343\n",
            " avg_train_loss: 0.72111035 \n",
            "BATCH: 360/1074\n",
            " train loss item: 0.7045124769210815\n",
            " f1:0.31719208\n",
            " avg_train_loss: 0.72109877 \n",
            "BATCH: 361/1074\n",
            " train loss item: 0.7477588057518005\n",
            " f1:0.31697104\n",
            " avg_train_loss: 0.72111735 \n",
            "BATCH: 362/1074\n",
            " train loss item: 0.6862468123435974\n",
            " f1:0.31675031\n",
            " avg_train_loss: 0.72109307 \n",
            "BATCH: 363/1074\n",
            " train loss item: 0.6879887580871582\n",
            " f1:0.31652989\n",
            " avg_train_loss: 0.72107003 \n",
            "BATCH: 364/1074\n",
            " train loss item: 0.6773912906646729\n",
            " f1:0.31630977\n",
            " avg_train_loss: 0.72103966 \n",
            "BATCH: 365/1074\n",
            " train loss item: 0.7058542966842651\n",
            " f1:0.31608996\n",
            " avg_train_loss: 0.72102911 \n",
            "BATCH: 366/1074\n",
            " train loss item: 0.6904371976852417\n",
            " f1:0.31587045\n",
            " avg_train_loss: 0.72100786 \n",
            "BATCH: 367/1074\n",
            " train loss item: 0.6986613869667053\n",
            " f1:0.31565125\n",
            " avg_train_loss: 0.72099235 \n",
            "BATCH: 368/1074\n",
            " train loss item: 0.6964287757873535\n",
            " f1:0.31543235\n",
            " avg_train_loss: 0.72097532 \n",
            "BATCH: 369/1074\n",
            " train loss item: 0.6931500434875488\n",
            " f1:0.31567575\n",
            " avg_train_loss: 0.72095604 \n",
            "BATCH: 370/1074\n",
            " train loss item: 0.687179446220398\n",
            " f1:0.31602142\n",
            " avg_train_loss: 0.72093265 \n",
            "BATCH: 371/1074\n",
            " train loss item: 0.6942185163497925\n",
            " f1:0.31626408\n",
            " avg_train_loss: 0.72091416 \n",
            "BATCH: 372/1074\n",
            " train loss item: 0.6866611242294312\n",
            " f1:0.31654329\n",
            " avg_train_loss: 0.72089047 \n",
            "BATCH: 373/1074\n",
            " train loss item: 0.6985210180282593\n",
            " f1:0.31678525\n",
            " avg_train_loss: 0.72087501 \n",
            "BATCH: 374/1074\n",
            " train loss item: 0.7318140268325806\n",
            " f1:0.31694317\n",
            " avg_train_loss: 0.72088256 \n",
            "BATCH: 375/1074\n",
            " train loss item: 0.6854745149612427\n",
            " f1:0.31722134\n",
            " avg_train_loss: 0.72085813 \n",
            "BATCH: 376/1074\n",
            " train loss item: 0.7329145669937134\n",
            " f1:0.31733097\n",
            " avg_train_loss: 0.72086644 \n",
            "BATCH: 377/1074\n",
            " train loss item: 0.7052358388900757\n",
            " f1:0.31748819\n",
            " avg_train_loss: 0.72085567 \n",
            "BATCH: 378/1074\n",
            " train loss item: 0.689392626285553\n",
            " f1:0.31726953\n",
            " avg_train_loss: 0.72083400 \n",
            "BATCH: 379/1074\n",
            " train loss item: 0.6676753759384155\n",
            " f1:0.31705118\n",
            " avg_train_loss: 0.72079742 \n",
            "BATCH: 380/1074\n",
            " train loss item: 0.7372223734855652\n",
            " f1:0.31683312\n",
            " avg_train_loss: 0.72080871 \n",
            "BATCH: 381/1074\n",
            " train loss item: 0.7688588500022888\n",
            " f1:0.31661537\n",
            " avg_train_loss: 0.72084174 \n",
            "BATCH: 382/1074\n",
            " train loss item: 0.6667662262916565\n",
            " f1:0.31639791\n",
            " avg_train_loss: 0.72080460 \n",
            "BATCH: 383/1074\n",
            " train loss item: 0.7027461528778076\n",
            " f1:0.31618076\n",
            " avg_train_loss: 0.72079220 \n",
            "BATCH: 384/1074\n",
            " train loss item: 0.685350775718689\n",
            " f1:0.31596390\n",
            " avg_train_loss: 0.72076790 \n",
            "BATCH: 385/1074\n",
            " train loss item: 0.7333511114120483\n",
            " f1:0.31574733\n",
            " avg_train_loss: 0.72077652 \n",
            "BATCH: 386/1074\n",
            " train loss item: 0.6820156574249268\n",
            " f1:0.31553107\n",
            " avg_train_loss: 0.72074997 \n",
            "BATCH: 387/1074\n",
            " train loss item: 0.6954438090324402\n",
            " f1:0.31531510\n",
            " avg_train_loss: 0.72073265 \n",
            "BATCH: 388/1074\n",
            " train loss item: 0.6933608651161194\n",
            " f1:0.31555542\n",
            " avg_train_loss: 0.72071393 \n",
            "BATCH: 389/1074\n",
            " train loss item: 0.6946406364440918\n",
            " f1:0.31579542\n",
            " avg_train_loss: 0.72069611 \n",
            "BATCH: 390/1074\n",
            " train loss item: 0.6759850978851318\n",
            " f1:0.31610514\n",
            " avg_train_loss: 0.72066557 \n",
            "BATCH: 391/1074\n",
            " train loss item: 0.7156052589416504\n",
            " f1:0.31630486\n",
            " avg_train_loss: 0.72066211 \n",
            "BATCH: 392/1074\n",
            " train loss item: 0.7524257898330688\n",
            " f1:0.31641392\n",
            " avg_train_loss: 0.72068378 \n",
            "BATCH: 393/1074\n",
            " train loss item: 0.697970986366272\n",
            " f1:0.31665268\n",
            " avg_train_loss: 0.72066830 \n",
            "BATCH: 394/1074\n",
            " train loss item: 0.7028524279594421\n",
            " f1:0.31685162\n",
            " avg_train_loss: 0.72065616 \n",
            "BATCH: 395/1074\n",
            " train loss item: 0.687484622001648\n",
            " f1:0.31715957\n",
            " avg_train_loss: 0.72063358 \n",
            "BATCH: 396/1074\n",
            " train loss item: 0.6918991804122925\n",
            " f1:0.31749811\n",
            " avg_train_loss: 0.72061403 \n",
            "BATCH: 397/1074\n",
            " train loss item: 0.692992627620697\n",
            " f1:0.31728227\n",
            " avg_train_loss: 0.72059525 \n",
            "BATCH: 398/1074\n",
            " train loss item: 0.6899020671844482\n",
            " f1:0.31706672\n",
            " avg_train_loss: 0.72057440 \n",
            "BATCH: 399/1074\n",
            " train loss item: 0.6904312372207642\n",
            " f1:0.31685147\n",
            " avg_train_loss: 0.72055394 \n",
            "BATCH: 400/1074\n",
            " train loss item: 0.6796786785125732\n",
            " f1:0.31663651\n",
            " avg_train_loss: 0.72052621 \n",
            "BATCH: 401/1074\n",
            " train loss item: 0.6945714950561523\n",
            " f1:0.31642184\n",
            " avg_train_loss: 0.72050861 \n",
            "BATCH: 402/1074\n",
            " train loss item: 0.6871888637542725\n",
            " f1:0.31620747\n",
            " avg_train_loss: 0.72048604 \n",
            "BATCH: 403/1074\n",
            " train loss item: 0.6867125034332275\n",
            " f1:0.31599338\n",
            " avg_train_loss: 0.72046317 \n",
            "BATCH: 404/1074\n",
            " train loss item: 0.6468185186386108\n",
            " f1:0.31577958\n",
            " avg_train_loss: 0.72041334 \n",
            "BATCH: 405/1074\n",
            " train loss item: 0.6858192682266235\n",
            " f1:0.31556607\n",
            " avg_train_loss: 0.72038995 \n",
            "BATCH: 406/1074\n",
            " train loss item: 0.6854603290557861\n",
            " f1:0.31535285\n",
            " avg_train_loss: 0.72036635 \n",
            "BATCH: 407/1074\n",
            " train loss item: 0.6699147820472717\n",
            " f1:0.31513992\n",
            " avg_train_loss: 0.72033229 \n",
            "BATCH: 408/1074\n",
            " train loss item: 0.6854606866836548\n",
            " f1:0.31492727\n",
            " avg_train_loss: 0.72030876 \n",
            "BATCH: 409/1074\n",
            " train loss item: 0.7666548490524292\n",
            " f1:0.31471492\n",
            " avg_train_loss: 0.72034001 \n",
            "BATCH: 410/1074\n",
            " train loss item: 0.7948105335235596\n",
            " f1:0.31450284\n",
            " avg_train_loss: 0.72039019 \n",
            "BATCH: 411/1074\n",
            " train loss item: 0.6749937534332275\n",
            " f1:0.31429106\n",
            " avg_train_loss: 0.72035962 \n",
            "BATCH: 412/1074\n",
            " train loss item: 0.6764369606971741\n",
            " f1:0.31407956\n",
            " avg_train_loss: 0.72033006 \n",
            "BATCH: 413/1074\n",
            " train loss item: 0.686471700668335\n",
            " f1:0.31386834\n",
            " avg_train_loss: 0.72030729 \n",
            "BATCH: 414/1074\n",
            " train loss item: 0.690626859664917\n",
            " f1:0.31365741\n",
            " avg_train_loss: 0.72028735 \n",
            "BATCH: 415/1074\n",
            " train loss item: 0.687984824180603\n",
            " f1:0.31344676\n",
            " avg_train_loss: 0.72026565 \n",
            "BATCH: 416/1074\n",
            " train loss item: 0.7010893821716309\n",
            " f1:0.31323639\n",
            " avg_train_loss: 0.72025278 \n",
            "BATCH: 417/1074\n",
            " train loss item: 0.6900339126586914\n",
            " f1:0.31302630\n",
            " avg_train_loss: 0.72023252 \n",
            "BATCH: 418/1074\n",
            " train loss item: 0.6861487030982971\n",
            " f1:0.31281650\n",
            " avg_train_loss: 0.72020967 \n",
            "BATCH: 419/1074\n",
            " train loss item: 0.6796640157699585\n",
            " f1:0.31260698\n",
            " avg_train_loss: 0.72018251 \n",
            "BATCH: 420/1074\n",
            " train loss item: 0.6746377944946289\n",
            " f1:0.31239774\n",
            " avg_train_loss: 0.72015203 \n",
            "BATCH: 421/1074\n",
            " train loss item: 0.6688396334648132\n",
            " f1:0.31218878\n",
            " avg_train_loss: 0.72011771 \n",
            "BATCH: 422/1074\n",
            " train loss item: 0.7086155414581299\n",
            " f1:0.31198009\n",
            " avg_train_loss: 0.72011002 \n",
            "BATCH: 423/1074\n",
            " train loss item: 0.7110646963119507\n",
            " f1:0.31177169\n",
            " avg_train_loss: 0.72010398 \n",
            "BATCH: 424/1074\n",
            " train loss item: 0.6855023503303528\n",
            " f1:0.31156356\n",
            " avg_train_loss: 0.72008088 \n",
            "BATCH: 425/1074\n",
            " train loss item: 0.6990217566490173\n",
            " f1:0.31135572\n",
            " avg_train_loss: 0.72006683 \n",
            "BATCH: 426/1074\n",
            " train loss item: 0.6854805946350098\n",
            " f1:0.31114815\n",
            " avg_train_loss: 0.72004377 \n",
            "BATCH: 427/1074\n",
            " train loss item: 0.7250011563301086\n",
            " f1:0.31094085\n",
            " avg_train_loss: 0.72004707 \n",
            "BATCH: 428/1074\n",
            " train loss item: 0.6857832074165344\n",
            " f1:0.31073383\n",
            " avg_train_loss: 0.72002426 \n",
            "BATCH: 429/1074\n",
            " train loss item: 0.7391672134399414\n",
            " f1:0.31052709\n",
            " avg_train_loss: 0.72003700 \n",
            "BATCH: 430/1074\n",
            " train loss item: 0.694945752620697\n",
            " f1:0.31032062\n",
            " avg_train_loss: 0.72002031 \n",
            "BATCH: 431/1074\n",
            " train loss item: 0.6726756691932678\n",
            " f1:0.31011443\n",
            " avg_train_loss: 0.71998886 \n",
            "BATCH: 432/1074\n",
            " train loss item: 0.7083514928817749\n",
            " f1:0.30990851\n",
            " avg_train_loss: 0.71998113 \n",
            "BATCH: 433/1074\n",
            " train loss item: 0.6940081119537354\n",
            " f1:0.30970287\n",
            " avg_train_loss: 0.71996389 \n",
            "BATCH: 434/1074\n",
            " train loss item: 0.7013745903968811\n",
            " f1:0.30981327\n",
            " avg_train_loss: 0.71995157 \n",
            "BATCH: 435/1074\n",
            " train loss item: 0.6935065984725952\n",
            " f1:0.31004975\n",
            " avg_train_loss: 0.71993404 \n",
            "BATCH: 436/1074\n",
            " train loss item: 0.682300329208374\n",
            " f1:0.31038403\n",
            " avg_train_loss: 0.71990912 \n",
            "BATCH: 437/1074\n",
            " train loss item: 0.6945090889930725\n",
            " f1:0.31061983\n",
            " avg_train_loss: 0.71989231 \n",
            "BATCH: 438/1074\n",
            " train loss item: 0.6868558526039124\n",
            " f1:0.31089058\n",
            " avg_train_loss: 0.71987046 \n",
            "BATCH: 439/1074\n",
            " train loss item: 0.685926616191864\n",
            " f1:0.31116098\n",
            " avg_train_loss: 0.71984802 \n",
            "BATCH: 440/1074\n",
            " train loss item: 0.6853947639465332\n",
            " f1:0.31143102\n",
            " avg_train_loss: 0.71982527 \n",
            "BATCH: 441/1074\n",
            " train loss item: 0.7530137896537781\n",
            " f1:0.31153977\n",
            " avg_train_loss: 0.71984717 \n",
            "BATCH: 442/1074\n",
            " train loss item: 0.7513948678970337\n",
            " f1:0.31159812\n",
            " avg_train_loss: 0.71986798 \n",
            "BATCH: 443/1074\n",
            " train loss item: 0.6984353065490723\n",
            " f1:0.31179396\n",
            " avg_train_loss: 0.71985386 \n",
            "BATCH: 444/1074\n",
            " train loss item: 0.6913384199142456\n",
            " f1:0.31158857\n",
            " avg_train_loss: 0.71983507 \n",
            "BATCH: 445/1074\n",
            " train loss item: 0.7062467336654663\n",
            " f1:0.31138344\n",
            " avg_train_loss: 0.71982613 \n",
            "BATCH: 446/1074\n",
            " train loss item: 0.702544093132019\n",
            " f1:0.31117858\n",
            " avg_train_loss: 0.71981476 \n",
            "BATCH: 447/1074\n",
            " train loss item: 0.6873316764831543\n",
            " f1:0.31097399\n",
            " avg_train_loss: 0.71979340 \n",
            "BATCH: 448/1074\n",
            " train loss item: 0.6946978569030762\n",
            " f1:0.31076967\n",
            " avg_train_loss: 0.71977691 \n",
            "BATCH: 449/1074\n",
            " train loss item: 0.6886318922042847\n",
            " f1:0.31056562\n",
            " avg_train_loss: 0.71975646 \n",
            "BATCH: 450/1074\n",
            " train loss item: 0.6935781240463257\n",
            " f1:0.31036184\n",
            " avg_train_loss: 0.71973928 \n",
            "BATCH: 451/1074\n",
            " train loss item: 0.6917139291763306\n",
            " f1:0.31015832\n",
            " avg_train_loss: 0.71972091 \n",
            "BATCH: 452/1074\n",
            " train loss item: 0.6938570737838745\n",
            " f1:0.31031252\n",
            " avg_train_loss: 0.71970396 \n",
            "BATCH: 453/1074\n",
            " train loss item: 0.694134533405304\n",
            " f1:0.31050792\n",
            " avg_train_loss: 0.71968721 \n",
            "BATCH: 454/1074\n",
            " train loss item: 0.6923447847366333\n",
            " f1:0.31077591\n",
            " avg_train_loss: 0.71966932 \n",
            "BATCH: 455/1074\n",
            " train loss item: 0.6892107725143433\n",
            " f1:0.31110556\n",
            " avg_train_loss: 0.71964940 \n",
            "BATCH: 456/1074\n",
            " train loss item: 0.685657262802124\n",
            " f1:0.31140499\n",
            " avg_train_loss: 0.71962718 \n",
            "BATCH: 457/1074\n",
            " train loss item: 0.7037305235862732\n",
            " f1:0.31159917\n",
            " avg_train_loss: 0.71961680 \n",
            "BATCH: 458/1074\n",
            " train loss item: 0.7078191041946411\n",
            " f1:0.31179310\n",
            " avg_train_loss: 0.71960910 \n",
            "BATCH: 459/1074\n",
            " train loss item: 0.7302295565605164\n",
            " f1:0.31190034\n",
            " avg_train_loss: 0.71961603 \n",
            "BATCH: 460/1074\n",
            " train loss item: 0.7011971473693848\n",
            " f1:0.31209382\n",
            " avg_train_loss: 0.71960402 \n",
            "BATCH: 461/1074\n",
            " train loss item: 0.6896877288818359\n",
            " f1:0.31239163\n",
            " avg_train_loss: 0.71958453 \n",
            "BATCH: 462/1074\n",
            " train loss item: 0.6924890875816345\n",
            " f1:0.31218825\n",
            " avg_train_loss: 0.71956689 \n",
            "BATCH: 463/1074\n",
            " train loss item: 0.6902827620506287\n",
            " f1:0.31198513\n",
            " avg_train_loss: 0.71954784 \n",
            "BATCH: 464/1074\n",
            " train loss item: 0.6941180229187012\n",
            " f1:0.31178228\n",
            " avg_train_loss: 0.71953130 \n",
            "BATCH: 465/1074\n",
            " train loss item: 0.687625527381897\n",
            " f1:0.31157969\n",
            " avg_train_loss: 0.71951057 \n",
            "BATCH: 466/1074\n",
            " train loss item: 0.6526587009429932\n",
            " f1:0.31137737\n",
            " avg_train_loss: 0.71946716 \n",
            "BATCH: 467/1074\n",
            " train loss item: 0.6967480182647705\n",
            " f1:0.31117531\n",
            " avg_train_loss: 0.71945242 \n",
            "BATCH: 468/1074\n",
            " train loss item: 0.6977264285087585\n",
            " f1:0.31097351\n",
            " avg_train_loss: 0.71943833 \n",
            "BATCH: 469/1074\n",
            " train loss item: 0.7110724449157715\n",
            " f1:0.31077197\n",
            " avg_train_loss: 0.71943290 \n",
            "BATCH: 470/1074\n",
            " train loss item: 0.7110364437103271\n",
            " f1:0.31057069\n",
            " avg_train_loss: 0.71942747 \n",
            "BATCH: 471/1074\n",
            " train loss item: 0.7097246646881104\n",
            " f1:0.31036967\n",
            " avg_train_loss: 0.71942119 \n",
            "BATCH: 472/1074\n",
            " train loss item: 0.7074171304702759\n",
            " f1:0.31016892\n",
            " avg_train_loss: 0.71941342 \n",
            "BATCH: 473/1074\n",
            " train loss item: 0.6780152916908264\n",
            " f1:0.30996842\n",
            " avg_train_loss: 0.71938666 \n",
            "BATCH: 474/1074\n",
            " train loss item: 0.680016040802002\n",
            " f1:0.30976818\n",
            " avg_train_loss: 0.71936123 \n",
            "BATCH: 475/1074\n",
            " train loss item: 0.6878966689109802\n",
            " f1:0.30956820\n",
            " avg_train_loss: 0.71934092 \n",
            "BATCH: 476/1074\n",
            " train loss item: 0.6758973598480225\n",
            " f1:0.30936848\n",
            " avg_train_loss: 0.71931289 \n",
            "BATCH: 477/1074\n",
            " train loss item: 0.6679948568344116\n",
            " f1:0.30916902\n",
            " avg_train_loss: 0.71927980 \n",
            "BATCH: 478/1074\n",
            " train loss item: 0.695348858833313\n",
            " f1:0.30896981\n",
            " avg_train_loss: 0.71926438 \n",
            "BATCH: 479/1074\n",
            " train loss item: 0.6769886016845703\n",
            " f1:0.30877086\n",
            " avg_train_loss: 0.71923716 \n",
            "BATCH: 480/1074\n",
            " train loss item: 0.7079594135284424\n",
            " f1:0.30857217\n",
            " avg_train_loss: 0.71922990 \n",
            "BATCH: 481/1074\n",
            " train loss item: 0.6858165860176086\n",
            " f1:0.30837373\n",
            " avg_train_loss: 0.71920841 \n",
            "BATCH: 482/1074\n",
            " train loss item: 0.7104832530021667\n",
            " f1:0.30817554\n",
            " avg_train_loss: 0.71920281 \n",
            "BATCH: 483/1074\n",
            " train loss item: 0.7357404232025146\n",
            " f1:0.30797762\n",
            " avg_train_loss: 0.71921343 \n",
            "BATCH: 484/1074\n",
            " train loss item: 0.6741111874580383\n",
            " f1:0.30777994\n",
            " avg_train_loss: 0.71918448 \n",
            "BATCH: 485/1074\n",
            " train loss item: 0.6972208023071289\n",
            " f1:0.30758252\n",
            " avg_train_loss: 0.71917039 \n",
            "BATCH: 486/1074\n",
            " train loss item: 0.6860678195953369\n",
            " f1:0.30738535\n",
            " avg_train_loss: 0.71914917 \n",
            "BATCH: 487/1074\n",
            " train loss item: 0.7071484327316284\n",
            " f1:0.30718844\n",
            " avg_train_loss: 0.71914148 \n",
            "BATCH: 488/1074\n",
            " train loss item: 0.7063459157943726\n",
            " f1:0.30699177\n",
            " avg_train_loss: 0.71913329 \n",
            "BATCH: 489/1074\n",
            " train loss item: 0.7054382562637329\n",
            " f1:0.30679536\n",
            " avg_train_loss: 0.71912453 \n",
            "BATCH: 490/1074\n",
            " train loss item: 0.713373064994812\n",
            " f1:0.30659920\n",
            " avg_train_loss: 0.71912085 \n",
            "BATCH: 491/1074\n",
            " train loss item: 0.695294201374054\n",
            " f1:0.30640329\n",
            " avg_train_loss: 0.71910563 \n",
            "BATCH: 492/1074\n",
            " train loss item: 0.7178715467453003\n",
            " f1:0.30620763\n",
            " avg_train_loss: 0.71910484 \n",
            "BATCH: 493/1074\n",
            " train loss item: 0.6946303844451904\n",
            " f1:0.30601222\n",
            " avg_train_loss: 0.71908922 \n",
            "BATCH: 494/1074\n",
            " train loss item: 0.6762675642967224\n",
            " f1:0.30581706\n",
            " avg_train_loss: 0.71906191 \n",
            "BATCH: 495/1074\n",
            " train loss item: 0.6827221512794495\n",
            " f1:0.30562215\n",
            " avg_train_loss: 0.71903875 \n",
            "BATCH: 496/1074\n",
            " train loss item: 0.6883867979049683\n",
            " f1:0.30542748\n",
            " avg_train_loss: 0.71901923 \n",
            "BATCH: 497/1074\n",
            " train loss item: 0.7128084897994995\n",
            " f1:0.30523307\n",
            " avg_train_loss: 0.71901527 \n",
            "BATCH: 498/1074\n",
            " train loss item: 0.6999632120132446\n",
            " f1:0.30503890\n",
            " avg_train_loss: 0.71900315 \n",
            "BATCH: 499/1074\n",
            " train loss item: 0.6939618587493896\n",
            " f1:0.30484498\n",
            " avg_train_loss: 0.71898723 \n",
            "BATCH: 500/1074\n",
            " train loss item: 0.6809503436088562\n",
            " f1:0.30465130\n",
            " avg_train_loss: 0.71896307 \n",
            "BATCH: 501/1074\n",
            " train loss item: 0.6893557906150818\n",
            " f1:0.30445787\n",
            " avg_train_loss: 0.71894427 \n",
            "BATCH: 502/1074\n",
            " train loss item: 0.6938886046409607\n",
            " f1:0.30426469\n",
            " avg_train_loss: 0.71892837 \n",
            "BATCH: 503/1074\n",
            " train loss item: 0.683800220489502\n",
            " f1:0.30407175\n",
            " avg_train_loss: 0.71890610 \n",
            "BATCH: 504/1074\n",
            " train loss item: 0.682442307472229\n",
            " f1:0.30387905\n",
            " avg_train_loss: 0.71888299 \n",
            "BATCH: 505/1074\n",
            " train loss item: 0.6947977542877197\n",
            " f1:0.30368660\n",
            " avg_train_loss: 0.71886774 \n",
            "BATCH: 506/1074\n",
            " train loss item: 0.6789361834526062\n",
            " f1:0.30349440\n",
            " avg_train_loss: 0.71884246 \n",
            "BATCH: 507/1074\n",
            " train loss item: 0.7055689096450806\n",
            " f1:0.30330243\n",
            " avg_train_loss: 0.71883407 \n",
            "BATCH: 508/1074\n",
            " train loss item: 0.696479320526123\n",
            " f1:0.30311071\n",
            " avg_train_loss: 0.71881994 \n",
            "BATCH: 509/1074\n",
            " train loss item: 0.6755331754684448\n",
            " f1:0.30291923\n",
            " avg_train_loss: 0.71879259 \n",
            "BATCH: 510/1074\n",
            " train loss item: 0.7086669206619263\n",
            " f1:0.30272800\n",
            " avg_train_loss: 0.71878620 \n",
            "BATCH: 511/1074\n",
            " train loss item: 0.7548094987869263\n",
            " f1:0.30253700\n",
            " avg_train_loss: 0.71880893 \n",
            "BATCH: 512/1074\n",
            " train loss item: 0.6864125728607178\n",
            " f1:0.30234625\n",
            " avg_train_loss: 0.71878850 \n",
            "BATCH: 513/1074\n",
            " train loss item: 0.6955168843269348\n",
            " f1:0.30215573\n",
            " avg_train_loss: 0.71877384 \n",
            "BATCH: 514/1074\n",
            " train loss item: 0.6949871778488159\n",
            " f1:0.30196546\n",
            " avg_train_loss: 0.71875886 \n",
            "BATCH: 515/1074\n",
            " train loss item: 0.7080796957015991\n",
            " f1:0.30177542\n",
            " avg_train_loss: 0.71875214 \n",
            "BATCH: 516/1074\n",
            " train loss item: 0.6942141056060791\n",
            " f1:0.30158563\n",
            " avg_train_loss: 0.71873670 \n",
            "BATCH: 517/1074\n",
            " train loss item: 0.6939412355422974\n",
            " f1:0.30139607\n",
            " avg_train_loss: 0.71872112 \n",
            "BATCH: 518/1074\n",
            " train loss item: 0.693743109703064\n",
            " f1:0.30120675\n",
            " avg_train_loss: 0.71870543 \n",
            "BATCH: 519/1074\n",
            " train loss item: 0.6973288059234619\n",
            " f1:0.30101767\n",
            " avg_train_loss: 0.71869201 \n",
            "BATCH: 520/1074\n",
            " train loss item: 0.6904779672622681\n",
            " f1:0.30082883\n",
            " avg_train_loss: 0.71867431 \n",
            "BATCH: 521/1074\n",
            " train loss item: 0.6933457851409912\n",
            " f1:0.30064022\n",
            " avg_train_loss: 0.71865843 \n",
            "BATCH: 522/1074\n",
            " train loss item: 0.6912290453910828\n",
            " f1:0.30045185\n",
            " avg_train_loss: 0.71864124 \n",
            "BATCH: 523/1074\n",
            " train loss item: 0.6912326812744141\n",
            " f1:0.30026371\n",
            " avg_train_loss: 0.71862408 \n",
            "BATCH: 524/1074\n",
            " train loss item: 0.6958416700363159\n",
            " f1:0.30007581\n",
            " avg_train_loss: 0.71860982 \n",
            "BATCH: 525/1074\n",
            " train loss item: 0.6909373998641968\n",
            " f1:0.29988815\n",
            " avg_train_loss: 0.71859252 \n",
            "BATCH: 526/1074\n",
            " train loss item: 0.6878527402877808\n",
            " f1:0.29970072\n",
            " avg_train_loss: 0.71857331 \n",
            "BATCH: 527/1074\n",
            " train loss item: 0.6978552341461182\n",
            " f1:0.29951352\n",
            " avg_train_loss: 0.71856037 \n",
            "BATCH: 528/1074\n",
            " train loss item: 0.6843690872192383\n",
            " f1:0.29932656\n",
            " avg_train_loss: 0.71853902 \n",
            "BATCH: 529/1074\n",
            " train loss item: 0.7070053815841675\n",
            " f1:0.29913983\n",
            " avg_train_loss: 0.71853183 \n",
            "BATCH: 530/1074\n",
            " train loss item: 0.6944181323051453\n",
            " f1:0.29895333\n",
            " avg_train_loss: 0.71851679 \n",
            "BATCH: 531/1074\n",
            " train loss item: 0.6821918487548828\n",
            " f1:0.29876707\n",
            " avg_train_loss: 0.71849416 \n",
            "BATCH: 532/1074\n",
            " train loss item: 0.6809384226799011\n",
            " f1:0.29858104\n",
            " avg_train_loss: 0.71847078 \n",
            "BATCH: 533/1074\n",
            " train loss item: 0.721244215965271\n",
            " f1:0.29839524\n",
            " avg_train_loss: 0.71847250 \n",
            "BATCH: 534/1074\n",
            " train loss item: 0.687274694442749\n",
            " f1:0.29820967\n",
            " avg_train_loss: 0.71845310 \n",
            "BATCH: 535/1074\n",
            " train loss item: 0.7024304866790771\n",
            " f1:0.29802433\n",
            " avg_train_loss: 0.71844314 \n",
            "BATCH: 536/1074\n",
            " train loss item: 0.6819234490394592\n",
            " f1:0.29783922\n",
            " avg_train_loss: 0.71842046 \n",
            "BATCH: 537/1074\n",
            " train loss item: 0.7065962553024292\n",
            " f1:0.29765434\n",
            " avg_train_loss: 0.71841312 \n",
            "BATCH: 538/1074\n",
            " train loss item: 0.6892786622047424\n",
            " f1:0.29746969\n",
            " avg_train_loss: 0.71839505 \n",
            "BATCH: 539/1074\n",
            " train loss item: 0.6970862150192261\n",
            " f1:0.29728527\n",
            " avg_train_loss: 0.71838184 \n",
            "BATCH: 540/1074\n",
            " train loss item: 0.6951336860656738\n",
            " f1:0.29710108\n",
            " avg_train_loss: 0.71836743 \n",
            "BATCH: 541/1074\n",
            " train loss item: 0.6931518316268921\n",
            " f1:0.29732992\n",
            " avg_train_loss: 0.71835182 \n",
            "BATCH: 542/1074\n",
            " train loss item: 0.6981998682022095\n",
            " f1:0.29748346\n",
            " avg_train_loss: 0.71833935 \n",
            "BATCH: 543/1074\n",
            " train loss item: 0.6775611639022827\n",
            " f1:0.29787669\n",
            " avg_train_loss: 0.71831413 \n",
            "BATCH: 544/1074\n",
            " train loss item: 0.709662675857544\n",
            " f1:0.29802970\n",
            " avg_train_loss: 0.71830878 \n",
            "BATCH: 545/1074\n",
            " train loss item: 0.6960475444793701\n",
            " f1:0.29825740\n",
            " avg_train_loss: 0.71829503 \n",
            "BATCH: 546/1074\n",
            " train loss item: 0.6968829035758972\n",
            " f1:0.29848481\n",
            " avg_train_loss: 0.71828182 \n",
            "BATCH: 547/1074\n",
            " train loss item: 0.6412472724914551\n",
            " f1:0.29885376\n",
            " avg_train_loss: 0.71823429 \n",
            "BATCH: 548/1074\n",
            " train loss item: 0.6687964200973511\n",
            " f1:0.29914376\n",
            " avg_train_loss: 0.71820381 \n",
            "BATCH: 549/1074\n",
            " train loss item: 0.757427990436554\n",
            " f1:0.29929552\n",
            " avg_train_loss: 0.71822798 \n",
            "BATCH: 550/1074\n",
            " train loss item: 0.6402014493942261\n",
            " f1:0.29961296\n",
            " avg_train_loss: 0.71817994 \n",
            "BATCH: 551/1074\n",
            " train loss item: 0.6889033317565918\n",
            " f1:0.29987166\n",
            " avg_train_loss: 0.71816192 \n",
            "BATCH: 552/1074\n",
            " train loss item: 0.7175766825675964\n",
            " f1:0.30009724\n",
            " avg_train_loss: 0.71816156 \n",
            "BATCH: 553/1074\n",
            " train loss item: 0.7134065628051758\n",
            " f1:0.30032254\n",
            " avg_train_loss: 0.71815864 \n",
            "BATCH: 554/1074\n",
            " train loss item: 0.7260857820510864\n",
            " f1:0.30051196\n",
            " avg_train_loss: 0.71816351 \n",
            "BATCH: 555/1074\n",
            " train loss item: 0.685651957988739\n",
            " f1:0.30076947\n",
            " avg_train_loss: 0.71814355 \n",
            "BATCH: 556/1074\n",
            " train loss item: 0.6946151256561279\n",
            " f1:0.30099395\n",
            " avg_train_loss: 0.71812911 \n",
            "BATCH: 557/1074\n",
            " train loss item: 0.6974860429763794\n",
            " f1:0.30114383\n",
            " avg_train_loss: 0.71811646 \n",
            "BATCH: 558/1074\n",
            " train loss item: 0.6958847045898438\n",
            " f1:0.30095931\n",
            " avg_train_loss: 0.71810283 \n",
            "BATCH: 559/1074\n",
            " train loss item: 0.6993603706359863\n",
            " f1:0.30077501\n",
            " avg_train_loss: 0.71809136 \n",
            "BATCH: 560/1074\n",
            " train loss item: 0.6946341395378113\n",
            " f1:0.30059094\n",
            " avg_train_loss: 0.71807700 \n",
            "BATCH: 561/1074\n",
            " train loss item: 0.6950241327285767\n",
            " f1:0.30040709\n",
            " avg_train_loss: 0.71806290 \n",
            "BATCH: 562/1074\n",
            " train loss item: 0.6791859865188599\n",
            " f1:0.30022347\n",
            " avg_train_loss: 0.71803914 \n",
            "BATCH: 563/1074\n",
            " train loss item: 0.6620378494262695\n",
            " f1:0.30004007\n",
            " avg_train_loss: 0.71800493 \n",
            "BATCH: 564/1074\n",
            " train loss item: 0.7049959897994995\n",
            " f1:0.29985689\n",
            " avg_train_loss: 0.71799699 \n",
            "BATCH: 565/1074\n",
            " train loss item: 0.6489626169204712\n",
            " f1:0.29967394\n",
            " avg_train_loss: 0.71795487 \n",
            "BATCH: 566/1074\n",
            " train loss item: 0.6638773679733276\n",
            " f1:0.29949121\n",
            " avg_train_loss: 0.71792189 \n",
            "BATCH: 567/1074\n",
            " train loss item: 0.6854104995727539\n",
            " f1:0.29930871\n",
            " avg_train_loss: 0.71790208 \n",
            "BATCH: 568/1074\n",
            " train loss item: 0.6682457327842712\n",
            " f1:0.29912643\n",
            " avg_train_loss: 0.71787184 \n",
            "BATCH: 569/1074\n",
            " train loss item: 0.6650084257125854\n",
            " f1:0.29894437\n",
            " avg_train_loss: 0.71783966 \n",
            "BATCH: 570/1074\n",
            " train loss item: 0.794291079044342\n",
            " f1:0.29876253\n",
            " avg_train_loss: 0.71788617 \n",
            "BATCH: 571/1074\n",
            " train loss item: 0.6876883506774902\n",
            " f1:0.29858091\n",
            " avg_train_loss: 0.71786781 \n",
            "BATCH: 572/1074\n",
            " train loss item: 0.7304491996765137\n",
            " f1:0.29839951\n",
            " avg_train_loss: 0.71787545 \n",
            "BATCH: 573/1074\n",
            " train loss item: 0.7872780561447144\n",
            " f1:0.29821833\n",
            " avg_train_loss: 0.71791759 \n",
            "BATCH: 574/1074\n",
            " train loss item: 0.7103874087333679\n",
            " f1:0.29803737\n",
            " avg_train_loss: 0.71791302 \n",
            "BATCH: 575/1074\n",
            " train loss item: 0.6931487917900085\n",
            " f1:0.29826092\n",
            " avg_train_loss: 0.71789801 \n",
            "BATCH: 576/1074\n",
            " train loss item: 0.6779985427856445\n",
            " f1:0.29857398\n",
            " avg_train_loss: 0.71787382 \n",
            "BATCH: 577/1074\n",
            " train loss item: 0.686108410358429\n",
            " f1:0.29882924\n",
            " avg_train_loss: 0.71785458 \n",
            "BATCH: 578/1074\n",
            " train loss item: 0.7312143445014954\n",
            " f1:0.29897853\n",
            " avg_train_loss: 0.71786267 \n",
            "BATCH: 579/1074\n",
            " train loss item: 0.7116963863372803\n",
            " f1:0.29916589\n",
            " avg_train_loss: 0.71785894 \n",
            "BATCH: 580/1074\n",
            " train loss item: 0.6991161108016968\n",
            " f1:0.29935303\n",
            " avg_train_loss: 0.71784761 \n",
            "BATCH: 581/1074\n",
            " train loss item: 0.7026574611663818\n",
            " f1:0.29917216\n",
            " avg_train_loss: 0.71783843 \n",
            "BATCH: 582/1074\n",
            " train loss item: 0.7176306247711182\n",
            " f1:0.29899150\n",
            " avg_train_loss: 0.71783830 \n",
            "BATCH: 583/1074\n",
            " train loss item: 0.668312668800354\n",
            " f1:0.29881106\n",
            " avg_train_loss: 0.71780842 \n",
            "BATCH: 584/1074\n",
            " train loss item: 0.7298712134361267\n",
            " f1:0.29863083\n",
            " avg_train_loss: 0.71781569 \n",
            "BATCH: 585/1074\n",
            " train loss item: 0.6389501094818115\n",
            " f1:0.29845083\n",
            " avg_train_loss: 0.71776815 \n",
            "BATCH: 586/1074\n",
            " train loss item: 0.6636107563972473\n",
            " f1:0.29827104\n",
            " avg_train_loss: 0.71773553 \n",
            "BATCH: 587/1074\n",
            " train loss item: 0.7300792932510376\n",
            " f1:0.29809146\n",
            " avg_train_loss: 0.71774296 \n",
            "BATCH: 588/1074\n",
            " train loss item: 0.6939140558242798\n",
            " f1:0.29791211\n",
            " avg_train_loss: 0.71772862 \n",
            "BATCH: 589/1074\n",
            " train loss item: 0.6380603909492493\n",
            " f1:0.29773296\n",
            " avg_train_loss: 0.71768072 \n",
            "BATCH: 590/1074\n",
            " train loss item: 0.7148152589797974\n",
            " f1:0.29755404\n",
            " avg_train_loss: 0.71767899 \n",
            "BATCH: 591/1074\n",
            " train loss item: 0.7211654186248779\n",
            " f1:0.29737533\n",
            " avg_train_loss: 0.71768109 \n",
            "BATCH: 592/1074\n",
            " train loss item: 0.7004604339599609\n",
            " f1:0.29738638\n",
            " avg_train_loss: 0.71767075 \n",
            "BATCH: 593/1074\n",
            " train loss item: 0.6777524352073669\n",
            " f1:0.29720798\n",
            " avg_train_loss: 0.71764681 \n",
            "BATCH: 594/1074\n",
            " train loss item: 0.6885645985603333\n",
            " f1:0.29702980\n",
            " avg_train_loss: 0.71762937 \n",
            "BATCH: 595/1074\n",
            " train loss item: 0.7001135349273682\n",
            " f1:0.29685183\n",
            " avg_train_loss: 0.71761888 \n",
            "BATCH: 596/1074\n",
            " train loss item: 0.907773494720459\n",
            " f1:0.29667408\n",
            " avg_train_loss: 0.71773274 \n",
            "BATCH: 597/1074\n",
            " train loss item: 0.7077594995498657\n",
            " f1:0.29649653\n",
            " avg_train_loss: 0.71772677 \n",
            "BATCH: 598/1074\n",
            " train loss item: 0.6858692169189453\n",
            " f1:0.29674983\n",
            " avg_train_loss: 0.71770772 \n",
            "BATCH: 599/1074\n",
            " train loss item: 0.6653947830200195\n",
            " f1:0.29703224\n",
            " avg_train_loss: 0.71767645 \n",
            "BATCH: 600/1074\n",
            " train loss item: 0.7418776154518127\n",
            " f1:0.29728491\n",
            " avg_train_loss: 0.71769091 \n",
            "BATCH: 601/1074\n",
            " train loss item: 0.7454112768173218\n",
            " f1:0.29753728\n",
            " avg_train_loss: 0.71770746 \n",
            "BATCH: 602/1074\n",
            " train loss item: 0.9729282855987549\n",
            " f1:0.29759841\n",
            " avg_train_loss: 0.71785974 \n",
            "BATCH: 603/1074\n",
            " train loss item: 0.7519674897193909\n",
            " f1:0.29774621\n",
            " avg_train_loss: 0.71788007 \n",
            "BATCH: 604/1074\n",
            " train loss item: 0.6868883371353149\n",
            " f1:0.29756877\n",
            " avg_train_loss: 0.71786160 \n",
            "BATCH: 605/1074\n",
            " train loss item: 0.7221856117248535\n",
            " f1:0.29739154\n",
            " avg_train_loss: 0.71786418 \n",
            "BATCH: 606/1074\n",
            " train loss item: 0.7341392040252686\n",
            " f1:0.29721452\n",
            " avg_train_loss: 0.71787387 \n",
            "BATCH: 607/1074\n",
            " train loss item: 0.685464084148407\n",
            " f1:0.29703771\n",
            " avg_train_loss: 0.71785459 \n",
            "BATCH: 608/1074\n",
            " train loss item: 0.6610332727432251\n",
            " f1:0.29686111\n",
            " avg_train_loss: 0.71782081 \n",
            "BATCH: 609/1074\n",
            " train loss item: 0.6793049573898315\n",
            " f1:0.29668473\n",
            " avg_train_loss: 0.71779792 \n",
            "BATCH: 610/1074\n",
            " train loss item: 0.7337894439697266\n",
            " f1:0.29650855\n",
            " avg_train_loss: 0.71780742 \n",
            "BATCH: 611/1074\n",
            " train loss item: 0.6658949255943298\n",
            " f1:0.29633258\n",
            " avg_train_loss: 0.71777661 \n",
            "BATCH: 612/1074\n",
            " train loss item: 0.7086963057518005\n",
            " f1:0.29615682\n",
            " avg_train_loss: 0.71777122 \n",
            "BATCH: 613/1074\n",
            " train loss item: 0.6886626482009888\n",
            " f1:0.29598127\n",
            " avg_train_loss: 0.71775397 \n",
            "BATCH: 614/1074\n",
            " train loss item: 0.7355597019195557\n",
            " f1:0.29580592\n",
            " avg_train_loss: 0.71776452 \n",
            "BATCH: 615/1074\n",
            " train loss item: 0.6905001401901245\n",
            " f1:0.29605707\n",
            " avg_train_loss: 0.71774837 \n",
            "BATCH: 616/1074\n",
            " train loss item: 0.6623796224594116\n",
            " f1:0.29633706\n",
            " avg_train_loss: 0.71771561 \n",
            "BATCH: 617/1074\n",
            " train loss item: 0.9597582817077637\n",
            " f1:0.29648438\n",
            " avg_train_loss: 0.71785875 \n",
            "BATCH: 618/1074\n",
            " train loss item: 0.6622341275215149\n",
            " f1:0.29676378\n",
            " avg_train_loss: 0.71782587 \n",
            "BATCH: 619/1074\n",
            " train loss item: 0.6843239068984985\n",
            " f1:0.29658849\n",
            " avg_train_loss: 0.71780608 \n",
            "BATCH: 620/1074\n",
            " train loss item: 0.6213489770889282\n",
            " f1:0.29641341\n",
            " avg_train_loss: 0.71774914 \n",
            "BATCH: 621/1074\n",
            " train loss item: 1.2319912910461426\n",
            " f1:0.29623853\n",
            " avg_train_loss: 0.71805253 \n",
            "BATCH: 622/1074\n",
            " train loss item: 0.5478401184082031\n",
            " f1:0.29606386\n",
            " avg_train_loss: 0.71795217 \n",
            "BATCH: 623/1074\n",
            " train loss item: 0.6854534149169922\n",
            " f1:0.29588940\n",
            " avg_train_loss: 0.71793302 \n",
            "BATCH: 624/1074\n",
            " train loss item: 0.6932808756828308\n",
            " f1:0.29571514\n",
            " avg_train_loss: 0.71791850 \n",
            "BATCH: 625/1074\n",
            " train loss item: 0.7755957841873169\n",
            " f1:0.29586213\n",
            " avg_train_loss: 0.71795245 \n",
            "BATCH: 626/1074\n",
            " train loss item: 0.6896244883537292\n",
            " f1:0.29611163\n",
            " avg_train_loss: 0.71793578 \n",
            "BATCH: 627/1074\n",
            " train loss item: 0.7539870738983154\n",
            " f1:0.29625821\n",
            " avg_train_loss: 0.71795698 \n",
            "BATCH: 628/1074\n",
            " train loss item: 0.6965257525444031\n",
            " f1:0.29631917\n",
            " avg_train_loss: 0.71794439 \n",
            "BATCH: 629/1074\n",
            " train loss item: 0.6619853377342224\n",
            " f1:0.29614517\n",
            " avg_train_loss: 0.71791153 \n",
            "BATCH: 630/1074\n",
            " train loss item: 0.7579036951065063\n",
            " f1:0.29597137\n",
            " avg_train_loss: 0.71793500 \n",
            "BATCH: 631/1074\n",
            " train loss item: 0.9388283491134644\n",
            " f1:0.29579778\n",
            " avg_train_loss: 0.71806455 \n",
            "BATCH: 632/1074\n",
            " train loss item: 0.7396848201751709\n",
            " f1:0.29562440\n",
            " avg_train_loss: 0.71807723 \n",
            "BATCH: 633/1074\n",
            " train loss item: 0.5885328054428101\n",
            " f1:0.29545121\n",
            " avg_train_loss: 0.71800134 \n",
            "BATCH: 634/1074\n",
            " train loss item: 0.6380603313446045\n",
            " f1:0.29527823\n",
            " avg_train_loss: 0.71795453 \n",
            "BATCH: 635/1074\n",
            " train loss item: 0.7037752270698547\n",
            " f1:0.29510545\n",
            " avg_train_loss: 0.71794624 \n",
            "BATCH: 636/1074\n",
            " train loss item: 0.674333393573761\n",
            " f1:0.29493288\n",
            " avg_train_loss: 0.71792073 \n",
            "BATCH: 637/1074\n",
            " train loss item: 0.6873274445533752\n",
            " f1:0.29476050\n",
            " avg_train_loss: 0.71790285 \n",
            "BATCH: 638/1074\n",
            " train loss item: 0.6887555718421936\n",
            " f1:0.29458833\n",
            " avg_train_loss: 0.71788583 \n",
            "BATCH: 639/1074\n",
            " train loss item: 0.6896820068359375\n",
            " f1:0.29441636\n",
            " avg_train_loss: 0.71786936 \n",
            "BATCH: 640/1074\n",
            " train loss item: 0.7007014751434326\n",
            " f1:0.29424459\n",
            " avg_train_loss: 0.71785934 \n",
            "BATCH: 641/1074\n",
            " train loss item: 0.687735915184021\n",
            " f1:0.29407301\n",
            " avg_train_loss: 0.71784178 \n",
            "BATCH: 642/1074\n",
            " train loss item: 0.6958119869232178\n",
            " f1:0.29390164\n",
            " avg_train_loss: 0.71782894 \n",
            "BATCH: 643/1074\n",
            " train loss item: 0.6909564733505249\n",
            " f1:0.29373047\n",
            " avg_train_loss: 0.71781329 \n",
            "BATCH: 644/1074\n",
            " train loss item: 0.6904555559158325\n",
            " f1:0.29355950\n",
            " avg_train_loss: 0.71779737 \n",
            "BATCH: 645/1074\n",
            " train loss item: 0.6853502988815308\n",
            " f1:0.29338873\n",
            " avg_train_loss: 0.71777849 \n",
            "BATCH: 646/1074\n",
            " train loss item: 0.687960147857666\n",
            " f1:0.29321815\n",
            " avg_train_loss: 0.71776116 \n",
            "BATCH: 647/1074\n",
            " train loss item: 0.7142511010169983\n",
            " f1:0.29304777\n",
            " avg_train_loss: 0.71775912 \n",
            "BATCH: 648/1074\n",
            " train loss item: 0.6961700320243835\n",
            " f1:0.29287760\n",
            " avg_train_loss: 0.71774658 \n",
            "BATCH: 649/1074\n",
            " train loss item: 0.676692008972168\n",
            " f1:0.29270761\n",
            " avg_train_loss: 0.71772275 \n",
            "BATCH: 650/1074\n",
            " train loss item: 0.6644473671913147\n",
            " f1:0.29253783\n",
            " avg_train_loss: 0.71769185 \n",
            "BATCH: 651/1074\n",
            " train loss item: 0.7398629188537598\n",
            " f1:0.29236824\n",
            " avg_train_loss: 0.71770470 \n",
            "BATCH: 652/1074\n",
            " train loss item: 0.6855454444885254\n",
            " f1:0.29219885\n",
            " avg_train_loss: 0.71768607 \n",
            "BATCH: 653/1074\n",
            " train loss item: 0.6604262590408325\n",
            " f1:0.29202966\n",
            " avg_train_loss: 0.71765291 \n",
            "BATCH: 654/1074\n",
            " train loss item: 0.671424150466919\n",
            " f1:0.29186066\n",
            " avg_train_loss: 0.71762616 \n",
            "BATCH: 655/1074\n",
            " train loss item: 0.717663049697876\n",
            " f1:0.29169186\n",
            " avg_train_loss: 0.71762618 \n",
            "BATCH: 656/1074\n",
            " train loss item: 0.6361045241355896\n",
            " f1:0.29152325\n",
            " avg_train_loss: 0.71757906 \n",
            "BATCH: 657/1074\n",
            " train loss item: 0.6472703814506531\n",
            " f1:0.29135484\n",
            " avg_train_loss: 0.71753844 \n",
            "BATCH: 658/1074\n",
            " train loss item: 0.8287825584411621\n",
            " f1:0.29118662\n",
            " avg_train_loss: 0.71760267 \n",
            "BATCH: 659/1074\n",
            " train loss item: 0.7483211755752563\n",
            " f1:0.29101859\n",
            " avg_train_loss: 0.71762040 \n",
            "BATCH: 660/1074\n",
            " train loss item: 0.7475557923316956\n",
            " f1:0.29085076\n",
            " avg_train_loss: 0.71763766 \n",
            "BATCH: 661/1074\n",
            " train loss item: 0.7412118911743164\n",
            " f1:0.29068312\n",
            " avg_train_loss: 0.71765125 \n",
            "BATCH: 662/1074\n",
            " train loss item: 0.68757164478302\n",
            " f1:0.29051568\n",
            " avg_train_loss: 0.71763392 \n",
            "BATCH: 663/1074\n",
            " train loss item: 0.6947668790817261\n",
            " f1:0.29069886\n",
            " avg_train_loss: 0.71762076 \n",
            "BATCH: 664/1074\n",
            " train loss item: 0.6933733224868774\n",
            " f1:0.29091518\n",
            " avg_train_loss: 0.71760681 \n",
            "BATCH: 665/1074\n",
            " train loss item: 0.7004443407058716\n",
            " f1:0.29102172\n",
            " avg_train_loss: 0.71759694 \n",
            "BATCH: 666/1074\n",
            " train loss item: 0.6914700269699097\n",
            " f1:0.29085447\n",
            " avg_train_loss: 0.71758192 \n",
            "BATCH: 667/1074\n",
            " train loss item: 0.7105806469917297\n",
            " f1:0.29068741\n",
            " avg_train_loss: 0.71757790 \n",
            "BATCH: 668/1074\n",
            " train loss item: 0.6752952933311462\n",
            " f1:0.29052054\n",
            " avg_train_loss: 0.71755363 \n",
            "BATCH: 669/1074\n",
            " train loss item: 0.6370986700057983\n",
            " f1:0.29035386\n",
            " avg_train_loss: 0.71750747 \n",
            "BATCH: 670/1074\n",
            " train loss item: 0.7441229820251465\n",
            " f1:0.29018737\n",
            " avg_train_loss: 0.71752273 \n",
            "BATCH: 671/1074\n",
            " train loss item: 0.6936311721801758\n",
            " f1:0.29002107\n",
            " avg_train_loss: 0.71750904 \n",
            "BATCH: 672/1074\n",
            " train loss item: 0.7296287417411804\n",
            " f1:0.28985497\n",
            " avg_train_loss: 0.71751598 \n",
            "BATCH: 673/1074\n",
            " train loss item: 0.7821291089057922\n",
            " f1:0.28968905\n",
            " avg_train_loss: 0.71755296 \n",
            "BATCH: 674/1074\n",
            " train loss item: 0.7205941081047058\n",
            " f1:0.28952333\n",
            " avg_train_loss: 0.71755470 \n",
            "BATCH: 675/1074\n",
            " train loss item: 0.6935417056083679\n",
            " f1:0.28935779\n",
            " avg_train_loss: 0.71754097 \n",
            "BATCH: 676/1074\n",
            " train loss item: 0.6872545480728149\n",
            " f1:0.28960387\n",
            " avg_train_loss: 0.71752367 \n",
            "BATCH: 677/1074\n",
            " train loss item: 0.7554212808609009\n",
            " f1:0.28971043\n",
            " avg_train_loss: 0.71754531 \n",
            "BATCH: 678/1074\n",
            " train loss item: 0.7046539187431335\n",
            " f1:0.28992559\n",
            " avg_train_loss: 0.71753795 \n",
            "BATCH: 679/1074\n",
            " train loss item: 0.6854835748672485\n",
            " f1:0.29017092\n",
            " avg_train_loss: 0.71751967 \n",
            "BATCH: 680/1074\n",
            " train loss item: 0.7018427848815918\n",
            " f1:0.29038557\n",
            " avg_train_loss: 0.71751073 \n",
            "BATCH: 681/1074\n",
            " train loss item: 0.7263209223747253\n",
            " f1:0.29053091\n",
            " avg_train_loss: 0.71751575 \n",
            "BATCH: 682/1074\n",
            " train loss item: 0.7193324565887451\n",
            " f1:0.29063664\n",
            " avg_train_loss: 0.71751678 \n",
            "BATCH: 683/1074\n",
            " train loss item: 0.6940315961837769\n",
            " f1:0.29078167\n",
            " avg_train_loss: 0.71750342 \n",
            "BATCH: 684/1074\n",
            " train loss item: 0.6876766085624695\n",
            " f1:0.29061627\n",
            " avg_train_loss: 0.71748645 \n",
            "BATCH: 685/1074\n",
            " train loss item: 0.6584993600845337\n",
            " f1:0.29045105\n",
            " avg_train_loss: 0.71745292 \n",
            "BATCH: 686/1074\n",
            " train loss item: 0.7269169688224792\n",
            " f1:0.29028602\n",
            " avg_train_loss: 0.71745829 \n",
            "BATCH: 687/1074\n",
            " train loss item: 0.7593722343444824\n",
            " f1:0.29012118\n",
            " avg_train_loss: 0.71748210 \n",
            "BATCH: 688/1074\n",
            " train loss item: 0.7753444910049438\n",
            " f1:0.28995652\n",
            " avg_train_loss: 0.71751494 \n",
            "BATCH: 689/1074\n",
            " train loss item: 0.7002882957458496\n",
            " f1:0.28979206\n",
            " avg_train_loss: 0.71750516 \n",
            "BATCH: 690/1074\n",
            " train loss item: 0.7024217844009399\n",
            " f1:0.28962778\n",
            " avg_train_loss: 0.71749661 \n",
            "BATCH: 691/1074\n",
            " train loss item: 0.6926845908164978\n",
            " f1:0.28987161\n",
            " avg_train_loss: 0.71748256 \n",
            "BATCH: 692/1074\n",
            " train loss item: 0.7292520403862\n",
            " f1:0.28993397\n",
            " avg_train_loss: 0.71748922 \n",
            "BATCH: 693/1074\n",
            " train loss item: 0.6860766410827637\n",
            " f1:0.29017736\n",
            " avg_train_loss: 0.71747144 \n",
            "BATCH: 694/1074\n",
            " train loss item: 0.7499743700027466\n",
            " f1:0.29023948\n",
            " avg_train_loss: 0.71748983 \n",
            "BATCH: 695/1074\n",
            " train loss item: 0.6679266691207886\n",
            " f1:0.29053602\n",
            " avg_train_loss: 0.71746181 \n",
            "BATCH: 696/1074\n",
            " train loss item: 0.6787463426589966\n",
            " f1:0.29080646\n",
            " avg_train_loss: 0.71743994 \n",
            "BATCH: 697/1074\n",
            " train loss item: 0.6954836845397949\n",
            " f1:0.29101869\n",
            " avg_train_loss: 0.71742754 \n",
            "BATCH: 698/1074\n",
            " train loss item: 0.695267915725708\n",
            " f1:0.29123069\n",
            " avg_train_loss: 0.71741503 \n",
            "BATCH: 699/1074\n",
            " train loss item: 0.7163149118423462\n",
            " f1:0.29133501\n",
            " avg_train_loss: 0.71741441 \n",
            "BATCH: 700/1074\n",
            " train loss item: 0.6846411228179932\n",
            " f1:0.29163009\n",
            " avg_train_loss: 0.71739594 \n",
            "BATCH: 701/1074\n",
            " train loss item: 0.691617488861084\n",
            " f1:0.29187143\n",
            " avg_train_loss: 0.71738141 \n",
            "BATCH: 702/1074\n",
            " train loss item: 0.6931930780410767\n",
            " f1:0.29208246\n",
            " avg_train_loss: 0.71736780 \n",
            "BATCH: 703/1074\n",
            " train loss item: 0.6924254298210144\n",
            " f1:0.29232327\n",
            " avg_train_loss: 0.71735376 \n",
            "BATCH: 704/1074\n",
            " train loss item: 0.6931927800178528\n",
            " f1:0.29253381\n",
            " avg_train_loss: 0.71734017 \n",
            "BATCH: 705/1074\n",
            " train loss item: 0.6932226419448853\n",
            " f1:0.29274411\n",
            " avg_train_loss: 0.71732661 \n",
            "BATCH: 706/1074\n",
            " train loss item: 0.689741849899292\n",
            " f1:0.29301180\n",
            " avg_train_loss: 0.71731112 \n",
            "BATCH: 707/1074\n",
            " train loss item: 0.7051548957824707\n",
            " f1:0.29311466\n",
            " avg_train_loss: 0.71730429 \n",
            "BATCH: 708/1074\n",
            " train loss item: 0.6980969309806824\n",
            " f1:0.29325626\n",
            " avg_train_loss: 0.71729351 \n",
            "BATCH: 709/1074\n",
            " train loss item: 0.6910164952278137\n",
            " f1:0.29309179\n",
            " avg_train_loss: 0.71727877 \n",
            "BATCH: 710/1074\n",
            " train loss item: 0.6823217868804932\n",
            " f1:0.29292750\n",
            " avg_train_loss: 0.71725918 \n",
            "BATCH: 711/1074\n",
            " train loss item: 0.6621386408805847\n",
            " f1:0.29276339\n",
            " avg_train_loss: 0.71722830 \n",
            "BATCH: 712/1074\n",
            " train loss item: 0.7243884205818176\n",
            " f1:0.29259947\n",
            " avg_train_loss: 0.71723231 \n",
            "BATCH: 713/1074\n",
            " train loss item: 0.7340306043624878\n",
            " f1:0.29243573\n",
            " avg_train_loss: 0.71724171 \n",
            "BATCH: 714/1074\n",
            " train loss item: 0.7111217975616455\n",
            " f1:0.29227218\n",
            " avg_train_loss: 0.71723829 \n",
            "BATCH: 715/1074\n",
            " train loss item: 0.7088247537612915\n",
            " f1:0.29210881\n",
            " avg_train_loss: 0.71723358 \n",
            "BATCH: 716/1074\n",
            " train loss item: 0.7245874404907227\n",
            " f1:0.29194562\n",
            " avg_train_loss: 0.71723769 \n",
            "BATCH: 717/1074\n",
            " train loss item: 0.7008136510848999\n",
            " f1:0.29178261\n",
            " avg_train_loss: 0.71722852 \n",
            "BATCH: 718/1074\n",
            " train loss item: 0.6857327222824097\n",
            " f1:0.29161979\n",
            " avg_train_loss: 0.71721095 \n",
            "BATCH: 719/1074\n",
            " train loss item: 0.6864603757858276\n",
            " f1:0.29145714\n",
            " avg_train_loss: 0.71719380 \n",
            "BATCH: 720/1074\n",
            " train loss item: 0.6790451407432556\n",
            " f1:0.29129468\n",
            " avg_train_loss: 0.71717253 \n",
            "BATCH: 721/1074\n",
            " train loss item: 0.7237358093261719\n",
            " f1:0.29113240\n",
            " avg_train_loss: 0.71717619 \n",
            "BATCH: 722/1074\n",
            " train loss item: 0.6881778836250305\n",
            " f1:0.29097030\n",
            " avg_train_loss: 0.71716004 \n",
            "BATCH: 723/1074\n",
            " train loss item: 0.688530445098877\n",
            " f1:0.29080838\n",
            " avg_train_loss: 0.71714411 \n",
            "BATCH: 724/1074\n",
            " train loss item: 0.704931378364563\n",
            " f1:0.29064664\n",
            " avg_train_loss: 0.71713732 \n",
            "BATCH: 725/1074\n",
            " train loss item: 0.6836812496185303\n",
            " f1:0.29048508\n",
            " avg_train_loss: 0.71711872 \n",
            "BATCH: 726/1074\n",
            " train loss item: 0.6834592819213867\n",
            " f1:0.29032370\n",
            " avg_train_loss: 0.71710002 \n",
            "BATCH: 727/1074\n",
            " train loss item: 0.7059831619262695\n",
            " f1:0.29016250\n",
            " avg_train_loss: 0.71709385 \n",
            "BATCH: 728/1074\n",
            " train loss item: 0.6643326282501221\n",
            " f1:0.29000147\n",
            " avg_train_loss: 0.71706457 \n",
            "BATCH: 729/1074\n",
            " train loss item: 0.6949138641357422\n",
            " f1:0.28984063\n",
            " avg_train_loss: 0.71705228 \n",
            "BATCH: 730/1074\n",
            " train loss item: 0.6956104040145874\n",
            " f1:0.28967996\n",
            " avg_train_loss: 0.71704040 \n",
            "BATCH: 731/1074\n",
            " train loss item: 0.7061986327171326\n",
            " f1:0.28951948\n",
            " avg_train_loss: 0.71703439 \n",
            "BATCH: 732/1074\n",
            " train loss item: 0.6862399578094482\n",
            " f1:0.28935917\n",
            " avg_train_loss: 0.71701734 \n",
            "BATCH: 733/1074\n",
            " train loss item: 0.6860694289207458\n",
            " f1:0.28919903\n",
            " avg_train_loss: 0.71700021 \n",
            "BATCH: 734/1074\n",
            " train loss item: 0.6744149923324585\n",
            " f1:0.28903908\n",
            " avg_train_loss: 0.71697666 \n",
            "BATCH: 735/1074\n",
            " train loss item: 0.6983605623245239\n",
            " f1:0.28887930\n",
            " avg_train_loss: 0.71696637 \n",
            "BATCH: 736/1074\n",
            " train loss item: 0.7125134468078613\n",
            " f1:0.28871970\n",
            " avg_train_loss: 0.71696391 \n",
            "BATCH: 737/1074\n",
            " train loss item: 0.6985775232315063\n",
            " f1:0.28856027\n",
            " avg_train_loss: 0.71695376 \n",
            "BATCH: 738/1074\n",
            " train loss item: 0.6978293061256409\n",
            " f1:0.28840102\n",
            " avg_train_loss: 0.71694320 \n",
            "BATCH: 739/1074\n",
            " train loss item: 0.7186274528503418\n",
            " f1:0.28824195\n",
            " avg_train_loss: 0.71694413 \n",
            "BATCH: 740/1074\n",
            " train loss item: 0.6954237222671509\n",
            " f1:0.28808305\n",
            " avg_train_loss: 0.71693227 \n",
            "BATCH: 741/1074\n",
            " train loss item: 0.6943839192390442\n",
            " f1:0.28792433\n",
            " avg_train_loss: 0.71691984 \n",
            "BATCH: 742/1074\n",
            " train loss item: 0.6979711651802063\n",
            " f1:0.28776578\n",
            " avg_train_loss: 0.71690941 \n",
            "BATCH: 743/1074\n",
            " train loss item: 0.6932984590530396\n",
            " f1:0.28760741\n",
            " avg_train_loss: 0.71689641 \n",
            "BATCH: 744/1074\n",
            " train loss item: 0.6939471960067749\n",
            " f1:0.28744921\n",
            " avg_train_loss: 0.71688379 \n",
            "BATCH: 745/1074\n",
            " train loss item: 0.6932653188705444\n",
            " f1:0.28765768\n",
            " avg_train_loss: 0.71687081 \n",
            "BATCH: 746/1074\n",
            " train loss item: 0.6975818872451782\n",
            " f1:0.28783408\n",
            " avg_train_loss: 0.71686021 \n",
            "BATCH: 747/1074\n",
            " train loss item: 0.7189196348190308\n",
            " f1:0.28784943\n",
            " avg_train_loss: 0.71686134 \n",
            "BATCH: 748/1074\n",
            " train loss item: 0.685443639755249\n",
            " f1:0.28813865\n",
            " avg_train_loss: 0.71684410 \n",
            "BATCH: 749/1074\n",
            " train loss item: 0.6932708621025085\n",
            " f1:0.28834629\n",
            " avg_train_loss: 0.71683117 \n",
            "BATCH: 750/1074\n",
            " train loss item: 0.6968702077865601\n",
            " f1:0.28844928\n",
            " avg_train_loss: 0.71682022 \n",
            "BATCH: 751/1074\n",
            " train loss item: 0.6971163749694824\n",
            " f1:0.28829122\n",
            " avg_train_loss: 0.71680943 \n",
            "BATCH: 752/1074\n",
            " train loss item: 0.6931891441345215\n",
            " f1:0.28813334\n",
            " avg_train_loss: 0.71679649 \n",
            "BATCH: 753/1074\n",
            " train loss item: 0.6906586289405823\n",
            " f1:0.28797563\n",
            " avg_train_loss: 0.71678218 \n",
            "BATCH: 754/1074\n",
            " train loss item: 0.6956139802932739\n",
            " f1:0.28781810\n",
            " avg_train_loss: 0.71677060 \n",
            "BATCH: 755/1074\n",
            " train loss item: 0.6960806846618652\n",
            " f1:0.28766073\n",
            " avg_train_loss: 0.71675929 \n",
            "BATCH: 756/1074\n",
            " train loss item: 0.690834641456604\n",
            " f1:0.28750354\n",
            " avg_train_loss: 0.71674512 \n",
            "BATCH: 757/1074\n",
            " train loss item: 0.6878818869590759\n",
            " f1:0.28734652\n",
            " avg_train_loss: 0.71672936 \n",
            "BATCH: 758/1074\n",
            " train loss item: 0.6820366382598877\n",
            " f1:0.28718967\n",
            " avg_train_loss: 0.71671042 \n",
            "BATCH: 759/1074\n",
            " train loss item: 0.6943749189376831\n",
            " f1:0.28703299\n",
            " avg_train_loss: 0.71669824 \n",
            "BATCH: 760/1074\n",
            " train loss item: 0.679050087928772\n",
            " f1:0.28687649\n",
            " avg_train_loss: 0.71667771 \n",
            "BATCH: 761/1074\n",
            " train loss item: 0.664987325668335\n",
            " f1:0.28672015\n",
            " avg_train_loss: 0.71664954 \n",
            "BATCH: 762/1074\n",
            " train loss item: 0.6996252536773682\n",
            " f1:0.28656399\n",
            " avg_train_loss: 0.71664027 \n",
            "BATCH: 763/1074\n",
            " train loss item: 0.6512603759765625\n",
            " f1:0.28640799\n",
            " avg_train_loss: 0.71660468 \n",
            "BATCH: 764/1074\n",
            " train loss item: 0.6863100528717041\n",
            " f1:0.28625217\n",
            " avg_train_loss: 0.71658820 \n",
            "BATCH: 765/1074\n",
            " train loss item: 0.7131962180137634\n",
            " f1:0.28609651\n",
            " avg_train_loss: 0.71658635 \n",
            "BATCH: 766/1074\n",
            " train loss item: 0.662422776222229\n",
            " f1:0.28594102\n",
            " avg_train_loss: 0.71655691 \n",
            "BATCH: 767/1074\n",
            " train loss item: 0.7186063528060913\n",
            " f1:0.28578570\n",
            " avg_train_loss: 0.71655803 \n",
            "BATCH: 768/1074\n",
            " train loss item: 0.716742992401123\n",
            " f1:0.28563055\n",
            " avg_train_loss: 0.71655813 \n",
            "BATCH: 769/1074\n",
            " train loss item: 0.7354078888893127\n",
            " f1:0.28547557\n",
            " avg_train_loss: 0.71656836 \n",
            "BATCH: 770/1074\n",
            " train loss item: 0.7225056290626526\n",
            " f1:0.28532076\n",
            " avg_train_loss: 0.71657158 \n",
            "BATCH: 771/1074\n",
            " train loss item: 0.710722804069519\n",
            " f1:0.28516611\n",
            " avg_train_loss: 0.71656841 \n",
            "BATCH: 772/1074\n",
            " train loss item: 0.6948941946029663\n",
            " f1:0.28501164\n",
            " avg_train_loss: 0.71655666 \n",
            "BATCH: 773/1074\n",
            " train loss item: 0.6898269653320312\n",
            " f1:0.28485733\n",
            " avg_train_loss: 0.71654219 \n",
            "BATCH: 774/1074\n",
            " train loss item: 0.6949586272239685\n",
            " f1:0.28470318\n",
            " avg_train_loss: 0.71653051 \n",
            "BATCH: 775/1074\n",
            " train loss item: 0.6931499242782593\n",
            " f1:0.28454920\n",
            " avg_train_loss: 0.71651787 \n",
            "BATCH: 776/1074\n",
            " train loss item: 0.6918879747390747\n",
            " f1:0.28483583\n",
            " avg_train_loss: 0.71650455 \n",
            "BATCH: 777/1074\n",
            " train loss item: 0.6932982802391052\n",
            " f1:0.28504212\n",
            " avg_train_loss: 0.71649202 \n",
            "BATCH: 778/1074\n",
            " train loss item: 0.6935746669769287\n",
            " f1:0.28524818\n",
            " avg_train_loss: 0.71647964 \n",
            "BATCH: 779/1074\n",
            " train loss item: 0.6986187100410461\n",
            " f1:0.28542273\n",
            " avg_train_loss: 0.71647000 \n",
            "BATCH: 780/1074\n",
            " train loss item: 0.6936938762664795\n",
            " f1:0.28562837\n",
            " avg_train_loss: 0.71645772 \n",
            "BATCH: 781/1074\n",
            " train loss item: 0.6993794441223145\n",
            " f1:0.28576843\n",
            " avg_train_loss: 0.71644851 \n",
            "BATCH: 782/1074\n",
            " train loss item: 0.691635012626648\n",
            " f1:0.28561446\n",
            " avg_train_loss: 0.71643514 \n",
            "BATCH: 783/1074\n",
            " train loss item: 0.6571038961410522\n",
            " f1:0.28546066\n",
            " avg_train_loss: 0.71640319 \n",
            "BATCH: 784/1074\n",
            " train loss item: 0.72865891456604\n",
            " f1:0.28530702\n",
            " avg_train_loss: 0.71640979 \n",
            "BATCH: 785/1074\n",
            " train loss item: 0.6027271151542664\n",
            " f1:0.28515355\n",
            " avg_train_loss: 0.71634864 \n",
            "BATCH: 786/1074\n",
            " train loss item: 0.7489302158355713\n",
            " f1:0.28500024\n",
            " avg_train_loss: 0.71636615 \n",
            "BATCH: 787/1074\n",
            " train loss item: 0.6682009100914001\n",
            " f1:0.28484710\n",
            " avg_train_loss: 0.71634027 \n",
            "BATCH: 788/1074\n",
            " train loss item: 0.7674344778060913\n",
            " f1:0.28469412\n",
            " avg_train_loss: 0.71636771 \n",
            "BATCH: 789/1074\n",
            " train loss item: 0.7073432803153992\n",
            " f1:0.28454130\n",
            " avg_train_loss: 0.71636287 \n",
            "BATCH: 790/1074\n",
            " train loss item: 0.7975207567214966\n",
            " f1:0.28438865\n",
            " avg_train_loss: 0.71640641 \n",
            "BATCH: 791/1074\n",
            " train loss item: 0.7078766226768494\n",
            " f1:0.28423616\n",
            " avg_train_loss: 0.71640184 \n",
            "BATCH: 792/1074\n",
            " train loss item: 0.6614966988563538\n",
            " f1:0.28408384\n",
            " avg_train_loss: 0.71637241 \n",
            "BATCH: 793/1074\n",
            " train loss item: 0.6800990700721741\n",
            " f1:0.28393168\n",
            " avg_train_loss: 0.71635298 \n",
            "BATCH: 794/1074\n",
            " train loss item: 0.6991686820983887\n",
            " f1:0.28377968\n",
            " avg_train_loss: 0.71634378 \n",
            "BATCH: 795/1074\n",
            " train loss item: 0.6855884790420532\n",
            " f1:0.28362785\n",
            " avg_train_loss: 0.71632733 \n",
            "BATCH: 796/1074\n",
            " train loss item: 0.6979612112045288\n",
            " f1:0.28347617\n",
            " avg_train_loss: 0.71631751 \n",
            "BATCH: 797/1074\n",
            " train loss item: 0.69884192943573\n",
            " f1:0.28332466\n",
            " avg_train_loss: 0.71630817 \n",
            "BATCH: 798/1074\n",
            " train loss item: 0.6942072510719299\n",
            " f1:0.28317331\n",
            " avg_train_loss: 0.71629636 \n",
            "BATCH: 799/1074\n",
            " train loss item: 0.7013681530952454\n",
            " f1:0.28302213\n",
            " avg_train_loss: 0.71628839 \n",
            "BATCH: 800/1074\n",
            " train loss item: 0.6948755979537964\n",
            " f1:0.28287110\n",
            " avg_train_loss: 0.71627696 \n",
            "BATCH: 801/1074\n",
            " train loss item: 0.7106614112854004\n",
            " f1:0.28272024\n",
            " avg_train_loss: 0.71627397 \n",
            "BATCH: 802/1074\n",
            " train loss item: 0.6877567172050476\n",
            " f1:0.28256953\n",
            " avg_train_loss: 0.71625877 \n",
            "BATCH: 803/1074\n",
            " train loss item: 0.707298994064331\n",
            " f1:0.28241899\n",
            " avg_train_loss: 0.71625399 \n",
            "BATCH: 804/1074\n",
            " train loss item: 0.7027764320373535\n",
            " f1:0.28226861\n",
            " avg_train_loss: 0.71624682 \n",
            "BATCH: 805/1074\n",
            " train loss item: 0.6857805252075195\n",
            " f1:0.28211838\n",
            " avg_train_loss: 0.71623060 \n",
            "BATCH: 806/1074\n",
            " train loss item: 0.6911360621452332\n",
            " f1:0.28196832\n",
            " avg_train_loss: 0.71621726 \n",
            "BATCH: 807/1074\n",
            " train loss item: 0.6899740099906921\n",
            " f1:0.28181842\n",
            " avg_train_loss: 0.71620330 \n",
            "BATCH: 808/1074\n",
            " train loss item: 0.6994646787643433\n",
            " f1:0.28166867\n",
            " avg_train_loss: 0.71619441 \n",
            "BATCH: 809/1074\n",
            " train loss item: 0.6943550109863281\n",
            " f1:0.28151909\n",
            " avg_train_loss: 0.71618281 \n",
            "BATCH: 810/1074\n",
            " train loss item: 0.7011054754257202\n",
            " f1:0.28136966\n",
            " avg_train_loss: 0.71617481 \n",
            "BATCH: 811/1074\n",
            " train loss item: 0.6701660752296448\n",
            " f1:0.28122039\n",
            " avg_train_loss: 0.71615040 \n",
            "BATCH: 812/1074\n",
            " train loss item: 0.6793212890625\n",
            " f1:0.28107129\n",
            " avg_train_loss: 0.71613087 \n",
            "BATCH: 813/1074\n",
            " train loss item: 0.6967519521713257\n",
            " f1:0.28092233\n",
            " avg_train_loss: 0.71612060 \n",
            "BATCH: 814/1074\n",
            " train loss item: 0.6856175661087036\n",
            " f1:0.28077354\n",
            " avg_train_loss: 0.71610445 \n",
            "BATCH: 815/1074\n",
            " train loss item: 0.7290112972259521\n",
            " f1:0.28062490\n",
            " avg_train_loss: 0.71611128 \n",
            "BATCH: 816/1074\n",
            " train loss item: 0.6853988170623779\n",
            " f1:0.28047643\n",
            " avg_train_loss: 0.71609503 \n",
            "BATCH: 817/1074\n",
            " train loss item: 0.7128808498382568\n",
            " f1:0.28032810\n",
            " avg_train_loss: 0.71609333 \n",
            "BATCH: 818/1074\n",
            " train loss item: 0.6978726387023926\n",
            " f1:0.28017994\n",
            " avg_train_loss: 0.71608370 \n",
            "BATCH: 819/1074\n",
            " train loss item: 0.6550430655479431\n",
            " f1:0.28003193\n",
            " avg_train_loss: 0.71605145 \n",
            "BATCH: 820/1074\n",
            " train loss item: 0.7288531064987183\n",
            " f1:0.27988408\n",
            " avg_train_loss: 0.71605821 \n",
            "BATCH: 821/1074\n",
            " train loss item: 0.7047896385192871\n",
            " f1:0.27973638\n",
            " avg_train_loss: 0.71605227 \n",
            "BATCH: 822/1074\n",
            " train loss item: 0.6946947574615479\n",
            " f1:0.27958884\n",
            " avg_train_loss: 0.71604100 \n",
            "BATCH: 823/1074\n",
            " train loss item: 0.6939740180969238\n",
            " f1:0.27944146\n",
            " avg_train_loss: 0.71602937 \n",
            "BATCH: 824/1074\n",
            " train loss item: 0.6866441965103149\n",
            " f1:0.27929423\n",
            " avg_train_loss: 0.71601389 \n",
            "BATCH: 825/1074\n",
            " train loss item: 0.6906972527503967\n",
            " f1:0.27914715\n",
            " avg_train_loss: 0.71600056 \n",
            "BATCH: 826/1074\n",
            " train loss item: 0.6933194398880005\n",
            " f1:0.27900023\n",
            " avg_train_loss: 0.71598862 \n",
            "BATCH: 827/1074\n",
            " train loss item: 0.6972718238830566\n",
            " f1:0.27885347\n",
            " avg_train_loss: 0.71597877 \n",
            "BATCH: 828/1074\n",
            " train loss item: 0.6896920204162598\n",
            " f1:0.27870686\n",
            " avg_train_loss: 0.71596495 \n",
            "BATCH: 829/1074\n",
            " train loss item: 0.6919336318969727\n",
            " f1:0.27856040\n",
            " avg_train_loss: 0.71595232 \n",
            "BATCH: 830/1074\n",
            " train loss item: 0.6948267221450806\n",
            " f1:0.27841410\n",
            " avg_train_loss: 0.71594123 \n",
            "BATCH: 831/1074\n",
            " train loss item: 0.69487464427948\n",
            " f1:0.27826795\n",
            " avg_train_loss: 0.71593017 \n",
            "BATCH: 832/1074\n",
            " train loss item: 0.6932140588760376\n",
            " f1:0.27812195\n",
            " avg_train_loss: 0.71591825 \n",
            "BATCH: 833/1074\n",
            " train loss item: 0.6894058585166931\n",
            " f1:0.27797611\n",
            " avg_train_loss: 0.71590435 \n",
            "BATCH: 834/1074\n",
            " train loss item: 0.6949977874755859\n",
            " f1:0.27783042\n",
            " avg_train_loss: 0.71589339 \n",
            "BATCH: 835/1074\n",
            " train loss item: 0.6854448318481445\n",
            " f1:0.27768488\n",
            " avg_train_loss: 0.71587744 \n",
            "BATCH: 836/1074\n",
            " train loss item: 0.6905515789985657\n",
            " f1:0.27753950\n",
            " avg_train_loss: 0.71586418 \n",
            "BATCH: 837/1074\n",
            " train loss item: 0.6935933828353882\n",
            " f1:0.27739427\n",
            " avg_train_loss: 0.71585253 \n",
            "BATCH: 838/1074\n",
            " train loss item: 0.6937733888626099\n",
            " f1:0.27724919\n",
            " avg_train_loss: 0.71584098 \n",
            "BATCH: 839/1074\n",
            " train loss item: 0.6790606379508972\n",
            " f1:0.27710426\n",
            " avg_train_loss: 0.71582175 \n",
            "BATCH: 840/1074\n",
            " train loss item: 0.6942048072814941\n",
            " f1:0.27695948\n",
            " avg_train_loss: 0.71581046 \n",
            "BATCH: 841/1074\n",
            " train loss item: 0.6944482326507568\n",
            " f1:0.27681485\n",
            " avg_train_loss: 0.71579930 \n",
            "BATCH: 842/1074\n",
            " train loss item: 0.6877968311309814\n",
            " f1:0.27667038\n",
            " avg_train_loss: 0.71578469 \n",
            "BATCH: 843/1074\n",
            " train loss item: 0.680185854434967\n",
            " f1:0.27652605\n",
            " avg_train_loss: 0.71576612 \n",
            "BATCH: 844/1074\n",
            " train loss item: 0.7032669186592102\n",
            " f1:0.27638188\n",
            " avg_train_loss: 0.71575960 \n",
            "BATCH: 845/1074\n",
            " train loss item: 0.695391058921814\n",
            " f1:0.27623786\n",
            " avg_train_loss: 0.71574899 \n",
            "BATCH: 846/1074\n",
            " train loss item: 0.7210845351219177\n",
            " f1:0.27609398\n",
            " avg_train_loss: 0.71575177 \n",
            "BATCH: 847/1074\n",
            " train loss item: 0.6794115304946899\n",
            " f1:0.27595026\n",
            " avg_train_loss: 0.71573285 \n",
            "BATCH: 848/1074\n",
            " train loss item: 0.6949690580368042\n",
            " f1:0.27580668\n",
            " avg_train_loss: 0.71572205 \n",
            "BATCH: 849/1074\n",
            " train loss item: 0.6947872638702393\n",
            " f1:0.27566326\n",
            " avg_train_loss: 0.71571116 \n",
            "BATCH: 850/1074\n",
            " train loss item: 0.6811622381210327\n",
            " f1:0.27551998\n",
            " avg_train_loss: 0.71569320 \n",
            "BATCH: 851/1074\n",
            " train loss item: 0.6813220977783203\n",
            " f1:0.27537685\n",
            " avg_train_loss: 0.71567535 \n",
            "BATCH: 852/1074\n",
            " train loss item: 0.7151393890380859\n",
            " f1:0.27523388\n",
            " avg_train_loss: 0.71567507 \n",
            "BATCH: 853/1074\n",
            " train loss item: 0.6944253444671631\n",
            " f1:0.27509105\n",
            " avg_train_loss: 0.71566404 \n",
            "BATCH: 854/1074\n",
            " train loss item: 0.699996292591095\n",
            " f1:0.27494836\n",
            " avg_train_loss: 0.71565592 \n",
            "BATCH: 855/1074\n",
            " train loss item: 0.7040082812309265\n",
            " f1:0.27480583\n",
            " avg_train_loss: 0.71564988 \n",
            "BATCH: 856/1074\n",
            " train loss item: 0.6936339735984802\n",
            " f1:0.27466344\n",
            " avg_train_loss: 0.71563847 \n",
            "BATCH: 857/1074\n",
            " train loss item: 0.6962462663650513\n",
            " f1:0.27452120\n",
            " avg_train_loss: 0.71562843 \n",
            "BATCH: 858/1074\n",
            " train loss item: 0.6899800300598145\n",
            " f1:0.27437911\n",
            " avg_train_loss: 0.71561515 \n",
            "BATCH: 859/1074\n",
            " train loss item: 0.6912024021148682\n",
            " f1:0.27423717\n",
            " avg_train_loss: 0.71560252 \n",
            "BATCH: 860/1074\n",
            " train loss item: 0.6876391172409058\n",
            " f1:0.27409537\n",
            " avg_train_loss: 0.71558806 \n",
            "BATCH: 861/1074\n",
            " train loss item: 0.6977351307868958\n",
            " f1:0.27395372\n",
            " avg_train_loss: 0.71557884 \n",
            "BATCH: 862/1074\n",
            " train loss item: 0.7076981663703918\n",
            " f1:0.27381221\n",
            " avg_train_loss: 0.71557477 \n",
            "BATCH: 863/1074\n",
            " train loss item: 0.6986923217773438\n",
            " f1:0.27367085\n",
            " avg_train_loss: 0.71556605 \n",
            "BATCH: 864/1074\n",
            " train loss item: 0.688940167427063\n",
            " f1:0.27352964\n",
            " avg_train_loss: 0.71555231 \n",
            "BATCH: 865/1074\n",
            " train loss item: 0.695320725440979\n",
            " f1:0.27338857\n",
            " avg_train_loss: 0.71554188 \n",
            "BATCH: 866/1074\n",
            " train loss item: 0.6932451725006104\n",
            " f1:0.27324765\n",
            " avg_train_loss: 0.71553039 \n",
            "BATCH: 867/1074\n",
            " train loss item: 0.6887938976287842\n",
            " f1:0.27310688\n",
            " avg_train_loss: 0.71551661 \n",
            "BATCH: 868/1074\n",
            " train loss item: 0.6915265321731567\n",
            " f1:0.27296624\n",
            " avg_train_loss: 0.71550426 \n",
            "BATCH: 869/1074\n",
            " train loss item: 0.6932824850082397\n",
            " f1:0.27282576\n",
            " avg_train_loss: 0.71549282 \n",
            "BATCH: 870/1074\n",
            " train loss item: 0.6933207511901855\n",
            " f1:0.27268541\n",
            " avg_train_loss: 0.71548142 \n",
            "BATCH: 871/1074\n",
            " train loss item: 0.698445737361908\n",
            " f1:0.27254522\n",
            " avg_train_loss: 0.71547266 \n",
            "BATCH: 872/1074\n",
            " train loss item: 0.698329508304596\n",
            " f1:0.27240516\n",
            " avg_train_loss: 0.71546385 \n",
            "BATCH: 873/1074\n",
            " train loss item: 0.6954973936080933\n",
            " f1:0.27226525\n",
            " avg_train_loss: 0.71545359 \n",
            "BATCH: 874/1074\n",
            " train loss item: 0.6896940469741821\n",
            " f1:0.27212548\n",
            " avg_train_loss: 0.71544037 \n",
            "BATCH: 875/1074\n",
            " train loss item: 0.6915834546089172\n",
            " f1:0.27198586\n",
            " avg_train_loss: 0.71542813 \n",
            "BATCH: 876/1074\n",
            " train loss item: 0.6849173307418823\n",
            " f1:0.27184638\n",
            " avg_train_loss: 0.71541248 \n",
            "BATCH: 877/1074\n",
            " train loss item: 0.6910092830657959\n",
            " f1:0.27170704\n",
            " avg_train_loss: 0.71539997 \n",
            "BATCH: 878/1074\n",
            " train loss item: 0.6876169443130493\n",
            " f1:0.27156785\n",
            " avg_train_loss: 0.71538574 \n",
            "BATCH: 879/1074\n",
            " train loss item: 0.6935366988182068\n",
            " f1:0.27142880\n",
            " avg_train_loss: 0.71537455 \n",
            "BATCH: 880/1074\n",
            " train loss item: 0.6936367750167847\n",
            " f1:0.27128989\n",
            " avg_train_loss: 0.71536343 \n",
            "BATCH: 881/1074\n",
            " train loss item: 0.6853021383285522\n",
            " f1:0.27115112\n",
            " avg_train_loss: 0.71534805 \n",
            "BATCH: 882/1074\n",
            " train loss item: 0.7027814388275146\n",
            " f1:0.27101250\n",
            " avg_train_loss: 0.71534163 \n",
            "BATCH: 883/1074\n",
            " train loss item: 0.6938199996948242\n",
            " f1:0.27087401\n",
            " avg_train_loss: 0.71533063 \n",
            "BATCH: 884/1074\n",
            " train loss item: 0.6846096515655518\n",
            " f1:0.27073567\n",
            " avg_train_loss: 0.71531494 \n",
            "BATCH: 885/1074\n",
            " train loss item: 0.6938514709472656\n",
            " f1:0.27059747\n",
            " avg_train_loss: 0.71530398 \n",
            "BATCH: 886/1074\n",
            " train loss item: 0.6749581694602966\n",
            " f1:0.27045941\n",
            " avg_train_loss: 0.71528340 \n",
            "BATCH: 887/1074\n",
            " train loss item: 0.693956732749939\n",
            " f1:0.27032149\n",
            " avg_train_loss: 0.71527252 \n",
            "BATCH: 888/1074\n",
            " train loss item: 0.6834498643875122\n",
            " f1:0.27018371\n",
            " avg_train_loss: 0.71525630 \n",
            "BATCH: 889/1074\n",
            " train loss item: 0.671202540397644\n",
            " f1:0.27004607\n",
            " avg_train_loss: 0.71523386 \n",
            "BATCH: 890/1074\n",
            " train loss item: 0.7080854773521423\n",
            " f1:0.26990858\n",
            " avg_train_loss: 0.71523022 \n",
            "BATCH: 891/1074\n",
            " train loss item: 0.6875593066215515\n",
            " f1:0.26977122\n",
            " avg_train_loss: 0.71521614 \n",
            "BATCH: 892/1074\n",
            " train loss item: 0.6872301697731018\n",
            " f1:0.26963400\n",
            " avg_train_loss: 0.71520191 \n",
            "BATCH: 893/1074\n",
            " train loss item: 0.7128914594650269\n",
            " f1:0.26949692\n",
            " avg_train_loss: 0.71520073 \n",
            "BATCH: 894/1074\n",
            " train loss item: 0.6609579920768738\n",
            " f1:0.26935998\n",
            " avg_train_loss: 0.71517317 \n",
            "BATCH: 895/1074\n",
            " train loss item: 0.6961102485656738\n",
            " f1:0.26922318\n",
            " avg_train_loss: 0.71516349 \n",
            "BATCH: 896/1074\n",
            " train loss item: 0.7173107862472534\n",
            " f1:0.26908652\n",
            " avg_train_loss: 0.71516458 \n",
            "BATCH: 897/1074\n",
            " train loss item: 0.7064415216445923\n",
            " f1:0.26895000\n",
            " avg_train_loss: 0.71516015 \n",
            "BATCH: 898/1074\n",
            " train loss item: 0.6958754062652588\n",
            " f1:0.26881361\n",
            " avg_train_loss: 0.71515037 \n",
            "BATCH: 899/1074\n",
            " train loss item: 0.7037744522094727\n",
            " f1:0.26867737\n",
            " avg_train_loss: 0.71514461 \n",
            "BATCH: 900/1074\n",
            " train loss item: 0.6875448822975159\n",
            " f1:0.26854126\n",
            " avg_train_loss: 0.71513062 \n",
            "BATCH: 901/1074\n",
            " train loss item: 0.7076576352119446\n",
            " f1:0.26840529\n",
            " avg_train_loss: 0.71512684 \n",
            "BATCH: 902/1074\n",
            " train loss item: 0.6773375272750854\n",
            " f1:0.26826946\n",
            " avg_train_loss: 0.71510772 \n",
            "BATCH: 903/1074\n",
            " train loss item: 0.6940093040466309\n",
            " f1:0.26813376\n",
            " avg_train_loss: 0.71509704 \n",
            "BATCH: 904/1074\n",
            " train loss item: 0.6938962936401367\n",
            " f1:0.26799820\n",
            " avg_train_loss: 0.71508633 \n",
            "BATCH: 905/1074\n",
            " train loss item: 0.6892645359039307\n",
            " f1:0.26786278\n",
            " avg_train_loss: 0.71507328 \n",
            "BATCH: 906/1074\n",
            " train loss item: 0.6937605142593384\n",
            " f1:0.26772750\n",
            " avg_train_loss: 0.71506251 \n",
            "BATCH: 907/1074\n",
            " train loss item: 0.689483642578125\n",
            " f1:0.26759235\n",
            " avg_train_loss: 0.71504960 \n",
            "BATCH: 908/1074\n",
            " train loss item: 0.702096164226532\n",
            " f1:0.26745734\n",
            " avg_train_loss: 0.71504307 \n",
            "BATCH: 909/1074\n",
            " train loss item: 0.6976612210273743\n",
            " f1:0.26732246\n",
            " avg_train_loss: 0.71503430 \n",
            "BATCH: 910/1074\n",
            " train loss item: 0.697333812713623\n",
            " f1:0.26718772\n",
            " avg_train_loss: 0.71502538 \n",
            "BATCH: 911/1074\n",
            " train loss item: 0.6901259422302246\n",
            " f1:0.26705312\n",
            " avg_train_loss: 0.71501284 \n",
            "BATCH: 912/1074\n",
            " train loss item: 0.6934647560119629\n",
            " f1:0.26691865\n",
            " avg_train_loss: 0.71500199 \n",
            "BATCH: 913/1074\n",
            " train loss item: 0.6905062794685364\n",
            " f1:0.26678432\n",
            " avg_train_loss: 0.71498966 \n",
            "BATCH: 914/1074\n",
            " train loss item: 0.6877824068069458\n",
            " f1:0.26665012\n",
            " avg_train_loss: 0.71497597 \n",
            "BATCH: 915/1074\n",
            " train loss item: 0.6934316158294678\n",
            " f1:0.26651606\n",
            " avg_train_loss: 0.71496514 \n",
            "BATCH: 916/1074\n",
            " train loss item: 0.6903377771377563\n",
            " f1:0.26638213\n",
            " avg_train_loss: 0.71495277 \n",
            "BATCH: 917/1074\n",
            " train loss item: 0.700240969657898\n",
            " f1:0.26624834\n",
            " avg_train_loss: 0.71494538 \n",
            "BATCH: 918/1074\n",
            " train loss item: 0.7000646591186523\n",
            " f1:0.26611468\n",
            " avg_train_loss: 0.71493791 \n",
            "BATCH: 919/1074\n",
            " train loss item: 0.6905289888381958\n",
            " f1:0.26598116\n",
            " avg_train_loss: 0.71492566 \n",
            "BATCH: 920/1074\n",
            " train loss item: 0.6933698654174805\n",
            " f1:0.26584777\n",
            " avg_train_loss: 0.71491485 \n",
            "BATCH: 921/1074\n",
            " train loss item: 0.6909496784210205\n",
            " f1:0.26571451\n",
            " avg_train_loss: 0.71490284 \n",
            "BATCH: 922/1074\n",
            " train loss item: 0.6956125497817993\n",
            " f1:0.26558139\n",
            " avg_train_loss: 0.71489317 \n",
            "BATCH: 923/1074\n",
            " train loss item: 0.6912750005722046\n",
            " f1:0.26544839\n",
            " avg_train_loss: 0.71488134 \n",
            "BATCH: 924/1074\n",
            " train loss item: 0.6932632923126221\n",
            " f1:0.26531554\n",
            " avg_train_loss: 0.71487052 \n",
            "BATCH: 925/1074\n",
            " train loss item: 0.6950424909591675\n",
            " f1:0.26518281\n",
            " avg_train_loss: 0.71486061 \n",
            "BATCH: 926/1074\n",
            " train loss item: 0.6917681694030762\n",
            " f1:0.26505022\n",
            " avg_train_loss: 0.71484906 \n",
            "BATCH: 927/1074\n",
            " train loss item: 0.6891965270042419\n",
            " f1:0.26491776\n",
            " avg_train_loss: 0.71483624 \n",
            "BATCH: 928/1074\n",
            " train loss item: 0.6932635307312012\n",
            " f1:0.26478544\n",
            " avg_train_loss: 0.71482546 \n",
            "BATCH: 929/1074\n",
            " train loss item: 0.6909559965133667\n",
            " f1:0.26465324\n",
            " avg_train_loss: 0.71481355 \n",
            "BATCH: 930/1074\n",
            " train loss item: 0.7022043466567993\n",
            " f1:0.26452118\n",
            " avg_train_loss: 0.71480725 \n",
            "BATCH: 931/1074\n",
            " train loss item: 0.6934084892272949\n",
            " f1:0.26438925\n",
            " avg_train_loss: 0.71479658 \n",
            "BATCH: 932/1074\n",
            " train loss item: 0.6906281113624573\n",
            " f1:0.26425745\n",
            " avg_train_loss: 0.71478453 \n",
            "BATCH: 933/1074\n",
            " train loss item: 0.6877585649490356\n",
            " f1:0.26412578\n",
            " avg_train_loss: 0.71477107 \n",
            "BATCH: 934/1074\n",
            " train loss item: 0.696643590927124\n",
            " f1:0.26399425\n",
            " avg_train_loss: 0.71476204 \n",
            "BATCH: 935/1074\n",
            " train loss item: 0.6934942007064819\n",
            " f1:0.26386284\n",
            " avg_train_loss: 0.71475145 \n",
            "BATCH: 936/1074\n",
            " train loss item: 0.6867770552635193\n",
            " f1:0.26373156\n",
            " avg_train_loss: 0.71473754 \n",
            "BATCH: 937/1074\n",
            " train loss item: 0.6935823559761047\n",
            " f1:0.26360042\n",
            " avg_train_loss: 0.71472702 \n",
            "BATCH: 938/1074\n",
            " train loss item: 0.6936424374580383\n",
            " f1:0.26346941\n",
            " avg_train_loss: 0.71471654 \n",
            "BATCH: 939/1074\n",
            " train loss item: 0.7019213438034058\n",
            " f1:0.26333852\n",
            " avg_train_loss: 0.71471018 \n",
            "BATCH: 940/1074\n",
            " train loss item: 0.6896664500236511\n",
            " f1:0.26320777\n",
            " avg_train_loss: 0.71469775 \n",
            "BATCH: 941/1074\n",
            " train loss item: 0.6896804571151733\n",
            " f1:0.26307714\n",
            " avg_train_loss: 0.71468533 \n",
            "BATCH: 942/1074\n",
            " train loss item: 0.6774923801422119\n",
            " f1:0.26294665\n",
            " avg_train_loss: 0.71466688 \n",
            "BATCH: 943/1074\n",
            " train loss item: 0.6938073635101318\n",
            " f1:0.26281628\n",
            " avg_train_loss: 0.71465654 \n",
            "BATCH: 944/1074\n",
            " train loss item: 0.6889865398406982\n",
            " f1:0.26268605\n",
            " avg_train_loss: 0.71464382 \n",
            "BATCH: 945/1074\n",
            " train loss item: 0.6887022256851196\n",
            " f1:0.26255594\n",
            " avg_train_loss: 0.71463097 \n",
            "BATCH: 946/1074\n",
            " train loss item: 0.6884130835533142\n",
            " f1:0.26242596\n",
            " avg_train_loss: 0.71461799 \n",
            "BATCH: 947/1074\n",
            " train loss item: 0.6944157481193542\n",
            " f1:0.26229611\n",
            " avg_train_loss: 0.71460800 \n",
            "BATCH: 948/1074\n",
            " train loss item: 0.7079209089279175\n",
            " f1:0.26216639\n",
            " avg_train_loss: 0.71460469 \n",
            "BATCH: 949/1074\n",
            " train loss item: 0.6879168152809143\n",
            " f1:0.26203680\n",
            " avg_train_loss: 0.71459150 \n",
            "BATCH: 950/1074\n",
            " train loss item: 0.6878915429115295\n",
            " f1:0.26190733\n",
            " avg_train_loss: 0.71457830 \n",
            "BATCH: 951/1074\n",
            " train loss item: 0.6606337428092957\n",
            " f1:0.26177800\n",
            " avg_train_loss: 0.71455167 \n",
            "BATCH: 952/1074\n",
            " train loss item: 0.702619194984436\n",
            " f1:0.26164879\n",
            " avg_train_loss: 0.71454578 \n",
            "BATCH: 953/1074\n",
            " train loss item: 0.6871331930160522\n",
            " f1:0.26151971\n",
            " avg_train_loss: 0.71453225 \n",
            "BATCH: 954/1074\n",
            " train loss item: 0.6781359910964966\n",
            " f1:0.26139075\n",
            " avg_train_loss: 0.71451431 \n",
            "BATCH: 955/1074\n",
            " train loss item: 0.6670501232147217\n",
            " f1:0.26126192\n",
            " avg_train_loss: 0.71449091 \n",
            "BATCH: 956/1074\n",
            " train loss item: 0.6743254661560059\n",
            " f1:0.26113322\n",
            " avg_train_loss: 0.71447113 \n",
            "BATCH: 957/1074\n",
            " train loss item: 0.657257080078125\n",
            " f1:0.26100465\n",
            " avg_train_loss: 0.71444296 \n",
            "BATCH: 958/1074\n",
            " train loss item: 0.7036792039871216\n",
            " f1:0.26087620\n",
            " avg_train_loss: 0.71443766 \n",
            "BATCH: 959/1074\n",
            " train loss item: 0.7290350794792175\n",
            " f1:0.26074788\n",
            " avg_train_loss: 0.71444484 \n",
            "BATCH: 960/1074\n",
            " train loss item: 0.7301276922225952\n",
            " f1:0.26061969\n",
            " avg_train_loss: 0.71445255 \n",
            "BATCH: 961/1074\n",
            " train loss item: 0.7644779682159424\n",
            " f1:0.26049162\n",
            " avg_train_loss: 0.71447713 \n",
            "BATCH: 962/1074\n",
            " train loss item: 0.6566721200942993\n",
            " f1:0.26036368\n",
            " avg_train_loss: 0.71444874 \n",
            "BATCH: 963/1074\n",
            " train loss item: 0.6742149591445923\n",
            " f1:0.26023586\n",
            " avg_train_loss: 0.71442899 \n",
            "BATCH: 964/1074\n",
            " train loss item: 0.7064276933670044\n",
            " f1:0.26010817\n",
            " avg_train_loss: 0.71442506 \n",
            "BATCH: 965/1074\n",
            " train loss item: 0.7121376991271973\n",
            " f1:0.25998060\n",
            " avg_train_loss: 0.71442394 \n",
            "BATCH: 966/1074\n",
            " train loss item: 0.6677110195159912\n",
            " f1:0.25985316\n",
            " avg_train_loss: 0.71440104 \n",
            "BATCH: 967/1074\n",
            " train loss item: 0.7063975930213928\n",
            " f1:0.25972584\n",
            " avg_train_loss: 0.71439712 \n",
            "BATCH: 968/1074\n",
            " train loss item: 0.6940977573394775\n",
            " f1:0.25959865\n",
            " avg_train_loss: 0.71438718 \n",
            "BATCH: 969/1074\n",
            " train loss item: 0.6888630390167236\n",
            " f1:0.25947158\n",
            " avg_train_loss: 0.71437469 \n",
            "BATCH: 970/1074\n",
            " train loss item: 0.6838755011558533\n",
            " f1:0.25934464\n",
            " avg_train_loss: 0.71435977 \n",
            "BATCH: 971/1074\n",
            " train loss item: 0.6836126446723938\n",
            " f1:0.25921782\n",
            " avg_train_loss: 0.71434473 \n",
            "BATCH: 972/1074\n",
            " train loss item: 0.6829953193664551\n",
            " f1:0.25909113\n",
            " avg_train_loss: 0.71432941 \n",
            "BATCH: 973/1074\n",
            " train loss item: 0.6820315718650818\n",
            " f1:0.25896456\n",
            " avg_train_loss: 0.71431363 \n",
            "BATCH: 974/1074\n",
            " train loss item: 0.6947193145751953\n",
            " f1:0.25883811\n",
            " avg_train_loss: 0.71430406 \n",
            "BATCH: 975/1074\n",
            " train loss item: 0.702958881855011\n",
            " f1:0.25871178\n",
            " avg_train_loss: 0.71429853 \n",
            "BATCH: 976/1074\n",
            " train loss item: 0.7121791243553162\n",
            " f1:0.25858558\n",
            " avg_train_loss: 0.71429749 \n",
            "BATCH: 977/1074\n",
            " train loss item: 0.6703097224235535\n",
            " f1:0.25845950\n",
            " avg_train_loss: 0.71427604 \n",
            "BATCH: 978/1074\n",
            " train loss item: 0.6690640449523926\n",
            " f1:0.25833355\n",
            " avg_train_loss: 0.71425401 \n",
            "BATCH: 979/1074\n",
            " train loss item: 0.6863476037979126\n",
            " f1:0.25820772\n",
            " avg_train_loss: 0.71424042 \n",
            "BATCH: 980/1074\n",
            " train loss item: 0.7309046387672424\n",
            " f1:0.25808201\n",
            " avg_train_loss: 0.71424853 \n",
            "BATCH: 981/1074\n",
            " train loss item: 0.6860135793685913\n",
            " f1:0.25795642\n",
            " avg_train_loss: 0.71423479 \n",
            "BATCH: 982/1074\n",
            " train loss item: 0.6424471139907837\n",
            " f1:0.25783096\n",
            " avg_train_loss: 0.71419988 \n",
            "BATCH: 983/1074\n",
            " train loss item: 0.7353695034980774\n",
            " f1:0.25770561\n",
            " avg_train_loss: 0.71421017 \n",
            "BATCH: 984/1074\n",
            " train loss item: 0.6857080459594727\n",
            " f1:0.25758039\n",
            " avg_train_loss: 0.71419632 \n",
            "BATCH: 985/1074\n",
            " train loss item: 0.6977831125259399\n",
            " f1:0.25745529\n",
            " avg_train_loss: 0.71418835 \n",
            "BATCH: 986/1074\n",
            " train loss item: 0.6858439445495605\n",
            " f1:0.25733031\n",
            " avg_train_loss: 0.71417459 \n",
            "BATCH: 987/1074\n",
            " train loss item: 0.685918390750885\n",
            " f1:0.25720546\n",
            " avg_train_loss: 0.71416088 \n",
            "BATCH: 988/1074\n",
            " train loss item: 0.6971491575241089\n",
            " f1:0.25708072\n",
            " avg_train_loss: 0.71415263 \n",
            "BATCH: 989/1074\n",
            " train loss item: 0.707687497138977\n",
            " f1:0.25695610\n",
            " avg_train_loss: 0.71414949 \n",
            "BATCH: 990/1074\n",
            " train loss item: 0.7260908484458923\n",
            " f1:0.25683161\n",
            " avg_train_loss: 0.71415528 \n",
            "BATCH: 991/1074\n",
            " train loss item: 0.7119881510734558\n",
            " f1:0.25670724\n",
            " avg_train_loss: 0.71415423 \n",
            "BATCH: 992/1074\n",
            " train loss item: 0.6741998195648193\n",
            " f1:0.25658298\n",
            " avg_train_loss: 0.71413489 \n",
            "BATCH: 993/1074\n",
            " train loss item: 0.6761177778244019\n",
            " f1:0.25645885\n",
            " avg_train_loss: 0.71411650 \n",
            "BATCH: 994/1074\n",
            " train loss item: 0.6999268531799316\n",
            " f1:0.25633484\n",
            " avg_train_loss: 0.71410964 \n",
            "BATCH: 995/1074\n",
            " train loss item: 0.6996342539787292\n",
            " f1:0.25621094\n",
            " avg_train_loss: 0.71410264 \n",
            "BATCH: 996/1074\n",
            " train loss item: 0.6995002031326294\n",
            " f1:0.25608717\n",
            " avg_train_loss: 0.71409559 \n",
            "BATCH: 997/1074\n",
            " train loss item: 0.7210969924926758\n",
            " f1:0.25596352\n",
            " avg_train_loss: 0.71409897 \n",
            "BATCH: 998/1074\n",
            " train loss item: 0.6940603256225586\n",
            " f1:0.25583998\n",
            " avg_train_loss: 0.71408929 \n",
            "BATCH: 999/1074\n",
            " train loss item: 0.6940510272979736\n",
            " f1:0.25571657\n",
            " avg_train_loss: 0.71407963 \n",
            "BATCH: 1000/1074\n",
            " train loss item: 0.6887421607971191\n",
            " f1:0.25559327\n",
            " avg_train_loss: 0.71406741 \n",
            "BATCH: 1001/1074\n",
            " train loss item: 0.6833611130714417\n",
            " f1:0.25547009\n",
            " avg_train_loss: 0.71405261 \n",
            "BATCH: 1002/1074\n",
            " train loss item: 0.6886011362075806\n",
            " f1:0.25534703\n",
            " avg_train_loss: 0.71404035 \n",
            "BATCH: 1003/1074\n",
            " train loss item: 0.688446044921875\n",
            " f1:0.25522409\n",
            " avg_train_loss: 0.71402803 \n",
            "BATCH: 1004/1074\n",
            " train loss item: 0.6943320035934448\n",
            " f1:0.25510127\n",
            " avg_train_loss: 0.71401855 \n",
            "BATCH: 1005/1074\n",
            " train loss item: 0.6880757808685303\n",
            " f1:0.25497857\n",
            " avg_train_loss: 0.71400607 \n",
            "BATCH: 1006/1074\n",
            " train loss item: 0.6743844151496887\n",
            " f1:0.25485598\n",
            " avg_train_loss: 0.71398703 \n",
            "BATCH: 1007/1074\n",
            " train loss item: 0.702544093132019\n",
            " f1:0.25473351\n",
            " avg_train_loss: 0.71398153 \n",
            "BATCH: 1008/1074\n",
            " train loss item: 0.7194374799728394\n",
            " f1:0.25461116\n",
            " avg_train_loss: 0.71398415 \n",
            "BATCH: 1009/1074\n",
            " train loss item: 0.6873518228530884\n",
            " f1:0.25448893\n",
            " avg_train_loss: 0.71397136 \n",
            "BATCH: 1010/1074\n",
            " train loss item: 0.6800074577331543\n",
            " f1:0.25436682\n",
            " avg_train_loss: 0.71395506 \n",
            "BATCH: 1011/1074\n",
            " train loss item: 0.694988489151001\n",
            " f1:0.25424482\n",
            " avg_train_loss: 0.71394597 \n",
            "BATCH: 1012/1074\n",
            " train loss item: 0.6722012758255005\n",
            " f1:0.25412294\n",
            " avg_train_loss: 0.71392596 \n",
            "BATCH: 1013/1074\n",
            " train loss item: 0.7280715703964233\n",
            " f1:0.25400117\n",
            " avg_train_loss: 0.71393273 \n",
            "BATCH: 1014/1074\n",
            " train loss item: 0.6949906349182129\n",
            " f1:0.25387952\n",
            " avg_train_loss: 0.71392366 \n",
            "BATCH: 1015/1074\n",
            " train loss item: 0.7016937732696533\n",
            " f1:0.25375799\n",
            " avg_train_loss: 0.71391781 \n",
            "BATCH: 1016/1074\n",
            " train loss item: 0.7006247043609619\n",
            " f1:0.25363658\n",
            " avg_train_loss: 0.71391145 \n",
            "BATCH: 1017/1074\n",
            " train loss item: 0.7104817628860474\n",
            " f1:0.25351528\n",
            " avg_train_loss: 0.71390981 \n",
            "BATCH: 1018/1074\n",
            " train loss item: 0.6892322897911072\n",
            " f1:0.25339409\n",
            " avg_train_loss: 0.71389801 \n",
            "BATCH: 1019/1074\n",
            " train loss item: 0.7017749547958374\n",
            " f1:0.25327303\n",
            " avg_train_loss: 0.71389222 \n",
            "BATCH: 1020/1074\n",
            " train loss item: 0.6899368762969971\n",
            " f1:0.25315207\n",
            " avg_train_loss: 0.71388078 \n",
            "BATCH: 1021/1074\n",
            " train loss item: 0.6969596147537231\n",
            " f1:0.25303124\n",
            " avg_train_loss: 0.71387270 \n",
            "BATCH: 1022/1074\n",
            " train loss item: 0.6934992671012878\n",
            " f1:0.25291052\n",
            " avg_train_loss: 0.71386298 \n",
            "BATCH: 1023/1074\n",
            " train loss item: 0.6902080178260803\n",
            " f1:0.25278991\n",
            " avg_train_loss: 0.71385170 \n",
            "BATCH: 1024/1074\n",
            " train loss item: 0.6969102621078491\n",
            " f1:0.25266942\n",
            " avg_train_loss: 0.71384363 \n",
            "BATCH: 1025/1074\n",
            " train loss item: 0.6832634210586548\n",
            " f1:0.25254904\n",
            " avg_train_loss: 0.71382906 \n",
            "BATCH: 1026/1074\n",
            " train loss item: 0.6936283111572266\n",
            " f1:0.25242878\n",
            " avg_train_loss: 0.71381944 \n",
            "BATCH: 1027/1074\n",
            " train loss item: 0.6980817914009094\n",
            " f1:0.25230864\n",
            " avg_train_loss: 0.71381195 \n",
            "BATCH: 1028/1074\n",
            " train loss item: 0.6847219467163086\n",
            " f1:0.25218860\n",
            " avg_train_loss: 0.71379811 \n",
            "BATCH: 1029/1074\n",
            " train loss item: 0.6888431310653687\n",
            " f1:0.25206868\n",
            " avg_train_loss: 0.71378624 \n",
            "BATCH: 1030/1074\n",
            " train loss item: 0.7061744928359985\n",
            " f1:0.25194888\n",
            " avg_train_loss: 0.71378262 \n",
            "BATCH: 1031/1074\n",
            " train loss item: 0.7002823352813721\n",
            " f1:0.25182919\n",
            " avg_train_loss: 0.71377621 \n",
            "BATCH: 1032/1074\n",
            " train loss item: 0.6773792505264282\n",
            " f1:0.25170961\n",
            " avg_train_loss: 0.71375893 \n",
            "BATCH: 1033/1074\n",
            " train loss item: 0.6882622241973877\n",
            " f1:0.25159015\n",
            " avg_train_loss: 0.71374683 \n",
            "BATCH: 1034/1074\n",
            " train loss item: 0.6945910453796387\n",
            " f1:0.25147080\n",
            " avg_train_loss: 0.71373774 \n",
            "BATCH: 1035/1074\n",
            " train loss item: 0.6947961449623108\n",
            " f1:0.25135156\n",
            " avg_train_loss: 0.71372876 \n",
            "BATCH: 1036/1074\n",
            " train loss item: 0.6800586581230164\n",
            " f1:0.25123244\n",
            " avg_train_loss: 0.71371280 \n",
            "BATCH: 1037/1074\n",
            " train loss item: 0.6953505277633667\n",
            " f1:0.25111343\n",
            " avg_train_loss: 0.71370410 \n",
            "BATCH: 1038/1074\n",
            " train loss item: 0.6867890357971191\n",
            " f1:0.25099453\n",
            " avg_train_loss: 0.71369136 \n",
            "BATCH: 1039/1074\n",
            " train loss item: 0.696114182472229\n",
            " f1:0.25087574\n",
            " avg_train_loss: 0.71368304 \n",
            "BATCH: 1040/1074\n",
            " train loss item: 0.6563196182250977\n",
            " f1:0.25075707\n",
            " avg_train_loss: 0.71365590 \n",
            "BATCH: 1041/1074\n",
            " train loss item: 0.7225842475891113\n",
            " f1:0.25063851\n",
            " avg_train_loss: 0.71366013 \n",
            "BATCH: 1042/1074\n",
            " train loss item: 0.6982147693634033\n",
            " f1:0.25052006\n",
            " avg_train_loss: 0.71365283 \n",
            "BATCH: 1043/1074\n",
            " train loss item: 0.6979883909225464\n",
            " f1:0.25040172\n",
            " avg_train_loss: 0.71364543 \n",
            "BATCH: 1044/1074\n",
            " train loss item: 0.6627684831619263\n",
            " f1:0.25028350\n",
            " avg_train_loss: 0.71362141 \n",
            "BATCH: 1045/1074\n",
            " train loss item: 0.6611183285713196\n",
            " f1:0.25016538\n",
            " avg_train_loss: 0.71359663 \n",
            "BATCH: 1046/1074\n",
            " train loss item: 0.6708469986915588\n",
            " f1:0.25004738\n",
            " avg_train_loss: 0.71357646 \n",
            "BATCH: 1047/1074\n",
            " train loss item: 0.7030465602874756\n",
            " f1:0.24992949\n",
            " avg_train_loss: 0.71357150 \n",
            "BATCH: 1048/1074\n",
            " train loss item: 0.6857680082321167\n",
            " f1:0.24981171\n",
            " avg_train_loss: 0.71355840 \n",
            "BATCH: 1049/1074\n",
            " train loss item: 0.7282372713088989\n",
            " f1:0.24969404\n",
            " avg_train_loss: 0.71356531 \n",
            "BATCH: 1050/1074\n",
            " train loss item: 0.744778573513031\n",
            " f1:0.24957648\n",
            " avg_train_loss: 0.71358001 \n",
            "BATCH: 1051/1074\n",
            " train loss item: 0.7005816102027893\n",
            " f1:0.24945903\n",
            " avg_train_loss: 0.71357389 \n",
            "BATCH: 1052/1074\n",
            " train loss item: 0.6971767544746399\n",
            " f1:0.24934170\n",
            " avg_train_loss: 0.71356618 \n",
            "BATCH: 1053/1074\n",
            " train loss item: 0.6792349815368652\n",
            " f1:0.24922447\n",
            " avg_train_loss: 0.71355004 \n",
            "BATCH: 1054/1074\n",
            " train loss item: 0.6821062564849854\n",
            " f1:0.24910735\n",
            " avg_train_loss: 0.71353526 \n",
            "BATCH: 1055/1074\n",
            " train loss item: 0.7092908620834351\n",
            " f1:0.24899035\n",
            " avg_train_loss: 0.71353327 \n",
            "BATCH: 1056/1074\n",
            " train loss item: 0.6896783113479614\n",
            " f1:0.24887345\n",
            " avg_train_loss: 0.71352207 \n",
            "BATCH: 1057/1074\n",
            " train loss item: 0.6968432068824768\n",
            " f1:0.24875666\n",
            " avg_train_loss: 0.71351424 \n",
            "BATCH: 1058/1074\n",
            " train loss item: 0.6904646754264832\n",
            " f1:0.24863998\n",
            " avg_train_loss: 0.71350343 \n",
            "BATCH: 1059/1074\n",
            " train loss item: 0.6962792873382568\n",
            " f1:0.24852342\n",
            " avg_train_loss: 0.71349535 \n",
            "BATCH: 1060/1074\n",
            " train loss item: 0.6934194564819336\n",
            " f1:0.24840696\n",
            " avg_train_loss: 0.71348595 \n",
            "BATCH: 1061/1074\n",
            " train loss item: 0.7027122378349304\n",
            " f1:0.24829061\n",
            " avg_train_loss: 0.71348090 \n",
            "BATCH: 1062/1074\n",
            " train loss item: 0.6840687990188599\n",
            " f1:0.24817437\n",
            " avg_train_loss: 0.71346713 \n",
            "BATCH: 1063/1074\n",
            " train loss item: 0.6935281753540039\n",
            " f1:0.24805823\n",
            " avg_train_loss: 0.71345780 \n",
            "BATCH: 1064/1074\n",
            " train loss item: 0.6936074495315552\n",
            " f1:0.24794221\n",
            " avg_train_loss: 0.71344852 \n",
            "BATCH: 1065/1074\n",
            " train loss item: 0.7060667872428894\n",
            " f1:0.24782629\n",
            " avg_train_loss: 0.71344506 \n",
            "BATCH: 1066/1074\n",
            " train loss item: 0.6857854723930359\n",
            " f1:0.24771049\n",
            " avg_train_loss: 0.71343214 \n",
            "BATCH: 1067/1074\n",
            " train loss item: 0.6977028846740723\n",
            " f1:0.24759479\n",
            " avg_train_loss: 0.71342479 \n",
            "BATCH: 1068/1074\n",
            " train loss item: 0.6897271275520325\n",
            " f1:0.24747920\n",
            " avg_train_loss: 0.71341373 \n",
            "BATCH: 1069/1074\n",
            " train loss item: 0.689708948135376\n",
            " f1:0.24736372\n",
            " avg_train_loss: 0.71340267 \n",
            "BATCH: 1070/1074\n",
            " train loss item: 0.6895784735679626\n",
            " f1:0.24724834\n",
            " avg_train_loss: 0.71339156 \n",
            "BATCH: 1071/1074\n",
            " train loss item: 0.684898316860199\n",
            " f1:0.24713307\n",
            " avg_train_loss: 0.71337827 \n",
            "BATCH: 1072/1074\n",
            " train loss item: 0.6940056681632996\n",
            " f1:0.24701791\n",
            " avg_train_loss: 0.71336925 \n",
            "BATCH: 1073/1074\n",
            " train loss item: 0.6942259073257446\n",
            " f1:0.24690286\n",
            " avg_train_loss: 0.71336033 \n",
            "BATCH: 1074/1074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " avg_train_loss: 0.70625866 \n",
            "BATCH: 185/1074\n",
            " train loss item: 0.6890916228294373\n",
            " f1:0.18292363\n",
            " avg_train_loss: 0.70625362 \n",
            "BATCH: 186/1074\n",
            " train loss item: 0.7211874723434448\n",
            " f1:0.18286996\n",
            " avg_train_loss: 0.70625800 \n",
            "BATCH: 187/1074\n",
            " train loss item: 0.7185541391372681\n",
            " f1:0.18281632\n",
            " avg_train_loss: 0.70626161 \n",
            "BATCH: 188/1074\n",
            " train loss item: 0.6871563792228699\n",
            " f1:0.18276270\n",
            " avg_train_loss: 0.70625600 \n",
            "BATCH: 189/1074\n",
            " train loss item: 0.741217851638794\n",
            " f1:0.18270912\n",
            " avg_train_loss: 0.70626625 \n",
            "BATCH: 190/1074\n",
            " train loss item: 0.6753817200660706\n",
            " f1:0.18265557\n",
            " avg_train_loss: 0.70625720 \n",
            "BATCH: 191/1074\n",
            " train loss item: 0.7131353616714478\n",
            " f1:0.18260206\n",
            " avg_train_loss: 0.70625922 \n",
            "BATCH: 192/1074\n",
            " train loss item: 0.6931794881820679\n",
            " f1:0.18254857\n",
            " avg_train_loss: 0.70625539 \n",
            "BATCH: 193/1074\n",
            " train loss item: 0.6983146071434021\n",
            " f1:0.18265484\n",
            " avg_train_loss: 0.70625306 \n",
            "BATCH: 194/1074\n",
            " train loss item: 0.7031118869781494\n",
            " f1:0.18274077\n",
            " avg_train_loss: 0.70625214 \n",
            "BATCH: 195/1074\n",
            " train loss item: 0.6931706070899963\n",
            " f1:0.18288239\n",
            " avg_train_loss: 0.70624831 \n",
            "BATCH: 196/1074\n",
            " train loss item: 0.695285439491272\n",
            " f1:0.18282889\n",
            " avg_train_loss: 0.70624511 \n",
            "BATCH: 197/1074\n",
            " train loss item: 0.67626953125\n",
            " f1:0.18277541\n",
            " avg_train_loss: 0.70623634 \n",
            "BATCH: 198/1074\n",
            " train loss item: 0.7062056064605713\n",
            " f1:0.18272197\n",
            " avg_train_loss: 0.70623633 \n",
            "BATCH: 199/1074\n",
            " train loss item: 0.6715357303619385\n",
            " f1:0.18266856\n",
            " avg_train_loss: 0.70622619 \n",
            "BATCH: 200/1074\n",
            " train loss item: 0.8004364967346191\n",
            " f1:0.18261518\n",
            " avg_train_loss: 0.70625372 \n",
            "BATCH: 201/1074\n",
            " train loss item: 0.7471057176589966\n",
            " f1:0.18256183\n",
            " avg_train_loss: 0.70626565 \n",
            "BATCH: 202/1074\n",
            " train loss item: 0.7225231528282166\n",
            " f1:0.18250851\n",
            " avg_train_loss: 0.70627040 \n",
            "BATCH: 203/1074\n",
            " train loss item: 0.6776317358016968\n",
            " f1:0.18245522\n",
            " avg_train_loss: 0.70626204 \n",
            "BATCH: 204/1074\n",
            " train loss item: 0.6898307800292969\n",
            " f1:0.18240196\n",
            " avg_train_loss: 0.70625724 \n",
            "BATCH: 205/1074\n",
            " train loss item: 0.7017116546630859\n",
            " f1:0.18234874\n",
            " avg_train_loss: 0.70625591 \n",
            "BATCH: 206/1074\n",
            " train loss item: 0.6933481693267822\n",
            " f1:0.18229555\n",
            " avg_train_loss: 0.70625215 \n",
            "BATCH: 207/1074\n",
            " train loss item: 0.6969606876373291\n",
            " f1:0.18224238\n",
            " avg_train_loss: 0.70624944 \n",
            "BATCH: 208/1074\n",
            " train loss item: 0.7019433975219727\n",
            " f1:0.18218925\n",
            " avg_train_loss: 0.70624818 \n",
            "BATCH: 209/1074\n",
            " train loss item: 0.6900831460952759\n",
            " f1:0.18213615\n",
            " avg_train_loss: 0.70624347 \n",
            "BATCH: 210/1074\n",
            " train loss item: 0.6867485642433167\n",
            " f1:0.18208308\n",
            " avg_train_loss: 0.70623779 \n",
            "BATCH: 211/1074\n",
            " train loss item: 0.6735175251960754\n",
            " f1:0.18203004\n",
            " avg_train_loss: 0.70622826 \n",
            "BATCH: 212/1074\n",
            " train loss item: 0.7196956872940063\n",
            " f1:0.18197703\n",
            " avg_train_loss: 0.70623218 \n",
            "BATCH: 213/1074\n",
            " train loss item: 0.6990318894386292\n",
            " f1:0.18192406\n",
            " avg_train_loss: 0.70623009 \n",
            "BATCH: 214/1074\n",
            " train loss item: 0.6709179878234863\n",
            " f1:0.18187111\n",
            " avg_train_loss: 0.70621981 \n",
            "BATCH: 215/1074\n",
            " train loss item: 0.7194634675979614\n",
            " f1:0.18181819\n",
            " avg_train_loss: 0.70622366 \n",
            "BATCH: 216/1074\n",
            " train loss item: 0.73268723487854\n",
            " f1:0.18176531\n",
            " avg_train_loss: 0.70623136 \n",
            "BATCH: 217/1074\n",
            " train loss item: 0.7255595326423645\n",
            " f1:0.18171245\n",
            " avg_train_loss: 0.70623698 \n",
            "BATCH: 218/1074\n",
            " train loss item: 0.6935421228408813\n",
            " f1:0.18165963\n",
            " avg_train_loss: 0.70623329 \n",
            "BATCH: 219/1074\n",
            " train loss item: 0.6861104965209961\n",
            " f1:0.18181608\n",
            " avg_train_loss: 0.70622744 \n",
            "BATCH: 220/1074\n",
            " train loss item: 0.7250999808311462\n",
            " f1:0.18194010\n",
            " avg_train_loss: 0.70623293 \n",
            "BATCH: 221/1074\n",
            " train loss item: 0.7092264890670776\n",
            " f1:0.18208089\n",
            " avg_train_loss: 0.70623380 \n",
            "BATCH: 222/1074\n",
            " train loss item: 0.747477650642395\n",
            " f1:0.18218640\n",
            " avg_train_loss: 0.70624577 \n",
            "BATCH: 223/1074\n",
            " train loss item: 0.6962720155715942\n",
            " f1:0.18232703\n",
            " avg_train_loss: 0.70624288 \n",
            "BATCH: 224/1074\n",
            " train loss item: 0.6933273673057556\n",
            " f1:0.18227412\n",
            " avg_train_loss: 0.70623913 \n",
            "BATCH: 225/1074\n",
            " train loss item: 0.6570762991905212\n",
            " f1:0.18222124\n",
            " avg_train_loss: 0.70622487 \n",
            "BATCH: 226/1074\n",
            " train loss item: 0.6297590136528015\n",
            " f1:0.18216839\n",
            " avg_train_loss: 0.70620269 \n",
            "BATCH: 227/1074\n",
            " train loss item: 0.6215357780456543\n",
            " f1:0.18211557\n",
            " avg_train_loss: 0.70617814 \n",
            "BATCH: 228/1074\n",
            " train loss item: 1.005293369293213\n",
            " f1:0.18206279\n",
            " avg_train_loss: 0.70626484 \n",
            "BATCH: 229/1074\n",
            " train loss item: 0.7913500666618347\n",
            " f1:0.18201003\n",
            " avg_train_loss: 0.70628950 \n",
            "BATCH: 230/1074\n",
            " train loss item: 0.7232911586761475\n",
            " f1:0.18195730\n",
            " avg_train_loss: 0.70629442 \n",
            "BATCH: 231/1074\n",
            " train loss item: 0.7087591290473938\n",
            " f1:0.18190461\n",
            " avg_train_loss: 0.70629513 \n",
            "BATCH: 232/1074\n",
            " train loss item: 0.694918692111969\n",
            " f1:0.18185194\n",
            " avg_train_loss: 0.70629184 \n",
            "BATCH: 233/1074\n",
            " train loss item: 0.7011193037033081\n",
            " f1:0.18189071\n",
            " avg_train_loss: 0.70629034 \n",
            "BATCH: 234/1074\n",
            " train loss item: 0.6885781288146973\n",
            " f1:0.18183808\n",
            " avg_train_loss: 0.70628522 \n",
            "BATCH: 235/1074\n",
            " train loss item: 0.8011547327041626\n",
            " f1:0.18178548\n",
            " avg_train_loss: 0.70631266 \n",
            "BATCH: 236/1074\n",
            " train loss item: 0.730341911315918\n",
            " f1:0.18173291\n",
            " avg_train_loss: 0.70631961 \n",
            "BATCH: 237/1074\n",
            " train loss item: 0.7223684787750244\n",
            " f1:0.18168037\n",
            " avg_train_loss: 0.70632425 \n",
            "BATCH: 238/1074\n",
            " train loss item: 0.6866106390953064\n",
            " f1:0.18162786\n",
            " avg_train_loss: 0.70631855 \n",
            "BATCH: 239/1074\n",
            " train loss item: 0.6950566172599792\n",
            " f1:0.18157538\n",
            " avg_train_loss: 0.70631530 \n",
            "BATCH: 240/1074\n",
            " train loss item: 0.703636109828949\n",
            " f1:0.18169876\n",
            " avg_train_loss: 0.70631452 \n",
            "BATCH: 241/1074\n",
            " train loss item: 0.7117815017700195\n",
            " f1:0.18182206\n",
            " avg_train_loss: 0.70631610 \n",
            "BATCH: 242/1074\n",
            " train loss item: 0.6858047246932983\n",
            " f1:0.18197742\n",
            " avg_train_loss: 0.70631018 \n",
            "BATCH: 243/1074\n",
            " train loss item: 0.7083313465118408\n",
            " f1:0.18210057\n",
            " avg_train_loss: 0.70631076 \n",
            "BATCH: 244/1074\n",
            " train loss item: 0.7045047283172607\n",
            " f1:0.18220541\n",
            " avg_train_loss: 0.70631024 \n",
            "BATCH: 245/1074\n",
            " train loss item: 0.7018166780471802\n",
            " f1:0.18215285\n",
            " avg_train_loss: 0.70630895 \n",
            "BATCH: 246/1074\n",
            " train loss item: 0.6717150807380676\n",
            " f1:0.18210033\n",
            " avg_train_loss: 0.70629897 \n",
            "BATCH: 247/1074\n",
            " train loss item: 0.732682466506958\n",
            " f1:0.18204784\n",
            " avg_train_loss: 0.70630658 \n",
            "BATCH: 248/1074\n",
            " train loss item: 0.7347937822341919\n",
            " f1:0.18199537\n",
            " avg_train_loss: 0.70631479 \n",
            "BATCH: 249/1074\n",
            " train loss item: 0.6854224801063538\n",
            " f1:0.18194294\n",
            " avg_train_loss: 0.70630877 \n",
            "BATCH: 250/1074\n",
            " train loss item: 0.7099902629852295\n",
            " f1:0.18189054\n",
            " avg_train_loss: 0.70630983 \n",
            "BATCH: 251/1074\n",
            " train loss item: 0.6973817348480225\n",
            " f1:0.18183816\n",
            " avg_train_loss: 0.70630726 \n",
            "BATCH: 252/1074\n",
            " train loss item: 0.6941149234771729\n",
            " f1:0.18197772\n",
            " avg_train_loss: 0.70630375 \n",
            "BATCH: 253/1074\n",
            " train loss item: 0.6856416463851929\n",
            " f1:0.18213255\n",
            " avg_train_loss: 0.70629780 \n",
            "BATCH: 254/1074\n",
            " train loss item: 0.7228037118911743\n",
            " f1:0.18225527\n",
            " avg_train_loss: 0.70630255 \n",
            "BATCH: 255/1074\n",
            " train loss item: 0.7250659465789795\n",
            " f1:0.18237791\n",
            " avg_train_loss: 0.70630795 \n",
            "BATCH: 256/1074\n",
            " train loss item: 0.7019364833831787\n",
            " f1:0.18251716\n",
            " avg_train_loss: 0.70630669 \n",
            "BATCH: 257/1074\n",
            " train loss item: 0.7487574815750122\n",
            " f1:0.18257967\n",
            " avg_train_loss: 0.70631889 \n",
            "BATCH: 258/1074\n",
            " train loss item: 0.6986063718795776\n",
            " f1:0.18270212\n",
            " avg_train_loss: 0.70631668 \n",
            "BATCH: 259/1074\n",
            " train loss item: 0.697849452495575\n",
            " f1:0.18264963\n",
            " avg_train_loss: 0.70631424 \n",
            "BATCH: 260/1074\n",
            " train loss item: 0.6897335052490234\n",
            " f1:0.18259718\n",
            " avg_train_loss: 0.70630948 \n",
            "BATCH: 261/1074\n",
            " train loss item: 0.7102932333946228\n",
            " f1:0.18254475\n",
            " avg_train_loss: 0.70631063 \n",
            "BATCH: 262/1074\n",
            " train loss item: 0.6854999661445618\n",
            " f1:0.18249236\n",
            " avg_train_loss: 0.70630465 \n",
            "BATCH: 263/1074\n",
            " train loss item: 0.6870156526565552\n",
            " f1:0.18243999\n",
            " avg_train_loss: 0.70629912 \n",
            "BATCH: 264/1074\n",
            " train loss item: 0.6869666576385498\n",
            " f1:0.18238766\n",
            " avg_train_loss: 0.70629357 \n",
            "BATCH: 265/1074\n",
            " train loss item: 0.6983902454376221\n",
            " f1:0.18233535\n",
            " avg_train_loss: 0.70629131 \n",
            "BATCH: 266/1074\n",
            " train loss item: 0.6983025074005127\n",
            " f1:0.18228308\n",
            " avg_train_loss: 0.70628902 \n",
            "BATCH: 267/1074\n",
            " train loss item: 0.6934160590171814\n",
            " f1:0.18223083\n",
            " avg_train_loss: 0.70628533 \n",
            "BATCH: 268/1074\n",
            " train loss item: 0.6981130838394165\n",
            " f1:0.18217861\n",
            " avg_train_loss: 0.70628298 \n",
            "BATCH: 269/1074\n",
            " train loss item: 0.7192463278770447\n",
            " f1:0.18226283\n",
            " avg_train_loss: 0.70628670 \n",
            "BATCH: 270/1074\n",
            " train loss item: 0.6969743967056274\n",
            " f1:0.18240155\n",
            " avg_train_loss: 0.70628403 \n",
            "BATCH: 271/1074\n",
            " train loss item: 0.6856557726860046\n",
            " f1:0.18255546\n",
            " avg_train_loss: 0.70627813 \n",
            "BATCH: 272/1074\n",
            " train loss item: 0.7136790752410889\n",
            " f1:0.18267742\n",
            " avg_train_loss: 0.70628024 \n",
            "BATCH: 273/1074\n",
            " train loss item: 0.6979140043258667\n",
            " f1:0.18281590\n",
            " avg_train_loss: 0.70627785 \n",
            "BATCH: 274/1074\n",
            " train loss item: 0.7237005233764648\n",
            " f1:0.18289982\n",
            " avg_train_loss: 0.70628283 \n",
            "BATCH: 275/1074\n",
            " train loss item: 0.6932463645935059\n",
            " f1:0.18303816\n",
            " avg_train_loss: 0.70627911 \n",
            "BATCH: 276/1074\n",
            " train loss item: 0.6978041529655457\n",
            " f1:0.18298583\n",
            " avg_train_loss: 0.70627668 \n",
            "BATCH: 277/1074\n",
            " train loss item: 0.6800317764282227\n",
            " f1:0.18293354\n",
            " avg_train_loss: 0.70626918 \n",
            "BATCH: 278/1074\n",
            " train loss item: 0.6636095643043518\n",
            " f1:0.18288127\n",
            " avg_train_loss: 0.70625699 \n",
            "BATCH: 279/1074\n",
            " train loss item: 0.669330358505249\n",
            " f1:0.18282903\n",
            " avg_train_loss: 0.70624645 \n",
            "BATCH: 280/1074\n",
            " train loss item: 0.6862119436264038\n",
            " f1:0.18277683\n",
            " avg_train_loss: 0.70624073 \n",
            "BATCH: 281/1074\n",
            " train loss item: 0.6880674362182617\n",
            " f1:0.18272465\n",
            " avg_train_loss: 0.70623554 \n",
            "BATCH: 282/1074\n",
            " train loss item: 0.6897422671318054\n",
            " f1:0.18267250\n",
            " avg_train_loss: 0.70623083 \n",
            "BATCH: 283/1074\n",
            " train loss item: 0.746700644493103\n",
            " f1:0.18262038\n",
            " avg_train_loss: 0.70624238 \n",
            "BATCH: 284/1074\n",
            " train loss item: 0.6640952825546265\n",
            " f1:0.18256830\n",
            " avg_train_loss: 0.70623036 \n",
            "BATCH: 285/1074\n",
            " train loss item: 0.6484724879264832\n",
            " f1:0.18251624\n",
            " avg_train_loss: 0.70621389 \n",
            "BATCH: 286/1074\n",
            " train loss item: 0.6855132579803467\n",
            " f1:0.18246421\n",
            " avg_train_loss: 0.70620798 \n",
            "BATCH: 287/1074\n",
            " train loss item: 0.7028905153274536\n",
            " f1:0.18241221\n",
            " avg_train_loss: 0.70620704 \n",
            "BATCH: 288/1074\n",
            " train loss item: 0.6573800444602966\n",
            " f1:0.18236024\n",
            " avg_train_loss: 0.70619313 \n",
            "BATCH: 289/1074\n",
            " train loss item: 0.6853182911872864\n",
            " f1:0.18230830\n",
            " avg_train_loss: 0.70618718 \n",
            "BATCH: 290/1074\n",
            " train loss item: 0.6496092081069946\n",
            " f1:0.18225639\n",
            " avg_train_loss: 0.70617107 \n",
            "BATCH: 291/1074\n",
            " train loss item: 0.8126421570777893\n",
            " f1:0.18220451\n",
            " avg_train_loss: 0.70620138 \n",
            "BATCH: 292/1074\n",
            " train loss item: 0.6853245496749878\n",
            " f1:0.18215266\n",
            " avg_train_loss: 0.70619544 \n",
            "BATCH: 293/1074\n",
            " train loss item: 0.6945081949234009\n",
            " f1:0.18210084\n",
            " avg_train_loss: 0.70619211 \n",
            "BATCH: 294/1074\n",
            " train loss item: 0.6886765956878662\n",
            " f1:0.18226783\n",
            " avg_train_loss: 0.70618713 \n",
            "BATCH: 295/1074\n",
            " train loss item: 0.7166928648948669\n",
            " f1:0.18238907\n",
            " avg_train_loss: 0.70619012 \n",
            "BATCH: 296/1074\n",
            " train loss item: 0.7503476738929749\n",
            " f1:0.18249228\n",
            " avg_train_loss: 0.70620267 \n",
            "BATCH: 297/1074\n",
            " train loss item: 0.7166709899902344\n",
            " f1:0.18261339\n",
            " avg_train_loss: 0.70620565 \n",
            "BATCH: 298/1074\n",
            " train loss item: 0.6760992407798767\n",
            " f1:0.18279299\n",
            " avg_train_loss: 0.70619709 \n",
            "BATCH: 299/1074\n",
            " train loss item: 0.6821717023849487\n",
            " f1:0.18297249\n",
            " avg_train_loss: 0.70619027 \n",
            "BATCH: 300/1074\n",
            " train loss item: 0.6946672201156616\n",
            " f1:0.18310983\n",
            " avg_train_loss: 0.70618700 \n",
            "BATCH: 301/1074\n",
            " train loss item: 0.7042579650878906\n",
            " f1:0.18323063\n",
            " avg_train_loss: 0.70618645 \n",
            "BATCH: 302/1074\n",
            " train loss item: 0.7174885272979736\n",
            " f1:0.18331376\n",
            " avg_train_loss: 0.70618966 \n",
            "BATCH: 303/1074\n",
            " train loss item: 0.6903823018074036\n",
            " f1:0.18347998\n",
            " avg_train_loss: 0.70618517 \n",
            "BATCH: 304/1074\n",
            " train loss item: 0.693186342716217\n",
            " f1:0.18342794\n",
            " avg_train_loss: 0.70618149 \n",
            "BATCH: 305/1074\n",
            " train loss item: 0.68739253282547\n",
            " f1:0.18337594\n",
            " avg_train_loss: 0.70617616 \n",
            "BATCH: 306/1074\n",
            " train loss item: 0.7122942209243774\n",
            " f1:0.18332396\n",
            " avg_train_loss: 0.70617789 \n",
            "BATCH: 307/1074\n",
            " train loss item: 0.6942731738090515\n",
            " f1:0.18327201\n",
            " avg_train_loss: 0.70617452 \n",
            "BATCH: 308/1074\n",
            " train loss item: 0.6829414367675781\n",
            " f1:0.18322009\n",
            " avg_train_loss: 0.70616794 \n",
            "BATCH: 309/1074\n",
            " train loss item: 0.7068314552307129\n",
            " f1:0.18316821\n",
            " avg_train_loss: 0.70616813 \n",
            "BATCH: 310/1074\n",
            " train loss item: 0.6831468343734741\n",
            " f1:0.18311635\n",
            " avg_train_loss: 0.70616161 \n",
            "BATCH: 311/1074\n",
            " train loss item: 0.67174232006073\n",
            " f1:0.18306452\n",
            " avg_train_loss: 0.70615187 \n",
            "BATCH: 312/1074\n",
            " train loss item: 0.6803551912307739\n",
            " f1:0.18301271\n",
            " avg_train_loss: 0.70614457 \n",
            "BATCH: 313/1074\n",
            " train loss item: 0.7051017880439758\n",
            " f1:0.18296094\n",
            " avg_train_loss: 0.70614427 \n",
            "BATCH: 314/1074\n",
            " train loss item: 0.7169470191001892\n",
            " f1:0.18290920\n",
            " avg_train_loss: 0.70614733 \n",
            "BATCH: 315/1074\n",
            " train loss item: 0.7160053253173828\n",
            " f1:0.18285749\n",
            " avg_train_loss: 0.70615011 \n",
            "BATCH: 316/1074\n",
            " train loss item: 0.6869743466377258\n",
            " f1:0.18280580\n",
            " avg_train_loss: 0.70614469 \n",
            "BATCH: 317/1074\n",
            " train loss item: 0.6579065322875977\n",
            " f1:0.18275415\n",
            " avg_train_loss: 0.70613106 \n",
            "BATCH: 318/1074\n",
            " train loss item: 0.6872400045394897\n",
            " f1:0.18270252\n",
            " avg_train_loss: 0.70612573 \n",
            "BATCH: 319/1074\n",
            " train loss item: 0.6784360408782959\n",
            " f1:0.18265093\n",
            " avg_train_loss: 0.70611791 \n",
            "BATCH: 320/1074\n",
            " train loss item: 0.7152538299560547\n",
            " f1:0.18259936\n",
            " avg_train_loss: 0.70612049 \n",
            "BATCH: 321/1074\n",
            " train loss item: 0.6960563659667969\n",
            " f1:0.18254782\n",
            " avg_train_loss: 0.70611765 \n",
            "BATCH: 322/1074\n",
            " train loss item: 0.6773878931999207\n",
            " f1:0.18249631\n",
            " avg_train_loss: 0.70610954 \n",
            "BATCH: 323/1074\n",
            " train loss item: 0.6675094962120056\n",
            " f1:0.18244483\n",
            " avg_train_loss: 0.70609865 \n",
            "BATCH: 324/1074\n",
            " train loss item: 0.6968143582344055\n",
            " f1:0.18239338\n",
            " avg_train_loss: 0.70609603 \n",
            "BATCH: 325/1074\n",
            " train loss item: 0.697396993637085\n",
            " f1:0.18234196\n",
            " avg_train_loss: 0.70609358 \n",
            "BATCH: 326/1074\n",
            " train loss item: 0.6977002620697021\n",
            " f1:0.18229057\n",
            " avg_train_loss: 0.70609122 \n",
            "BATCH: 327/1074\n",
            " train loss item: 0.685764729976654\n",
            " f1:0.18223920\n",
            " avg_train_loss: 0.70608549 \n",
            "BATCH: 328/1074\n",
            " train loss item: 0.6977744102478027\n",
            " f1:0.18218787\n",
            " avg_train_loss: 0.70608315 \n",
            "BATCH: 329/1074\n",
            " train loss item: 0.6388291716575623\n",
            " f1:0.18213656\n",
            " avg_train_loss: 0.70606421 \n",
            "BATCH: 330/1074\n",
            " train loss item: 0.6990652680397034\n",
            " f1:0.18208529\n",
            " avg_train_loss: 0.70606224 \n",
            "BATCH: 331/1074\n",
            " train loss item: 0.7151222229003906\n",
            " f1:0.18203404\n",
            " avg_train_loss: 0.70606479 \n",
            "BATCH: 332/1074\n",
            " train loss item: 0.6704486608505249\n",
            " f1:0.18198282\n",
            " avg_train_loss: 0.70605477 \n",
            "BATCH: 333/1074\n",
            " train loss item: 0.7008362412452698\n",
            " f1:0.18193163\n",
            " avg_train_loss: 0.70605330 \n",
            "BATCH: 334/1074\n",
            " train loss item: 0.7162621021270752\n",
            " f1:0.18188046\n",
            " avg_train_loss: 0.70605617 \n",
            "BATCH: 335/1074\n",
            " train loss item: 0.6711716055870056\n",
            " f1:0.18182933\n",
            " avg_train_loss: 0.70604636 \n",
            "BATCH: 336/1074\n",
            " train loss item: 0.6991729736328125\n",
            " f1:0.18177823\n",
            " avg_train_loss: 0.70604443 \n",
            "BATCH: 337/1074\n",
            " train loss item: 0.7112553119659424\n",
            " f1:0.18172715\n",
            " avg_train_loss: 0.70604589 \n",
            "BATCH: 338/1074\n",
            " train loss item: 0.6748040914535522\n",
            " f1:0.18167610\n",
            " avg_train_loss: 0.70603712 \n",
            "BATCH: 339/1074\n",
            " train loss item: 0.7274653911590576\n",
            " f1:0.18162509\n",
            " avg_train_loss: 0.70604314 \n",
            "BATCH: 340/1074\n",
            " train loss item: 0.7033576369285583\n",
            " f1:0.18157410\n",
            " avg_train_loss: 0.70604238 \n",
            "BATCH: 341/1074\n",
            " train loss item: 0.671021044254303\n",
            " f1:0.18152314\n",
            " avg_train_loss: 0.70603255 \n",
            "BATCH: 342/1074\n",
            " train loss item: 0.6939442157745361\n",
            " f1:0.18147220\n",
            " avg_train_loss: 0.70602916 \n",
            "BATCH: 343/1074\n",
            " train loss item: 0.6937222480773926\n",
            " f1:0.18142130\n",
            " avg_train_loss: 0.70602571 \n",
            "BATCH: 344/1074\n",
            " train loss item: 0.682895302772522\n",
            " f1:0.18137042\n",
            " avg_train_loss: 0.70601922 \n",
            "BATCH: 345/1074\n",
            " train loss item: 0.697185754776001\n",
            " f1:0.18131958\n",
            " avg_train_loss: 0.70601675 \n",
            "BATCH: 346/1074\n",
            " train loss item: 0.7037797570228577\n",
            " f1:0.18126876\n",
            " avg_train_loss: 0.70601612 \n",
            "BATCH: 347/1074\n",
            " train loss item: 0.6855127811431885\n",
            " f1:0.18121797\n",
            " avg_train_loss: 0.70601037 \n",
            "BATCH: 348/1074\n",
            " train loss item: 0.6983189582824707\n",
            " f1:0.18116721\n",
            " avg_train_loss: 0.70600822 \n",
            "BATCH: 349/1074\n",
            " train loss item: 0.6873158812522888\n",
            " f1:0.18111648\n",
            " avg_train_loss: 0.70600298 \n",
            "BATCH: 350/1074\n",
            " train loss item: 0.701596736907959\n",
            " f1:0.18106577\n",
            " avg_train_loss: 0.70600175 \n",
            "BATCH: 351/1074\n",
            " train loss item: 0.691754937171936\n",
            " f1:0.18101509\n",
            " avg_train_loss: 0.70599776 \n",
            "BATCH: 352/1074\n",
            " train loss item: 0.6888678073883057\n",
            " f1:0.18096445\n",
            " avg_train_loss: 0.70599297 \n",
            "BATCH: 353/1074\n",
            " train loss item: 0.6932132244110107\n",
            " f1:0.18091383\n",
            " avg_train_loss: 0.70598940 \n",
            "BATCH: 354/1074\n",
            " train loss item: 0.6966827511787415\n",
            " f1:0.18086324\n",
            " avg_train_loss: 0.70598679 \n",
            "BATCH: 355/1074\n",
            " train loss item: 0.6915655136108398\n",
            " f1:0.18081267\n",
            " avg_train_loss: 0.70598276 \n",
            "BATCH: 356/1074\n",
            " train loss item: 0.694991409778595\n",
            " f1:0.18076214\n",
            " avg_train_loss: 0.70597969 \n",
            "BATCH: 357/1074\n",
            " train loss item: 0.6965892314910889\n",
            " f1:0.18071163\n",
            " avg_train_loss: 0.70597707 \n",
            "BATCH: 358/1074\n",
            " train loss item: 0.6932064890861511\n",
            " f1:0.18066115\n",
            " avg_train_loss: 0.70597350 \n",
            "BATCH: 359/1074\n",
            " train loss item: 0.6899679899215698\n",
            " f1:0.18061070\n",
            " avg_train_loss: 0.70596903 \n",
            "BATCH: 360/1074\n",
            " train loss item: 0.6931921243667603\n",
            " f1:0.18056028\n",
            " avg_train_loss: 0.70596546 \n",
            "BATCH: 361/1074\n",
            " train loss item: 0.6957426071166992\n",
            " f1:0.18050989\n",
            " avg_train_loss: 0.70596261 \n",
            "BATCH: 362/1074\n",
            " train loss item: 0.6920676231384277\n",
            " f1:0.18045952\n",
            " avg_train_loss: 0.70595873 \n",
            "BATCH: 363/1074\n",
            " train loss item: 0.6899197697639465\n",
            " f1:0.18040919\n",
            " avg_train_loss: 0.70595426 \n",
            "BATCH: 364/1074\n",
            " train loss item: 0.6973148584365845\n",
            " f1:0.18035888\n",
            " avg_train_loss: 0.70595185 \n",
            "BATCH: 365/1074\n",
            " train loss item: 0.6945176124572754\n",
            " f1:0.18030860\n",
            " avg_train_loss: 0.70594866 \n",
            "BATCH: 366/1074\n",
            " train loss item: 0.6931906938552856\n",
            " f1:0.18025834\n",
            " avg_train_loss: 0.70594511 \n",
            "BATCH: 367/1074\n",
            " train loss item: 0.6921538710594177\n",
            " f1:0.18020812\n",
            " avg_train_loss: 0.70594126 \n",
            "BATCH: 368/1074\n",
            " train loss item: 0.6921855211257935\n",
            " f1:0.18015792\n",
            " avg_train_loss: 0.70593743 \n",
            "BATCH: 369/1074\n",
            " train loss item: 0.6931825280189514\n",
            " f1:0.18010775\n",
            " avg_train_loss: 0.70593388 \n",
            "BATCH: 370/1074\n",
            " train loss item: 0.6920902729034424\n",
            " f1:0.18005761\n",
            " avg_train_loss: 0.70593003 \n",
            "BATCH: 371/1074\n",
            " train loss item: 0.6968300342559814\n",
            " f1:0.18000750\n",
            " avg_train_loss: 0.70592749 \n",
            "BATCH: 372/1074\n",
            " train loss item: 0.6942508220672607\n",
            " f1:0.17995741\n",
            " avg_train_loss: 0.70592424 \n",
            "BATCH: 373/1074\n",
            " train loss item: 0.6948623657226562\n",
            " f1:0.17990735\n",
            " avg_train_loss: 0.70592117 \n",
            "BATCH: 374/1074\n",
            " train loss item: 0.6931536793708801\n",
            " f1:0.17985732\n",
            " avg_train_loss: 0.70591762 \n",
            "BATCH: 375/1074\n",
            " train loss item: 0.6929997205734253\n",
            " f1:0.17980732\n",
            " avg_train_loss: 0.70591403 \n",
            "BATCH: 376/1074\n",
            " train loss item: 0.6931471824645996\n",
            " f1:0.17994263\n",
            " avg_train_loss: 0.70591048 \n",
            "BATCH: 377/1074\n",
            " train loss item: 0.693011999130249\n",
            " f1:0.18009269\n",
            " avg_train_loss: 0.70590689 \n",
            "BATCH: 378/1074\n",
            " train loss item: 0.69315105676651\n",
            " f1:0.18022785\n",
            " avg_train_loss: 0.70590335 \n",
            "BATCH: 379/1074\n",
            " train loss item: 0.6947767734527588\n",
            " f1:0.18031004\n",
            " avg_train_loss: 0.70590026 \n",
            "BATCH: 380/1074\n",
            " train loss item: 0.6928576231002808\n",
            " f1:0.18045987\n",
            " avg_train_loss: 0.70589664 \n",
            "BATCH: 381/1074\n",
            " train loss item: 0.6939518451690674\n",
            " f1:0.18052080\n",
            " avg_train_loss: 0.70589332 \n",
            "BATCH: 382/1074\n",
            " train loss item: 0.6928353905677795\n",
            " f1:0.18047071\n",
            " avg_train_loss: 0.70588970 \n",
            "BATCH: 383/1074\n",
            " train loss item: 0.691598117351532\n",
            " f1:0.18042065\n",
            " avg_train_loss: 0.70588574 \n",
            "BATCH: 384/1074\n",
            " train loss item: 0.6931955814361572\n",
            " f1:0.18037062\n",
            " avg_train_loss: 0.70588222 \n",
            "BATCH: 385/1074\n",
            " train loss item: 0.6901626586914062\n",
            " f1:0.18032061\n",
            " avg_train_loss: 0.70587786 \n",
            "BATCH: 386/1074\n",
            " train loss item: 0.6950201988220215\n",
            " f1:0.18027064\n",
            " avg_train_loss: 0.70587485 \n",
            "BATCH: 387/1074\n",
            " train loss item: 0.6876086592674255\n",
            " f1:0.18022069\n",
            " avg_train_loss: 0.70586979 \n",
            "BATCH: 388/1074\n",
            " train loss item: 0.6892436146736145\n",
            " f1:0.18017076\n",
            " avg_train_loss: 0.70586518 \n",
            "BATCH: 389/1074\n",
            " train loss item: 0.7020078897476196\n",
            " f1:0.18012087\n",
            " avg_train_loss: 0.70586411 \n",
            "BATCH: 390/1074\n",
            " train loss item: 0.6912024021148682\n",
            " f1:0.18007100\n",
            " avg_train_loss: 0.70586006 \n",
            "BATCH: 391/1074\n",
            " train loss item: 0.6952829360961914\n",
            " f1:0.18002116\n",
            " avg_train_loss: 0.70585713 \n",
            "BATCH: 392/1074\n",
            " train loss item: 0.6859944462776184\n",
            " f1:0.17997135\n",
            " avg_train_loss: 0.70585163 \n",
            "BATCH: 393/1074\n",
            " train loss item: 0.6991991400718689\n",
            " f1:0.17992156\n",
            " avg_train_loss: 0.70584979 \n",
            "BATCH: 394/1074\n",
            " train loss item: 0.6914445161819458\n",
            " f1:0.17987181\n",
            " avg_train_loss: 0.70584581 \n",
            "BATCH: 395/1074\n",
            " train loss item: 0.6915057897567749\n",
            " f1:0.17982208\n",
            " avg_train_loss: 0.70584184 \n",
            "BATCH: 396/1074\n",
            " train loss item: 0.6914680004119873\n",
            " f1:0.17977238\n",
            " avg_train_loss: 0.70583787 \n",
            "BATCH: 397/1074\n",
            " train loss item: 0.6913309097290039\n",
            " f1:0.17972270\n",
            " avg_train_loss: 0.70583386 \n",
            "BATCH: 398/1074\n",
            " train loss item: 0.6954979300498962\n",
            " f1:0.17967305\n",
            " avg_train_loss: 0.70583101 \n",
            "BATCH: 399/1074\n",
            " train loss item: 0.6864686608314514\n",
            " f1:0.17962343\n",
            " avg_train_loss: 0.70582566 \n",
            "BATCH: 400/1074\n",
            " train loss item: 0.6906415224075317\n",
            " f1:0.17957384\n",
            " avg_train_loss: 0.70582147 \n",
            "BATCH: 401/1074\n",
            " train loss item: 0.6934899091720581\n",
            " f1:0.17952428\n",
            " avg_train_loss: 0.70581806 \n",
            "BATCH: 402/1074\n",
            " train loss item: 0.7010206580162048\n",
            " f1:0.17947474\n",
            " avg_train_loss: 0.70581674 \n",
            "BATCH: 403/1074\n",
            " train loss item: 0.6974263787269592\n",
            " f1:0.17942523\n",
            " avg_train_loss: 0.70581442 \n",
            "BATCH: 404/1074\n",
            " train loss item: 0.686114490032196\n",
            " f1:0.17937575\n",
            " avg_train_loss: 0.70580899 \n",
            "BATCH: 405/1074\n",
            " train loss item: 0.6975265145301819\n",
            " f1:0.17932629\n",
            " avg_train_loss: 0.70580671 \n",
            "BATCH: 406/1074\n",
            " train loss item: 0.6936280727386475\n",
            " f1:0.17927686\n",
            " avg_train_loss: 0.70580335 \n",
            "BATCH: 407/1074\n",
            " train loss item: 0.6821204423904419\n",
            " f1:0.17922746\n",
            " avg_train_loss: 0.70579683 \n",
            "BATCH: 408/1074\n",
            " train loss item: 0.6895745992660522\n",
            " f1:0.17917809\n",
            " avg_train_loss: 0.70579236 \n",
            "BATCH: 409/1074\n",
            " train loss item: 0.6893248558044434\n",
            " f1:0.17912874\n",
            " avg_train_loss: 0.70578782 \n",
            "BATCH: 410/1074\n",
            " train loss item: 0.7036340832710266\n",
            " f1:0.17907942\n",
            " avg_train_loss: 0.70578723 \n",
            "BATCH: 411/1074\n",
            " train loss item: 0.7086639404296875\n",
            " f1:0.17903013\n",
            " avg_train_loss: 0.70578802 \n",
            "BATCH: 412/1074\n",
            " train loss item: 0.6983327865600586\n",
            " f1:0.17898086\n",
            " avg_train_loss: 0.70578597 \n",
            "BATCH: 413/1074\n",
            " train loss item: 0.6936799883842468\n",
            " f1:0.17893162\n",
            " avg_train_loss: 0.70578264 \n",
            "BATCH: 414/1074\n",
            " train loss item: 0.6972910761833191\n",
            " f1:0.17888241\n",
            " avg_train_loss: 0.70578030 \n",
            "BATCH: 415/1074\n",
            " train loss item: 0.6934995651245117\n",
            " f1:0.17883323\n",
            " avg_train_loss: 0.70577693 \n",
            "BATCH: 416/1074\n",
            " train loss item: 0.6964419484138489\n",
            " f1:0.17878407\n",
            " avg_train_loss: 0.70577436 \n",
            "BATCH: 417/1074\n",
            " train loss item: 0.690690279006958\n",
            " f1:0.17873494\n",
            " avg_train_loss: 0.70577022 \n",
            "BATCH: 418/1074\n",
            " train loss item: 0.695837140083313\n",
            " f1:0.17868584\n",
            " avg_train_loss: 0.70576749 \n",
            "BATCH: 419/1074\n",
            " train loss item: 0.6910495758056641\n",
            " f1:0.17863676\n",
            " avg_train_loss: 0.70576344 \n",
            "BATCH: 420/1074\n",
            " train loss item: 0.6868724822998047\n",
            " f1:0.17858771\n",
            " avg_train_loss: 0.70575826 \n",
            "BATCH: 421/1074\n",
            " train loss item: 0.6956135630607605\n",
            " f1:0.17853869\n",
            " avg_train_loss: 0.70575547 \n",
            "BATCH: 422/1074\n",
            " train loss item: 0.6933259963989258\n",
            " f1:0.17848970\n",
            " avg_train_loss: 0.70575206 \n",
            "BATCH: 423/1074\n",
            " train loss item: 0.6884818077087402\n",
            " f1:0.17844073\n",
            " avg_train_loss: 0.70574732 \n",
            "BATCH: 424/1074\n",
            " train loss item: 0.6960709095001221\n",
            " f1:0.17839179\n",
            " avg_train_loss: 0.70574467 \n",
            "BATCH: 425/1074\n",
            " train loss item: 0.6934012174606323\n",
            " f1:0.17834287\n",
            " avg_train_loss: 0.70574128 \n",
            "BATCH: 426/1074\n",
            " train loss item: 0.6846678853034973\n",
            " f1:0.17829398\n",
            " avg_train_loss: 0.70573551 \n",
            "BATCH: 427/1074\n",
            " train loss item: 0.6969098448753357\n",
            " f1:0.17824512\n",
            " avg_train_loss: 0.70573309 \n",
            "BATCH: 428/1074\n",
            " train loss item: 0.700910210609436\n",
            " f1:0.17819629\n",
            " avg_train_loss: 0.70573177 \n",
            "BATCH: 429/1074\n",
            " train loss item: 0.6970982551574707\n",
            " f1:0.17814748\n",
            " avg_train_loss: 0.70572940 \n",
            "BATCH: 430/1074\n",
            " train loss item: 0.6999984383583069\n",
            " f1:0.17809870\n",
            " avg_train_loss: 0.70572783 \n",
            "BATCH: 431/1074\n",
            " train loss item: 0.6960974931716919\n",
            " f1:0.17804995\n",
            " avg_train_loss: 0.70572520 \n",
            "BATCH: 432/1074\n",
            " train loss item: 0.6932957172393799\n",
            " f1:0.17800122\n",
            " avg_train_loss: 0.70572180 \n",
            "BATCH: 433/1074\n",
            " train loss item: 0.6864327192306519\n",
            " f1:0.17795252\n",
            " avg_train_loss: 0.70571652 \n",
            "BATCH: 434/1074\n",
            " train loss item: 0.6932364702224731\n",
            " f1:0.17790384\n",
            " avg_train_loss: 0.70571311 \n",
            "BATCH: 435/1074\n",
            " train loss item: 0.6932340860366821\n",
            " f1:0.17785520\n",
            " avg_train_loss: 0.70570969 \n",
            "BATCH: 436/1074\n",
            " train loss item: 0.6899632215499878\n",
            " f1:0.17780658\n",
            " avg_train_loss: 0.70570539 \n",
            "BATCH: 437/1074\n",
            " train loss item: 0.6949902772903442\n",
            " f1:0.17775798\n",
            " avg_train_loss: 0.70570246 \n",
            "BATCH: 438/1074\n",
            " train loss item: 0.6896775960922241\n",
            " f1:0.17770941\n",
            " avg_train_loss: 0.70569808 \n",
            "BATCH: 439/1074\n",
            " train loss item: 0.6951807737350464\n",
            " f1:0.17766087\n",
            " avg_train_loss: 0.70569521 \n",
            "BATCH: 440/1074\n",
            " train loss item: 0.6972345113754272\n",
            " f1:0.17761236\n",
            " avg_train_loss: 0.70569290 \n",
            "BATCH: 441/1074\n",
            " train loss item: 0.6932692527770996\n",
            " f1:0.17756387\n",
            " avg_train_loss: 0.70568951 \n",
            "BATCH: 442/1074\n",
            " train loss item: 0.6894140839576721\n",
            " f1:0.17751541\n",
            " avg_train_loss: 0.70568506 \n",
            "BATCH: 443/1074\n",
            " train loss item: 0.6952526569366455\n",
            " f1:0.17746697\n",
            " avg_train_loss: 0.70568222 \n",
            "BATCH: 444/1074\n",
            " train loss item: 0.6932731866836548\n",
            " f1:0.17741856\n",
            " avg_train_loss: 0.70567883 \n",
            "BATCH: 445/1074\n",
            " train loss item: 0.6912881135940552\n",
            " f1:0.17737018\n",
            " avg_train_loss: 0.70567491 \n",
            "BATCH: 446/1074\n",
            " train loss item: 0.6932779550552368\n",
            " f1:0.17732183\n",
            " avg_train_loss: 0.70567153 \n",
            "BATCH: 447/1074\n",
            " train loss item: 0.6871346235275269\n",
            " f1:0.17727350\n",
            " avg_train_loss: 0.70566648 \n",
            "BATCH: 448/1074\n",
            " train loss item: 0.6889266967773438\n",
            " f1:0.17722519\n",
            " avg_train_loss: 0.70566192 \n",
            "BATCH: 449/1074\n",
            " train loss item: 0.6885638236999512\n",
            " f1:0.17717692\n",
            " avg_train_loss: 0.70565726 \n",
            "BATCH: 450/1074\n",
            " train loss item: 0.6907343864440918\n",
            " f1:0.17712866\n",
            " avg_train_loss: 0.70565319 \n",
            "BATCH: 451/1074\n",
            " train loss item: 0.6876040697097778\n",
            " f1:0.17708044\n",
            " avg_train_loss: 0.70564828 \n",
            "BATCH: 452/1074\n",
            " train loss item: 0.6836814880371094\n",
            " f1:0.17703224\n",
            " avg_train_loss: 0.70564230 \n",
            "BATCH: 453/1074\n",
            " train loss item: 0.6859079599380493\n",
            " f1:0.17698407\n",
            " avg_train_loss: 0.70563693 \n",
            "BATCH: 454/1074\n",
            " train loss item: 0.7032557725906372\n",
            " f1:0.17693592\n",
            " avg_train_loss: 0.70563628 \n",
            "BATCH: 455/1074\n",
            " train loss item: 0.6888473629951477\n",
            " f1:0.17688780\n",
            " avg_train_loss: 0.70563172 \n",
            "BATCH: 456/1074\n",
            " train loss item: 0.7056236267089844\n",
            " f1:0.17683971\n",
            " avg_train_loss: 0.70563172 \n",
            "BATCH: 457/1074\n",
            " train loss item: 0.688515841960907\n",
            " f1:0.17679164\n",
            " avg_train_loss: 0.70562706 \n",
            "BATCH: 458/1074\n",
            " train loss item: 0.6942029595375061\n",
            " f1:0.17674360\n",
            " avg_train_loss: 0.70562396 \n",
            "BATCH: 459/1074\n",
            " train loss item: 0.6941968202590942\n",
            " f1:0.17669559\n",
            " avg_train_loss: 0.70562085 \n",
            "BATCH: 460/1074\n",
            " train loss item: 0.6885409355163574\n",
            " f1:0.17664760\n",
            " avg_train_loss: 0.70561622 \n",
            "BATCH: 461/1074\n",
            " train loss item: 0.671507716178894\n",
            " f1:0.17659964\n",
            " avg_train_loss: 0.70560695 \n",
            "BATCH: 462/1074\n",
            " train loss item: 0.6744781136512756\n",
            " f1:0.17655170\n",
            " avg_train_loss: 0.70559850 \n",
            "BATCH: 463/1074\n",
            " train loss item: 0.678098738193512\n",
            " f1:0.17650379\n",
            " avg_train_loss: 0.70559104 \n",
            "BATCH: 464/1074\n",
            " train loss item: 0.6504603624343872\n",
            " f1:0.17645590\n",
            " avg_train_loss: 0.70557609 \n",
            "BATCH: 465/1074\n",
            " train loss item: 0.6674816012382507\n",
            " f1:0.17640804\n",
            " avg_train_loss: 0.70556575 \n",
            "BATCH: 466/1074\n",
            " train loss item: 0.688876748085022\n",
            " f1:0.17636021\n",
            " avg_train_loss: 0.70556123 \n",
            "BATCH: 467/1074\n",
            " train loss item: 0.6616860628128052\n",
            " f1:0.17631240\n",
            " avg_train_loss: 0.70554933 \n",
            "BATCH: 468/1074\n",
            " train loss item: 0.8294087648391724\n",
            " f1:0.17626462\n",
            " avg_train_loss: 0.70558290 \n",
            "BATCH: 469/1074\n",
            " train loss item: 0.7959394454956055\n",
            " f1:0.17621687\n",
            " avg_train_loss: 0.70560738 \n",
            "BATCH: 470/1074\n",
            " train loss item: 0.7055039405822754\n",
            " f1:0.17616914\n",
            " avg_train_loss: 0.70560735 \n",
            "BATCH: 471/1074\n",
            " train loss item: 0.6862589716911316\n",
            " f1:0.17612143\n",
            " avg_train_loss: 0.70560211 \n",
            "BATCH: 472/1074\n",
            " train loss item: 0.7059513330459595\n",
            " f1:0.17607376\n",
            " avg_train_loss: 0.70560221 \n",
            "BATCH: 473/1074\n",
            " train loss item: 0.6890872716903687\n",
            " f1:0.17602610\n",
            " avg_train_loss: 0.70559774 \n",
            "BATCH: 474/1074\n",
            " train loss item: 0.7077158689498901\n",
            " f1:0.17597848\n",
            " avg_train_loss: 0.70559831 \n",
            "BATCH: 475/1074\n",
            " train loss item: 0.6959867477416992\n",
            " f1:0.17593088\n",
            " avg_train_loss: 0.70559571 \n",
            "BATCH: 476/1074\n",
            " train loss item: 0.7239202260971069\n",
            " f1:0.17588330\n",
            " avg_train_loss: 0.70560067 \n",
            "BATCH: 477/1074\n",
            " train loss item: 0.6714510321617126\n",
            " f1:0.17583575\n",
            " avg_train_loss: 0.70559143 \n",
            "BATCH: 478/1074\n",
            " train loss item: 0.685325026512146\n",
            " f1:0.17578823\n",
            " avg_train_loss: 0.70558596 \n",
            "BATCH: 479/1074\n",
            " train loss item: 0.6855747699737549\n",
            " f1:0.17574073\n",
            " avg_train_loss: 0.70558055 \n",
            "BATCH: 480/1074\n",
            " train loss item: 0.7271586060523987\n",
            " f1:0.17569326\n",
            " avg_train_loss: 0.70558638 \n",
            "BATCH: 481/1074\n",
            " train loss item: 0.741560697555542\n",
            " f1:0.17564582\n",
            " avg_train_loss: 0.70559609 \n",
            "BATCH: 482/1074\n",
            " train loss item: 0.661023736000061\n",
            " f1:0.17559840\n",
            " avg_train_loss: 0.70558406 \n",
            "BATCH: 483/1074\n",
            " train loss item: 0.696169376373291\n",
            " f1:0.17555100\n",
            " avg_train_loss: 0.70558152 \n",
            "BATCH: 484/1074\n",
            " train loss item: 0.7081304788589478\n",
            " f1:0.17550363\n",
            " avg_train_loss: 0.70558221 \n",
            "BATCH: 485/1074\n",
            " train loss item: 0.6948446035385132\n",
            " f1:0.17545629\n",
            " avg_train_loss: 0.70557931 \n",
            "BATCH: 486/1074\n",
            " train loss item: 0.7015511989593506\n",
            " f1:0.17555607\n",
            " avg_train_loss: 0.70557822 \n",
            "BATCH: 487/1074\n",
            " train loss item: 0.7006803154945374\n",
            " f1:0.17567285\n",
            " avg_train_loss: 0.70557690 \n",
            "BATCH: 488/1074\n",
            " train loss item: 0.6746380925178528\n",
            " f1:0.17584513\n",
            " avg_train_loss: 0.70556856 \n",
            "BATCH: 489/1074\n",
            " train loss item: 0.6960631012916565\n",
            " f1:0.17597739\n",
            " avg_train_loss: 0.70556600 \n",
            "BATCH: 490/1074\n",
            " train loss item: 0.6972959041595459\n",
            " f1:0.17610958\n",
            " avg_train_loss: 0.70556378 \n",
            "BATCH: 491/1074\n",
            " train loss item: 0.6977875828742981\n",
            " f1:0.17624170\n",
            " avg_train_loss: 0.70556168 \n",
            "BATCH: 492/1074\n",
            " train loss item: 0.6742885112762451\n",
            " f1:0.17640136\n",
            " avg_train_loss: 0.70555326 \n",
            "BATCH: 493/1074\n",
            " train loss item: 0.6982500553131104\n",
            " f1:0.17653333\n",
            " avg_train_loss: 0.70555129 \n",
            "BATCH: 494/1074\n",
            " train loss item: 0.710608720779419\n",
            " f1:0.17664963\n",
            " avg_train_loss: 0.70555266 \n",
            "BATCH: 495/1074\n",
            " train loss item: 0.686369776725769\n",
            " f1:0.17679581\n",
            " avg_train_loss: 0.70554749 \n",
            "BATCH: 496/1074\n",
            " train loss item: 0.6871461272239685\n",
            " f1:0.17694191\n",
            " avg_train_loss: 0.70554255 \n",
            "BATCH: 497/1074\n",
            " train loss item: 0.7087242603302002\n",
            " f1:0.17704100\n",
            " avg_train_loss: 0.70554340 \n",
            "BATCH: 498/1074\n",
            " train loss item: 0.6935017108917236\n",
            " f1:0.17717262\n",
            " avg_train_loss: 0.70554016 \n",
            "BATCH: 499/1074\n",
            " train loss item: 0.6931866407394409\n",
            " f1:0.17712500\n",
            " avg_train_loss: 0.70553684 \n",
            "BATCH: 500/1074\n",
            " train loss item: 0.6892490983009338\n",
            " f1:0.17707741\n",
            " avg_train_loss: 0.70553247 \n",
            "BATCH: 501/1074\n",
            " train loss item: 0.7048642039299011\n",
            " f1:0.17702985\n",
            " avg_train_loss: 0.70553229 \n",
            "BATCH: 502/1074\n",
            " train loss item: 0.7006282806396484\n",
            " f1:0.17698231\n",
            " avg_train_loss: 0.70553097 \n",
            "BATCH: 503/1074\n",
            " train loss item: 0.6942126154899597\n",
            " f1:0.17693480\n",
            " avg_train_loss: 0.70552793 \n",
            "BATCH: 504/1074\n",
            " train loss item: 0.6990666389465332\n",
            " f1:0.17688732\n",
            " avg_train_loss: 0.70552620 \n",
            "BATCH: 505/1074\n",
            " train loss item: 0.6971592903137207\n",
            " f1:0.17683985\n",
            " avg_train_loss: 0.70552395 \n",
            "BATCH: 506/1074\n",
            " train loss item: 0.6916147470474243\n",
            " f1:0.17679242\n",
            " avg_train_loss: 0.70552022 \n",
            "BATCH: 507/1074\n",
            " train loss item: 0.6906858086585999\n",
            " f1:0.17674501\n",
            " avg_train_loss: 0.70551624 \n",
            "BATCH: 508/1074\n",
            " train loss item: 0.6912147998809814\n",
            " f1:0.17669762\n",
            " avg_train_loss: 0.70551241 \n",
            "BATCH: 509/1074\n",
            " train loss item: 0.6786819100379944\n",
            " f1:0.17665026\n",
            " avg_train_loss: 0.70550522 \n",
            "BATCH: 510/1074\n",
            " train loss item: 0.6812205910682678\n",
            " f1:0.17660293\n",
            " avg_train_loss: 0.70549871 \n",
            "BATCH: 511/1074\n",
            " train loss item: 0.69633948802948\n",
            " f1:0.17655562\n",
            " avg_train_loss: 0.70549626 \n",
            "BATCH: 512/1074\n",
            " train loss item: 0.7232629656791687\n",
            " f1:0.17650834\n",
            " avg_train_loss: 0.70550102 \n",
            "BATCH: 513/1074\n",
            " train loss item: 0.685630202293396\n",
            " f1:0.17646108\n",
            " avg_train_loss: 0.70549570 \n",
            "BATCH: 514/1074\n",
            " train loss item: 0.7230135202407837\n",
            " f1:0.17641385\n",
            " avg_train_loss: 0.70550039 \n",
            "BATCH: 515/1074\n",
            " train loss item: 0.7064095735549927\n",
            " f1:0.17636664\n",
            " avg_train_loss: 0.70550063 \n",
            "BATCH: 516/1074\n",
            " train loss item: 0.7013896703720093\n",
            " f1:0.17631946\n",
            " avg_train_loss: 0.70549953 \n",
            "BATCH: 517/1074\n",
            " train loss item: 0.696656346321106\n",
            " f1:0.17627230\n",
            " avg_train_loss: 0.70549716 \n",
            "BATCH: 518/1074\n",
            " train loss item: 0.6931535005569458\n",
            " f1:0.17640342\n",
            " avg_train_loss: 0.70549386 \n",
            "BATCH: 519/1074\n",
            " train loss item: 0.6935406923294067\n",
            " f1:0.17653447\n",
            " avg_train_loss: 0.70549067 \n",
            "BATCH: 520/1074\n",
            " train loss item: 0.718176007270813\n",
            " f1:0.17659419\n",
            " avg_train_loss: 0.70549406 \n",
            "BATCH: 521/1074\n",
            " train loss item: 0.7036392688751221\n",
            " f1:0.17669274\n",
            " avg_train_loss: 0.70549356 \n",
            "BATCH: 522/1074\n",
            " train loss item: 0.6910292506217957\n",
            " f1:0.17683785\n",
            " avg_train_loss: 0.70548970 \n",
            "BATCH: 523/1074\n",
            " train loss item: 0.6919384002685547\n",
            " f1:0.17700821\n",
            " avg_train_loss: 0.70548608 \n",
            "BATCH: 524/1074\n",
            " train loss item: 0.6930255889892578\n",
            " f1:0.17715316\n",
            " avg_train_loss: 0.70548275 \n",
            "BATCH: 525/1074\n",
            " train loss item: 0.6931509971618652\n",
            " f1:0.17728380\n",
            " avg_train_loss: 0.70547946 \n",
            "BATCH: 526/1074\n",
            " train loss item: 0.6957294940948486\n",
            " f1:0.17732076\n",
            " avg_train_loss: 0.70547686 \n",
            "BATCH: 527/1074\n",
            " train loss item: 0.6845418810844421\n",
            " f1:0.17727346\n",
            " avg_train_loss: 0.70547128 \n",
            "BATCH: 528/1074\n",
            " train loss item: 0.6810212135314941\n",
            " f1:0.17722619\n",
            " avg_train_loss: 0.70546476 \n",
            "BATCH: 529/1074\n",
            " train loss item: 0.673578679561615\n",
            " f1:0.17717894\n",
            " avg_train_loss: 0.70545626 \n",
            "BATCH: 530/1074\n",
            " train loss item: 0.6855559349060059\n",
            " f1:0.17713171\n",
            " avg_train_loss: 0.70545095 \n",
            "BATCH: 531/1074\n",
            " train loss item: 0.6387726068496704\n",
            " f1:0.17708452\n",
            " avg_train_loss: 0.70543319 \n",
            "BATCH: 532/1074\n",
            " train loss item: 0.6950021386146545\n",
            " f1:0.17703735\n",
            " avg_train_loss: 0.70543041 \n",
            "BATCH: 533/1074\n",
            " train loss item: 0.6631338596343994\n",
            " f1:0.17699020\n",
            " avg_train_loss: 0.70541914 \n",
            "BATCH: 534/1074\n",
            " train loss item: 0.7947174310684204\n",
            " f1:0.17694308\n",
            " avg_train_loss: 0.70544292 \n",
            "BATCH: 535/1074\n",
            " train loss item: 0.6616032123565674\n",
            " f1:0.17689598\n",
            " avg_train_loss: 0.70543125 \n",
            "BATCH: 536/1074\n",
            " train loss item: 0.6634411811828613\n",
            " f1:0.17684891\n",
            " avg_train_loss: 0.70542008 \n",
            "BATCH: 537/1074\n",
            " train loss item: 0.7376201748847961\n",
            " f1:0.17680186\n",
            " avg_train_loss: 0.70542864 \n",
            "BATCH: 538/1074\n",
            " train loss item: 0.7018001079559326\n",
            " f1:0.17675484\n",
            " avg_train_loss: 0.70542768 \n",
            "BATCH: 539/1074\n",
            " train loss item: 0.6975185871124268\n",
            " f1:0.17688510\n",
            " avg_train_loss: 0.70542557 \n",
            "BATCH: 540/1074\n",
            " train loss item: 0.7564345002174377\n",
            " f1:0.17698307\n",
            " avg_train_loss: 0.70543913 \n",
            "BATCH: 541/1074\n",
            " train loss item: 0.7608773708343506\n",
            " f1:0.17708099\n",
            " avg_train_loss: 0.70545387 \n",
            "BATCH: 542/1074\n",
            " train loss item: 0.756523072719574\n",
            " f1:0.17716046\n",
            " avg_train_loss: 0.70546743 \n",
            "BATCH: 543/1074\n",
            " train loss item: 0.7157617211341858\n",
            " f1:0.17723988\n",
            " avg_train_loss: 0.70547017 \n",
            "BATCH: 544/1074\n",
            " train loss item: 0.6954879760742188\n",
            " f1:0.17719282\n",
            " avg_train_loss: 0.70546752 \n",
            "BATCH: 545/1074\n",
            " train loss item: 0.694520115852356\n",
            " f1:0.17714578\n",
            " avg_train_loss: 0.70546461 \n",
            "BATCH: 546/1074\n",
            " train loss item: 0.6718761920928955\n",
            " f1:0.17709877\n",
            " avg_train_loss: 0.70545570 \n",
            "BATCH: 547/1074\n",
            " train loss item: 0.6871469020843506\n",
            " f1:0.17705178\n",
            " avg_train_loss: 0.70545084 \n",
            "BATCH: 548/1074\n",
            " train loss item: 0.7028411626815796\n",
            " f1:0.17700481\n",
            " avg_train_loss: 0.70545015 \n",
            "BATCH: 549/1074\n",
            " train loss item: 0.7027125358581543\n",
            " f1:0.17695788\n",
            " avg_train_loss: 0.70544942 \n",
            "BATCH: 550/1074\n",
            " train loss item: 0.6937446594238281\n",
            " f1:0.17708770\n",
            " avg_train_loss: 0.70544632 \n",
            "BATCH: 551/1074\n",
            " train loss item: 0.698887825012207\n",
            " f1:0.17721746\n",
            " avg_train_loss: 0.70544458 \n",
            "BATCH: 552/1074\n",
            " train loss item: 0.6859484314918518\n",
            " f1:0.17736128\n",
            " avg_train_loss: 0.70543941 \n",
            "BATCH: 553/1074\n",
            " train loss item: 0.7664279937744141\n",
            " f1:0.17745879\n",
            " avg_train_loss: 0.70545557 \n",
            "BATCH: 554/1074\n",
            " train loss item: 0.6853312849998474\n",
            " f1:0.17760247\n",
            " avg_train_loss: 0.70545024 \n",
            "BATCH: 555/1074\n",
            " train loss item: 0.6948604583740234\n",
            " f1:0.17773196\n",
            " avg_train_loss: 0.70544744 \n",
            "BATCH: 556/1074\n",
            " train loss item: 0.690291702747345\n",
            " f1:0.17768491\n",
            " avg_train_loss: 0.70544343 \n",
            "BATCH: 557/1074\n",
            " train loss item: 0.6688705682754517\n",
            " f1:0.17763790\n",
            " avg_train_loss: 0.70543375 \n",
            "BATCH: 558/1074\n",
            " train loss item: 0.7618986368179321\n",
            " f1:0.17759090\n",
            " avg_train_loss: 0.70544869 \n",
            "BATCH: 559/1074\n",
            " train loss item: 0.6615945100784302\n",
            " f1:0.17754393\n",
            " avg_train_loss: 0.70543709 \n",
            "BATCH: 560/1074\n",
            " train loss item: 0.7903767228126526\n",
            " f1:0.17749699\n",
            " avg_train_loss: 0.70545955 \n",
            "BATCH: 561/1074\n",
            " train loss item: 0.671602725982666\n",
            " f1:0.17745007\n",
            " avg_train_loss: 0.70545060 \n",
            "BATCH: 562/1074\n",
            " train loss item: 0.6838855147361755\n",
            " f1:0.17740317\n",
            " avg_train_loss: 0.70544490 \n",
            "BATCH: 563/1074\n",
            " train loss item: 0.6798973083496094\n",
            " f1:0.17735630\n",
            " avg_train_loss: 0.70543815 \n",
            "BATCH: 564/1074\n",
            " train loss item: 0.7728272080421448\n",
            " f1:0.17730946\n",
            " avg_train_loss: 0.70545595 \n",
            "BATCH: 565/1074\n",
            " train loss item: 0.7025230526924133\n",
            " f1:0.17726264\n",
            " avg_train_loss: 0.70545517 \n",
            "BATCH: 566/1074\n",
            " train loss item: 0.6969478130340576\n",
            " f1:0.17734155\n",
            " avg_train_loss: 0.70545293 \n",
            "BATCH: 567/1074\n",
            " train loss item: 0.6954171657562256\n",
            " f1:0.17745540\n",
            " avg_train_loss: 0.70545028 \n",
            "BATCH: 568/1074\n",
            " train loss item: 0.694862961769104\n",
            " f1:0.17755249\n",
            " avg_train_loss: 0.70544748 \n",
            "BATCH: 569/1074\n",
            " train loss item: 0.6934692859649658\n",
            " f1:0.17750566\n",
            " avg_train_loss: 0.70544432 \n",
            "BATCH: 570/1074\n",
            " train loss item: 0.706562876701355\n",
            " f1:0.17745885\n",
            " avg_train_loss: 0.70544462 \n",
            "BATCH: 571/1074\n",
            " train loss item: 0.6942125558853149\n",
            " f1:0.17741206\n",
            " avg_train_loss: 0.70544166 \n",
            "BATCH: 572/1074\n",
            " train loss item: 0.6888478398323059\n",
            " f1:0.17736530\n",
            " avg_train_loss: 0.70543728 \n",
            "BATCH: 573/1074\n",
            " train loss item: 0.6940224170684814\n",
            " f1:0.17731856\n",
            " avg_train_loss: 0.70543428 \n",
            "BATCH: 574/1074\n",
            " train loss item: 0.6939623355865479\n",
            " f1:0.17727185\n",
            " avg_train_loss: 0.70543125 \n",
            "BATCH: 575/1074\n",
            " train loss item: 0.6985332369804382\n",
            " f1:0.17722516\n",
            " avg_train_loss: 0.70542944 \n",
            "BATCH: 576/1074\n",
            " train loss item: 0.6935577392578125\n",
            " f1:0.17717850\n",
            " avg_train_loss: 0.70542631 \n",
            "BATCH: 577/1074\n",
            " train loss item: 0.6984667778015137\n",
            " f1:0.17713186\n",
            " avg_train_loss: 0.70542448 \n",
            "BATCH: 578/1074\n",
            " train loss item: 0.6931566596031189\n",
            " f1:0.17708525\n",
            " avg_train_loss: 0.70542125 \n",
            "BATCH: 579/1074\n",
            " train loss item: 0.6920220255851746\n",
            " f1:0.17722808\n",
            " avg_train_loss: 0.70541773 \n",
            "BATCH: 580/1074\n",
            " train loss item: 0.6903262734413147\n",
            " f1:0.17737084\n",
            " avg_train_loss: 0.70541376 \n",
            "BATCH: 581/1074\n",
            " train loss item: 0.6887207627296448\n",
            " f1:0.17751353\n",
            " avg_train_loss: 0.70540937 \n",
            "BATCH: 582/1074\n",
            " train loss item: 0.687308132648468\n",
            " f1:0.17765614\n",
            " avg_train_loss: 0.70540461 \n",
            "BATCH: 583/1074\n",
            " train loss item: 0.7070132493972778\n",
            " f1:0.17776942\n",
            " avg_train_loss: 0.70540503 \n",
            "BATCH: 584/1074\n",
            " train loss item: 0.6972334980964661\n",
            " f1:0.17789787\n",
            " avg_train_loss: 0.70540289 \n",
            "BATCH: 585/1074\n",
            " train loss item: 0.720026969909668\n",
            " f1:0.17799442\n",
            " avg_train_loss: 0.70540673 \n",
            "BATCH: 586/1074\n",
            " train loss item: 0.7044522166252136\n",
            " f1:0.17810753\n",
            " avg_train_loss: 0.70540648 \n",
            "BATCH: 587/1074\n",
            " train loss item: 0.688743531703949\n",
            " f1:0.17824979\n",
            " avg_train_loss: 0.70540210 \n",
            "BATCH: 588/1074\n",
            " train loss item: 0.6989398002624512\n",
            " f1:0.17834617\n",
            " avg_train_loss: 0.70540040 \n",
            "BATCH: 589/1074\n",
            " train loss item: 0.6955857276916504\n",
            " f1:0.17829937\n",
            " avg_train_loss: 0.70539783 \n",
            "BATCH: 590/1074\n",
            " train loss item: 0.6892122030258179\n",
            " f1:0.17825260\n",
            " avg_train_loss: 0.70539358 \n",
            "BATCH: 591/1074\n",
            " train loss item: 0.6978313326835632\n",
            " f1:0.17820585\n",
            " avg_train_loss: 0.70539160 \n",
            "BATCH: 592/1074\n",
            " train loss item: 0.6835358738899231\n",
            " f1:0.17815913\n",
            " avg_train_loss: 0.70538587 \n",
            "BATCH: 593/1074\n",
            " train loss item: 0.6876600980758667\n",
            " f1:0.17811243\n",
            " avg_train_loss: 0.70538122 \n",
            "BATCH: 594/1074\n",
            " train loss item: 0.7228808403015137\n",
            " f1:0.17806575\n",
            " avg_train_loss: 0.70538581 \n",
            "BATCH: 595/1074\n",
            " train loss item: 0.6782777309417725\n",
            " f1:0.17801910\n",
            " avg_train_loss: 0.70537871 \n",
            "BATCH: 596/1074\n",
            " train loss item: 0.6957516074180603\n",
            " f1:0.17797248\n",
            " avg_train_loss: 0.70537619 \n",
            "BATCH: 597/1074\n",
            " train loss item: 0.6957687139511108\n",
            " f1:0.17792587\n",
            " avg_train_loss: 0.70537367 \n",
            "BATCH: 598/1074\n",
            " train loss item: 0.6956129670143127\n",
            " f1:0.17787930\n",
            " avg_train_loss: 0.70537112 \n",
            "BATCH: 599/1074\n",
            " train loss item: 0.6953420042991638\n",
            " f1:0.17783274\n",
            " avg_train_loss: 0.70536849 \n",
            "BATCH: 600/1074\n",
            " train loss item: 0.6950156092643738\n",
            " f1:0.17778621\n",
            " avg_train_loss: 0.70536578 \n",
            "BATCH: 601/1074\n",
            " train loss item: 0.7016107439994812\n",
            " f1:0.17773971\n",
            " avg_train_loss: 0.70536480 \n",
            "BATCH: 602/1074\n",
            " train loss item: 0.6883668899536133\n",
            " f1:0.17769323\n",
            " avg_train_loss: 0.70536035 \n",
            "BATCH: 603/1074\n",
            " train loss item: 0.6992327570915222\n",
            " f1:0.17764677\n",
            " avg_train_loss: 0.70535875 \n",
            "BATCH: 604/1074\n",
            " train loss item: 0.6937580108642578\n",
            " f1:0.17760034\n",
            " avg_train_loss: 0.70535572 \n",
            "BATCH: 605/1074\n",
            " train loss item: 0.6935710906982422\n",
            " f1:0.17755394\n",
            " avg_train_loss: 0.70535264 \n",
            "BATCH: 606/1074\n",
            " train loss item: 0.6904210448265076\n",
            " f1:0.17750755\n",
            " avg_train_loss: 0.70534874 \n",
            "BATCH: 607/1074\n",
            " train loss item: 0.6933735013008118\n",
            " f1:0.17746119\n",
            " avg_train_loss: 0.70534561 \n",
            "BATCH: 608/1074\n",
            " train loss item: 0.6839043498039246\n",
            " f1:0.17741486\n",
            " avg_train_loss: 0.70534001 \n",
            "BATCH: 609/1074\n",
            " train loss item: 0.6906554698944092\n",
            " f1:0.17736855\n",
            " avg_train_loss: 0.70533618 \n",
            "BATCH: 610/1074\n",
            " train loss item: 0.6902984976768494\n",
            " f1:0.17732226\n",
            " avg_train_loss: 0.70533226 \n",
            "BATCH: 611/1074\n",
            " train loss item: 0.7044810652732849\n",
            " f1:0.17727600\n",
            " avg_train_loss: 0.70533204 \n",
            "BATCH: 612/1074\n",
            " train loss item: 0.6861748695373535\n",
            " f1:0.17722976\n",
            " avg_train_loss: 0.70532704 \n",
            "BATCH: 613/1074\n",
            " train loss item: 0.6857594847679138\n",
            " f1:0.17718355\n",
            " avg_train_loss: 0.70532194 \n",
            "BATCH: 614/1074\n",
            " train loss item: 0.6937397718429565\n",
            " f1:0.17713736\n",
            " avg_train_loss: 0.70531892 \n",
            "BATCH: 615/1074\n",
            " train loss item: 0.6892309784889221\n",
            " f1:0.17709119\n",
            " avg_train_loss: 0.70531472 \n",
            "BATCH: 616/1074\n",
            " train loss item: 0.6841294169425964\n",
            " f1:0.17704505\n",
            " avg_train_loss: 0.70530920 \n",
            "BATCH: 617/1074\n",
            " train loss item: 0.6940395832061768\n",
            " f1:0.17699894\n",
            " avg_train_loss: 0.70530627 \n",
            "BATCH: 618/1074\n",
            " train loss item: 0.6717653274536133\n",
            " f1:0.17695284\n",
            " avg_train_loss: 0.70529753 \n",
            "BATCH: 619/1074\n",
            " train loss item: 0.6819906234741211\n",
            " f1:0.17690677\n",
            " avg_train_loss: 0.70529147 \n",
            "BATCH: 620/1074\n",
            " train loss item: 0.7085564732551575\n",
            " f1:0.17686073\n",
            " avg_train_loss: 0.70529232 \n",
            "BATCH: 621/1074\n",
            " train loss item: 0.7094444036483765\n",
            " f1:0.17681471\n",
            " avg_train_loss: 0.70529340 \n",
            "BATCH: 622/1074\n",
            " train loss item: 0.6731085777282715\n",
            " f1:0.17676871\n",
            " avg_train_loss: 0.70528502 \n",
            "BATCH: 623/1074\n",
            " train loss item: 0.6798833608627319\n",
            " f1:0.17672273\n",
            " avg_train_loss: 0.70527842 \n",
            "BATCH: 624/1074\n",
            " train loss item: 0.6952322721481323\n",
            " f1:0.17667678\n",
            " avg_train_loss: 0.70527581 \n",
            "BATCH: 625/1074\n",
            " train loss item: 0.686967134475708\n",
            " f1:0.17663086\n",
            " avg_train_loss: 0.70527105 \n",
            "BATCH: 626/1074\n",
            " train loss item: 0.6777907609939575\n",
            " f1:0.17658496\n",
            " avg_train_loss: 0.70526390 \n",
            "BATCH: 627/1074\n",
            " train loss item: 0.7256631255149841\n",
            " f1:0.17653908\n",
            " avg_train_loss: 0.70526920 \n",
            "BATCH: 628/1074\n",
            " train loss item: 0.7054998874664307\n",
            " f1:0.17649322\n",
            " avg_train_loss: 0.70526926 \n",
            "BATCH: 629/1074\n",
            " train loss item: 0.6781006455421448\n",
            " f1:0.17644739\n",
            " avg_train_loss: 0.70526221 \n",
            "BATCH: 630/1074\n",
            " train loss item: 0.6700371503829956\n",
            " f1:0.17640159\n",
            " avg_train_loss: 0.70525306 \n",
            "BATCH: 631/1074\n",
            " train loss item: 0.6956121921539307\n",
            " f1:0.17635580\n",
            " avg_train_loss: 0.70525056 \n",
            "BATCH: 632/1074\n",
            " train loss item: 0.7135571837425232\n",
            " f1:0.17631005\n",
            " avg_train_loss: 0.70525272 \n",
            "BATCH: 633/1074\n",
            " train loss item: 0.6869906187057495\n",
            " f1:0.17626431\n",
            " avg_train_loss: 0.70524798 \n",
            "BATCH: 634/1074\n",
            " train loss item: 0.7195461392402649\n",
            " f1:0.17621860\n",
            " avg_train_loss: 0.70525169 \n",
            "BATCH: 635/1074\n",
            " train loss item: 0.6804593801498413\n",
            " f1:0.17617291\n",
            " avg_train_loss: 0.70524526 \n",
            "BATCH: 636/1074\n",
            " train loss item: 0.7012197971343994\n",
            " f1:0.17612725\n",
            " avg_train_loss: 0.70524422 \n",
            "BATCH: 637/1074\n",
            " train loss item: 0.6760839223861694\n",
            " f1:0.17608160\n",
            " avg_train_loss: 0.70523666 \n",
            "BATCH: 638/1074\n",
            " train loss item: 0.6942613124847412\n",
            " f1:0.17603599\n",
            " avg_train_loss: 0.70523382 \n",
            "BATCH: 639/1074\n",
            " train loss item: 0.6770069003105164\n",
            " f1:0.17599039\n",
            " avg_train_loss: 0.70522651 \n",
            "BATCH: 640/1074\n",
            " train loss item: 0.6942316293716431\n",
            " f1:0.17594482\n",
            " avg_train_loss: 0.70522366 \n",
            "BATCH: 641/1074\n",
            " train loss item: 0.6824949979782104\n",
            " f1:0.17589928\n",
            " avg_train_loss: 0.70521778 \n",
            "BATCH: 642/1074\n",
            " train loss item: 0.6943172216415405\n",
            " f1:0.17585376\n",
            " avg_train_loss: 0.70521496 \n",
            "BATCH: 643/1074\n",
            " train loss item: 0.7005350589752197\n",
            " f1:0.17580826\n",
            " avg_train_loss: 0.70521374 \n",
            "BATCH: 644/1074\n",
            " train loss item: 0.6881821155548096\n",
            " f1:0.17576278\n",
            " avg_train_loss: 0.70520934 \n",
            "BATCH: 645/1074\n",
            " train loss item: 0.6694050431251526\n",
            " f1:0.17571733\n",
            " avg_train_loss: 0.70520008 \n",
            "BATCH: 646/1074\n",
            " train loss item: 0.7075105905532837\n",
            " f1:0.17567190\n",
            " avg_train_loss: 0.70520068 \n",
            "BATCH: 647/1074\n",
            " train loss item: 0.7143191695213318\n",
            " f1:0.17562650\n",
            " avg_train_loss: 0.70520303 \n",
            "BATCH: 648/1074\n",
            " train loss item: 0.6880086064338684\n",
            " f1:0.17558111\n",
            " avg_train_loss: 0.70519859 \n",
            "BATCH: 649/1074\n",
            " train loss item: 0.7008821964263916\n",
            " f1:0.17553576\n",
            " avg_train_loss: 0.70519748 \n",
            "BATCH: 650/1074\n",
            " train loss item: 0.7195805311203003\n",
            " f1:0.17549042\n",
            " avg_train_loss: 0.70520119 \n",
            "BATCH: 651/1074\n",
            " train loss item: 0.6823339462280273\n",
            " f1:0.17544511\n",
            " avg_train_loss: 0.70519529 \n",
            "BATCH: 652/1074\n",
            " train loss item: 0.6942166090011597\n",
            " f1:0.17539982\n",
            " avg_train_loss: 0.70519245 \n",
            "BATCH: 653/1074\n",
            " train loss item: 0.6829531788825989\n",
            " f1:0.17535456\n",
            " avg_train_loss: 0.70518671 \n",
            "BATCH: 654/1074\n",
            " train loss item: 0.6941194534301758\n",
            " f1:0.17530932\n",
            " avg_train_loss: 0.70518386 \n",
            "BATCH: 655/1074\n",
            " train loss item: 0.6886625289916992\n",
            " f1:0.17526410\n",
            " avg_train_loss: 0.70517960 \n",
            "BATCH: 656/1074\n",
            " train loss item: 0.6779263019561768\n",
            " f1:0.17521891\n",
            " avg_train_loss: 0.70517257 \n",
            "BATCH: 657/1074\n",
            " train loss item: 0.7050262689590454\n",
            " f1:0.17517373\n",
            " avg_train_loss: 0.70517253 \n",
            "BATCH: 658/1074\n",
            " train loss item: 0.6940927505493164\n",
            " f1:0.17512859\n",
            " avg_train_loss: 0.70516968 \n",
            "BATCH: 659/1074\n",
            " train loss item: 0.6724756956100464\n",
            " f1:0.17508346\n",
            " avg_train_loss: 0.70516125 \n",
            "BATCH: 660/1074\n",
            " train loss item: 0.6996496915817261\n",
            " f1:0.17503836\n",
            " avg_train_loss: 0.70515983 \n",
            "BATCH: 661/1074\n",
            " train loss item: 0.6773982048034668\n",
            " f1:0.17499328\n",
            " avg_train_loss: 0.70515268 \n",
            "BATCH: 662/1074\n",
            " train loss item: 0.69994056224823\n",
            " f1:0.17494823\n",
            " avg_train_loss: 0.70515134 \n",
            "BATCH: 663/1074\n",
            " train loss item: 0.7000584006309509\n",
            " f1:0.17490320\n",
            " avg_train_loss: 0.70515003 \n",
            "BATCH: 664/1074\n",
            " train loss item: 0.6766985058784485\n",
            " f1:0.17485819\n",
            " avg_train_loss: 0.70514271 \n",
            "BATCH: 665/1074\n",
            " train loss item: 0.6942877173423767\n",
            " f1:0.17481320\n",
            " avg_train_loss: 0.70513991 \n",
            "BATCH: 666/1074\n",
            " train loss item: 0.6882575750350952\n",
            " f1:0.17476824\n",
            " avg_train_loss: 0.70513557 \n",
            "BATCH: 667/1074\n",
            " train loss item: 0.6881893873214722\n",
            " f1:0.17472330\n",
            " avg_train_loss: 0.70513122 \n",
            "BATCH: 668/1074\n",
            " train loss item: 0.6881124973297119\n",
            " f1:0.17467838\n",
            " avg_train_loss: 0.70512684 \n",
            "BATCH: 669/1074\n",
            " train loss item: 0.7138361930847168\n",
            " f1:0.17463349\n",
            " avg_train_loss: 0.70512908 \n",
            "BATCH: 670/1074\n",
            " train loss item: 0.7137065529823303\n",
            " f1:0.17458862\n",
            " avg_train_loss: 0.70513128 \n",
            "BATCH: 671/1074\n",
            " train loss item: 0.681929349899292\n",
            " f1:0.17454377\n",
            " avg_train_loss: 0.70512532 \n",
            "BATCH: 672/1074\n",
            " train loss item: 0.6820747256278992\n",
            " f1:0.17449895\n",
            " avg_train_loss: 0.70511940 \n",
            "BATCH: 673/1074\n",
            " train loss item: 0.6882263422012329\n",
            " f1:0.17445415\n",
            " avg_train_loss: 0.70511507 \n",
            "BATCH: 674/1074\n",
            " train loss item: 0.7004660367965698\n",
            " f1:0.17440937\n",
            " avg_train_loss: 0.70511387 \n",
            "BATCH: 675/1074\n",
            " train loss item: 0.688249945640564\n",
            " f1:0.17436462\n",
            " avg_train_loss: 0.70510955 \n",
            "BATCH: 676/1074\n",
            " train loss item: 0.7003855109214783\n",
            " f1:0.17431989\n",
            " avg_train_loss: 0.70510833 \n",
            "BATCH: 677/1074\n",
            " train loss item: 0.6882930994033813\n",
            " f1:0.17427518\n",
            " avg_train_loss: 0.70510402 \n",
            "BATCH: 678/1074\n",
            " train loss item: 0.6942911148071289\n",
            " f1:0.17423049\n",
            " avg_train_loss: 0.70510125 \n",
            "BATCH: 679/1074\n",
            " train loss item: 0.6704972982406616\n",
            " f1:0.17418583\n",
            " avg_train_loss: 0.70509238 \n",
            "BATCH: 680/1074\n",
            " train loss item: 0.7003065347671509\n",
            " f1:0.17414119\n",
            " avg_train_loss: 0.70509115 \n",
            "BATCH: 681/1074\n",
            " train loss item: 0.6882833242416382\n",
            " f1:0.17409657\n",
            " avg_train_loss: 0.70508684 \n",
            "BATCH: 682/1074\n",
            " train loss item: 0.7064365148544312\n",
            " f1:0.17405198\n",
            " avg_train_loss: 0.70508719 \n",
            "BATCH: 683/1074\n",
            " train loss item: 0.7003387212753296\n",
            " f1:0.17400740\n",
            " avg_train_loss: 0.70508597 \n",
            "BATCH: 684/1074\n",
            " train loss item: 0.6942876577377319\n",
            " f1:0.17396286\n",
            " avg_train_loss: 0.70508321 \n",
            "BATCH: 685/1074\n",
            " train loss item: 0.7001762390136719\n",
            " f1:0.17391833\n",
            " avg_train_loss: 0.70508195 \n",
            "BATCH: 686/1074\n",
            " train loss item: 0.6825776100158691\n",
            " f1:0.17387383\n",
            " avg_train_loss: 0.70507620 \n",
            "BATCH: 687/1074\n",
            " train loss item: 0.6884291172027588\n",
            " f1:0.17382935\n",
            " avg_train_loss: 0.70507194 \n",
            "BATCH: 688/1074\n",
            " train loss item: 0.6942126750946045\n",
            " f1:0.17378489\n",
            " avg_train_loss: 0.70506916 \n",
            "BATCH: 689/1074\n",
            " train loss item: 0.6884568333625793\n",
            " f1:0.17374045\n",
            " avg_train_loss: 0.70506491 \n",
            "BATCH: 690/1074\n",
            " train loss item: 0.6942007541656494\n",
            " f1:0.17369604\n",
            " avg_train_loss: 0.70506213 \n",
            "BATCH: 691/1074\n",
            " train loss item: 0.6999195218086243\n",
            " f1:0.17365165\n",
            " avg_train_loss: 0.70506082 \n",
            "BATCH: 692/1074\n",
            " train loss item: 0.6884979605674744\n",
            " f1:0.17360729\n",
            " avg_train_loss: 0.70505659 \n",
            "BATCH: 693/1074\n",
            " train loss item: 0.7054871320724487\n",
            " f1:0.17356294\n",
            " avg_train_loss: 0.70505670 \n",
            "BATCH: 694/1074\n",
            " train loss item: 0.6997308731079102\n",
            " f1:0.17351862\n",
            " avg_train_loss: 0.70505534 \n",
            "BATCH: 695/1074\n",
            " train loss item: 0.7105833292007446\n",
            " f1:0.17347432\n",
            " avg_train_loss: 0.70505675 \n",
            "BATCH: 696/1074\n",
            " train loss item: 0.6940553188323975\n",
            " f1:0.17343004\n",
            " avg_train_loss: 0.70505394 \n",
            "BATCH: 697/1074\n",
            " train loss item: 0.6940023899078369\n",
            " f1:0.17338579\n",
            " avg_train_loss: 0.70505112 \n",
            "BATCH: 698/1074\n",
            " train loss item: 0.6789026856422424\n",
            " f1:0.17334156\n",
            " avg_train_loss: 0.70504445 \n",
            "BATCH: 699/1074\n",
            " train loss item: 0.6939312815666199\n",
            " f1:0.17329735\n",
            " avg_train_loss: 0.70504162 \n",
            "BATCH: 700/1074\n",
            " train loss item: 0.6987930536270142\n",
            " f1:0.17325317\n",
            " avg_train_loss: 0.70504002 \n",
            "BATCH: 701/1074\n",
            " train loss item: 0.6938793659210205\n",
            " f1:0.17320900\n",
            " avg_train_loss: 0.70503718 \n",
            "BATCH: 702/1074\n",
            " train loss item: 0.6985334157943726\n",
            " f1:0.17316486\n",
            " avg_train_loss: 0.70503552 \n",
            "BATCH: 703/1074\n",
            " train loss item: 0.6801584959030151\n",
            " f1:0.17312074\n",
            " avg_train_loss: 0.70502918 \n",
            "BATCH: 704/1074\n",
            " train loss item: 0.7028464078903198\n",
            " f1:0.17307665\n",
            " avg_train_loss: 0.70502863 \n",
            "BATCH: 705/1074\n",
            " train loss item: 0.6849374771118164\n",
            " f1:0.17303257\n",
            " avg_train_loss: 0.70502351 \n",
            "BATCH: 706/1074\n",
            " train loss item: 0.6762237548828125\n",
            " f1:0.17298852\n",
            " avg_train_loss: 0.70501618 \n",
            "BATCH: 707/1074\n",
            " train loss item: 0.6982824802398682\n",
            " f1:0.17294449\n",
            " avg_train_loss: 0.70501447 \n",
            "BATCH: 708/1074\n",
            " train loss item: 0.6892679929733276\n",
            " f1:0.17290049\n",
            " avg_train_loss: 0.70501046 \n",
            "BATCH: 709/1074\n",
            " train loss item: 0.6892237067222595\n",
            " f1:0.17285650\n",
            " avg_train_loss: 0.70500644 \n",
            "BATCH: 710/1074\n",
            " train loss item: 0.6798248291015625\n",
            " f1:0.17281254\n",
            " avg_train_loss: 0.70500004 \n",
            "BATCH: 711/1074\n",
            " train loss item: 0.6986917853355408\n",
            " f1:0.17276860\n",
            " avg_train_loss: 0.70499843 \n",
            "BATCH: 712/1074\n",
            " train loss item: 0.6987964510917664\n",
            " f1:0.17272469\n",
            " avg_train_loss: 0.70499686 \n",
            "BATCH: 713/1074\n",
            " train loss item: 0.6889990568161011\n",
            " f1:0.17268079\n",
            " avg_train_loss: 0.70499279 \n",
            "BATCH: 714/1074\n",
            " train loss item: 0.6839971542358398\n",
            " f1:0.17263692\n",
            " avg_train_loss: 0.70498746 \n",
            "BATCH: 715/1074\n",
            " train loss item: 0.719184160232544\n",
            " f1:0.17259307\n",
            " avg_train_loss: 0.70499106 \n",
            "BATCH: 716/1074\n",
            " train loss item: 0.6939469575881958\n",
            " f1:0.17254924\n",
            " avg_train_loss: 0.70498826 \n",
            "BATCH: 717/1074\n",
            " train loss item: 0.6790745258331299\n",
            " f1:0.17250544\n",
            " avg_train_loss: 0.70498168 \n",
            "BATCH: 718/1074\n",
            " train loss item: 0.6691184043884277\n",
            " f1:0.17246165\n",
            " avg_train_loss: 0.70497258 \n",
            "BATCH: 719/1074\n",
            " train loss item: 0.6889026761054993\n",
            " f1:0.17241789\n",
            " avg_train_loss: 0.70496850 \n",
            "BATCH: 720/1074\n",
            " train loss item: 0.6784979701042175\n",
            " f1:0.17237415\n",
            " avg_train_loss: 0.70496179 \n",
            "BATCH: 721/1074\n",
            " train loss item: 0.7046672105789185\n",
            " f1:0.17233044\n",
            " avg_train_loss: 0.70496171 \n",
            "BATCH: 722/1074\n",
            " train loss item: 0.6994653940200806\n",
            " f1:0.17228674\n",
            " avg_train_loss: 0.70496032 \n",
            "BATCH: 723/1074\n",
            " train loss item: 0.6995248794555664\n",
            " f1:0.17224307\n",
            " avg_train_loss: 0.70495894 \n",
            "BATCH: 724/1074\n",
            " train loss item: 0.6723110675811768\n",
            " f1:0.17219942\n",
            " avg_train_loss: 0.70495067 \n",
            "BATCH: 725/1074\n",
            " train loss item: 0.6775466799736023\n",
            " f1:0.17215579\n",
            " avg_train_loss: 0.70494372 \n",
            "BATCH: 726/1074\n",
            " train loss item: 0.6885190010070801\n",
            " f1:0.17211219\n",
            " avg_train_loss: 0.70493956 \n",
            "BATCH: 727/1074\n",
            " train loss item: 0.699988842010498\n",
            " f1:0.17206860\n",
            " avg_train_loss: 0.70493831 \n",
            "BATCH: 728/1074\n",
            " train loss item: 0.7001040577888489\n",
            " f1:0.17202504\n",
            " avg_train_loss: 0.70493709 \n",
            "BATCH: 729/1074\n",
            " train loss item: 0.6942615509033203\n",
            " f1:0.17198150\n",
            " avg_train_loss: 0.70493438 \n",
            "BATCH: 730/1074\n",
            " train loss item: 0.7002044916152954\n",
            " f1:0.17193798\n",
            " avg_train_loss: 0.70493319 \n",
            "BATCH: 731/1074\n",
            " train loss item: 0.7061214447021484\n",
            " f1:0.17189449\n",
            " avg_train_loss: 0.70493349 \n",
            "BATCH: 732/1074\n",
            " train loss item: 0.6825147867202759\n",
            " f1:0.17185102\n",
            " avg_train_loss: 0.70492782 \n",
            "BATCH: 733/1074\n",
            " train loss item: 0.6883920431137085\n",
            " f1:0.17180756\n",
            " avg_train_loss: 0.70492364 \n",
            "BATCH: 734/1074\n",
            " train loss item: 0.6883931756019592\n",
            " f1:0.17176413\n",
            " avg_train_loss: 0.70491946 \n",
            "BATCH: 735/1074\n",
            " train loss item: 0.688386082649231\n",
            " f1:0.17172073\n",
            " avg_train_loss: 0.70491528 \n",
            "BATCH: 736/1074\n",
            " train loss item: 0.6883717775344849\n",
            " f1:0.17167734\n",
            " avg_train_loss: 0.70491110 \n",
            "BATCH: 737/1074\n",
            " train loss item: 0.6883511543273926\n",
            " f1:0.17163398\n",
            " avg_train_loss: 0.70490692 \n",
            "BATCH: 738/1074\n",
            " train loss item: 0.6823663711547852\n",
            " f1:0.17159063\n",
            " avg_train_loss: 0.70490123 \n",
            "BATCH: 739/1074\n",
            " train loss item: 0.688279926776886\n",
            " f1:0.17154731\n",
            " avg_train_loss: 0.70489703 \n",
            "BATCH: 740/1074\n",
            " train loss item: 0.688231348991394\n",
            " f1:0.17150402\n",
            " avg_train_loss: 0.70489282 \n",
            "BATCH: 741/1074\n",
            " train loss item: 0.6757875680923462\n",
            " f1:0.17146074\n",
            " avg_train_loss: 0.70488548 \n",
            "BATCH: 742/1074\n",
            " train loss item: 0.694431483745575\n",
            " f1:0.17141749\n",
            " avg_train_loss: 0.70488284 \n",
            "BATCH: 743/1074\n",
            " train loss item: 0.662197470664978\n",
            " f1:0.17137425\n",
            " avg_train_loss: 0.70487208 \n",
            "BATCH: 744/1074\n",
            " train loss item: 0.7146905660629272\n",
            " f1:0.17133104\n",
            " avg_train_loss: 0.70487455 \n",
            "BATCH: 745/1074\n",
            " train loss item: 0.6878163814544678\n",
            " f1:0.17128785\n",
            " avg_train_loss: 0.70487025 \n",
            "BATCH: 746/1074\n",
            " train loss item: 0.6946890354156494\n",
            " f1:0.17124469\n",
            " avg_train_loss: 0.70486769 \n",
            "BATCH: 747/1074\n",
            " train loss item: 0.6876918077468872\n",
            " f1:0.17120154\n",
            " avg_train_loss: 0.70486336 \n",
            "BATCH: 748/1074\n",
            " train loss item: 0.6661701202392578\n",
            " f1:0.17115842\n",
            " avg_train_loss: 0.70485361 \n",
            "BATCH: 749/1074\n",
            " train loss item: 0.6800609230995178\n",
            " f1:0.17111531\n",
            " avg_train_loss: 0.70484737 \n",
            "BATCH: 750/1074\n",
            " train loss item: 0.6873018145561218\n",
            " f1:0.17107223\n",
            " avg_train_loss: 0.70484295 \n",
            "BATCH: 751/1074\n",
            " train loss item: 0.6871087551116943\n",
            " f1:0.17102918\n",
            " avg_train_loss: 0.70483849 \n",
            "BATCH: 752/1074\n",
            " train loss item: 0.7041294574737549\n",
            " f1:0.17098614\n",
            " avg_train_loss: 0.70483831 \n",
            "BATCH: 753/1074\n",
            " train loss item: 0.6867997646331787\n",
            " f1:0.17094312\n",
            " avg_train_loss: 0.70483377 \n",
            "BATCH: 754/1074\n",
            " train loss item: 0.6958154439926147\n",
            " f1:0.17090013\n",
            " avg_train_loss: 0.70483150 \n",
            "BATCH: 755/1074\n",
            " train loss item: 0.6772884130477905\n",
            " f1:0.17085716\n",
            " avg_train_loss: 0.70482458 \n",
            "BATCH: 756/1074\n",
            " train loss item: 0.6864568591117859\n",
            " f1:0.17081421\n",
            " avg_train_loss: 0.70481996 \n",
            "BATCH: 757/1074\n",
            " train loss item: 0.7065435647964478\n",
            " f1:0.17077128\n",
            " avg_train_loss: 0.70482039 \n",
            "BATCH: 758/1074\n",
            " train loss item: 0.7270951271057129\n",
            " f1:0.17072837\n",
            " avg_train_loss: 0.70482599 \n",
            "BATCH: 759/1074\n",
            " train loss item: 0.6961061954498291\n",
            " f1:0.17068548\n",
            " avg_train_loss: 0.70482380 \n",
            "BATCH: 760/1074\n",
            " train loss item: 0.6867048740386963\n",
            " f1:0.17064262\n",
            " avg_train_loss: 0.70481925 \n",
            "BATCH: 761/1074\n",
            " train loss item: 0.6694724559783936\n",
            " f1:0.17059978\n",
            " avg_train_loss: 0.70481037 \n",
            "BATCH: 762/1074\n",
            " train loss item: 0.7041225433349609\n",
            " f1:0.17055696\n",
            " avg_train_loss: 0.70481020 \n",
            "BATCH: 763/1074\n",
            " train loss item: 0.678584098815918\n",
            " f1:0.17051416\n",
            " avg_train_loss: 0.70480362 \n",
            "BATCH: 764/1074\n",
            " train loss item: 0.6870249509811401\n",
            " f1:0.17047138\n",
            " avg_train_loss: 0.70479916 \n",
            "BATCH: 765/1074\n",
            " train loss item: 0.6953682899475098\n",
            " f1:0.17042862\n",
            " avg_train_loss: 0.70479679 \n",
            "BATCH: 766/1074\n",
            " train loss item: 0.7036327123641968\n",
            " f1:0.17038589\n",
            " avg_train_loss: 0.70479650 \n",
            "BATCH: 767/1074\n",
            " train loss item: 0.6952811479568481\n",
            " f1:0.17034317\n",
            " avg_train_loss: 0.70479412 \n",
            "BATCH: 768/1074\n",
            " train loss item: 0.7113127112388611\n",
            " f1:0.17030048\n",
            " avg_train_loss: 0.70479575 \n",
            "BATCH: 769/1074\n",
            " train loss item: 0.7108155488967896\n",
            " f1:0.17025781\n",
            " avg_train_loss: 0.70479726 \n",
            "BATCH: 770/1074\n",
            " train loss item: 0.7178585529327393\n",
            " f1:0.17021516\n",
            " avg_train_loss: 0.70480053 \n",
            "BATCH: 771/1074\n",
            " train loss item: 0.687515914440155\n",
            " f1:0.17017253\n",
            " avg_train_loss: 0.70479620 \n",
            "BATCH: 772/1074\n",
            " train loss item: 0.6659997701644897\n",
            " f1:0.17012992\n",
            " avg_train_loss: 0.70478649 \n",
            "BATCH: 773/1074\n",
            " train loss item: 0.7019423246383667\n",
            " f1:0.17008734\n",
            " avg_train_loss: 0.70478578 \n",
            "BATCH: 774/1074\n",
            " train loss item: 0.680531919002533\n",
            " f1:0.17004477\n",
            " avg_train_loss: 0.70477971 \n",
            "BATCH: 775/1074\n",
            " train loss item: 0.6733560562133789\n",
            " f1:0.17000223\n",
            " avg_train_loss: 0.70477185 \n",
            "BATCH: 776/1074\n",
            " train loss item: 0.6803221702575684\n",
            " f1:0.16995971\n",
            " avg_train_loss: 0.70476573 \n",
            "BATCH: 777/1074\n",
            " train loss item: 0.672598659992218\n",
            " f1:0.16991721\n",
            " avg_train_loss: 0.70475769 \n",
            "BATCH: 778/1074\n",
            " train loss item: 0.7261157631874084\n",
            " f1:0.16987473\n",
            " avg_train_loss: 0.70476303 \n",
            "BATCH: 779/1074\n",
            " train loss item: 0.7107352018356323\n",
            " f1:0.16983227\n",
            " avg_train_loss: 0.70476452 \n",
            "BATCH: 780/1074\n",
            " train loss item: 0.6950511932373047\n",
            " f1:0.16978983\n",
            " avg_train_loss: 0.70476209 \n",
            "BATCH: 781/1074\n",
            " train loss item: 0.7025958299636841\n",
            " f1:0.16974742\n",
            " avg_train_loss: 0.70476155 \n",
            "BATCH: 782/1074\n",
            " train loss item: 0.6800562143325806\n",
            " f1:0.16970502\n",
            " avg_train_loss: 0.70475538 \n",
            "BATCH: 783/1074\n",
            " train loss item: 0.6728262901306152\n",
            " f1:0.16966265\n",
            " avg_train_loss: 0.70474741 \n",
            "BATCH: 784/1074\n",
            " train loss item: 0.6653329730033875\n",
            " f1:0.16962030\n",
            " avg_train_loss: 0.70473757 \n",
            "BATCH: 785/1074\n",
            " train loss item: 0.6798017621040344\n",
            " f1:0.16957797\n",
            " avg_train_loss: 0.70473135 \n",
            "BATCH: 786/1074\n",
            " train loss item: 0.7109048366546631\n",
            " f1:0.16953566\n",
            " avg_train_loss: 0.70473289 \n",
            "BATCH: 787/1074\n",
            " train loss item: 0.6792025566101074\n",
            " f1:0.16949337\n",
            " avg_train_loss: 0.70472652 \n",
            "BATCH: 788/1074\n",
            " train loss item: 0.6870939135551453\n",
            " f1:0.16945110\n",
            " avg_train_loss: 0.70472212 \n",
            "BATCH: 789/1074\n",
            " train loss item: 0.6954312324523926\n",
            " f1:0.16940885\n",
            " avg_train_loss: 0.70471980 \n",
            "BATCH: 790/1074\n",
            " train loss item: 0.7041616439819336\n",
            " f1:0.16936663\n",
            " avg_train_loss: 0.70471967 \n",
            "BATCH: 791/1074\n",
            " train loss item: 0.7041709423065186\n",
            " f1:0.16932442\n",
            " avg_train_loss: 0.70471953 \n",
            "BATCH: 792/1074\n",
            " train loss item: 0.6954511404037476\n",
            " f1:0.16928224\n",
            " avg_train_loss: 0.70471722 \n",
            "BATCH: 793/1074\n",
            " train loss item: 0.7119805216789246\n",
            " f1:0.16924008\n",
            " avg_train_loss: 0.70471903 \n",
            "BATCH: 794/1074\n",
            " train loss item: 0.6872082352638245\n",
            " f1:0.16919794\n",
            " avg_train_loss: 0.70471467 \n",
            "BATCH: 795/1074\n",
            " train loss item: 0.6564090847969055\n",
            " f1:0.16915582\n",
            " avg_train_loss: 0.70470264 \n",
            "BATCH: 796/1074\n",
            " train loss item: 0.6717867851257324\n",
            " f1:0.16911372\n",
            " avg_train_loss: 0.70469445 \n",
            "BATCH: 797/1074\n",
            " train loss item: 0.711010217666626\n",
            " f1:0.16907164\n",
            " avg_train_loss: 0.70469602 \n",
            "BATCH: 798/1074\n",
            " train loss item: 0.6792949438095093\n",
            " f1:0.16902958\n",
            " avg_train_loss: 0.70468970 \n",
            "BATCH: 799/1074\n",
            " train loss item: 0.6952059268951416\n",
            " f1:0.16898754\n",
            " avg_train_loss: 0.70468735 \n",
            "BATCH: 800/1074\n",
            " train loss item: 0.6871566772460938\n",
            " f1:0.16894553\n",
            " avg_train_loss: 0.70468299 \n",
            "BATCH: 801/1074\n",
            " train loss item: 0.7115821838378906\n",
            " f1:0.16890353\n",
            " avg_train_loss: 0.70468470 \n",
            "BATCH: 802/1074\n",
            " train loss item: 0.7194218039512634\n",
            " f1:0.16886156\n",
            " avg_train_loss: 0.70468836 \n",
            "BATCH: 803/1074\n",
            " train loss item: 0.6872949600219727\n",
            " f1:0.16881961\n",
            " avg_train_loss: 0.70468404 \n",
            "BATCH: 804/1074\n",
            " train loss item: 0.6873916983604431\n",
            " f1:0.16877767\n",
            " avg_train_loss: 0.70467975 \n",
            "BATCH: 805/1074\n",
            " train loss item: 0.6799892783164978\n",
            " f1:0.16873576\n",
            " avg_train_loss: 0.70467362 \n",
            "BATCH: 806/1074\n",
            " train loss item: 0.687493085861206\n",
            " f1:0.16869387\n",
            " avg_train_loss: 0.70466935 \n",
            "BATCH: 807/1074\n",
            " train loss item: 0.6875118017196655\n",
            " f1:0.16865200\n",
            " avg_train_loss: 0.70466509 \n",
            "BATCH: 808/1074\n",
            " train loss item: 0.731677770614624\n",
            " f1:0.16861015\n",
            " avg_train_loss: 0.70467180 \n",
            "BATCH: 809/1074\n",
            " train loss item: 0.7236957550048828\n",
            " f1:0.16856832\n",
            " avg_train_loss: 0.70467651 \n",
            "BATCH: 810/1074\n",
            " train loss item: 0.6947253942489624\n",
            " f1:0.16852652\n",
            " avg_train_loss: 0.70467405 \n",
            "BATCH: 811/1074\n",
            " train loss item: 0.6877884864807129\n",
            " f1:0.16848473\n",
            " avg_train_loss: 0.70466986 \n",
            "BATCH: 812/1074\n",
            " train loss item: 0.6675857901573181\n",
            " f1:0.16844296\n",
            " avg_train_loss: 0.70466067 \n",
            "BATCH: 813/1074\n",
            " train loss item: 0.7013247609138489\n",
            " f1:0.16840122\n",
            " avg_train_loss: 0.70465984 \n",
            "BATCH: 814/1074\n",
            " train loss item: 0.7079739570617676\n",
            " f1:0.16835949\n",
            " avg_train_loss: 0.70466066 \n",
            "BATCH: 815/1074\n",
            " train loss item: 0.6746616363525391\n",
            " f1:0.16831779\n",
            " avg_train_loss: 0.70465323 \n",
            "BATCH: 816/1074\n",
            " train loss item: 0.6945536136627197\n",
            " f1:0.16827611\n",
            " avg_train_loss: 0.70465073 \n",
            "BATCH: 817/1074\n",
            " train loss item: 0.6879228949546814\n",
            " f1:0.16823444\n",
            " avg_train_loss: 0.70464659 \n",
            "BATCH: 818/1074\n",
            " train loss item: 0.6879109740257263\n",
            " f1:0.16819280\n",
            " avg_train_loss: 0.70464245 \n",
            "BATCH: 819/1074\n",
            " train loss item: 0.6878864765167236\n",
            " f1:0.16815118\n",
            " avg_train_loss: 0.70463830 \n",
            "BATCH: 820/1074\n",
            " train loss item: 0.6810905933380127\n",
            " f1:0.16810958\n",
            " avg_train_loss: 0.70463247 \n",
            "BATCH: 821/1074\n",
            " train loss item: 0.680889904499054\n",
            " f1:0.16806800\n",
            " avg_train_loss: 0.70462660 \n",
            "BATCH: 822/1074\n",
            " train loss item: 0.6876641511917114\n",
            " f1:0.16802644\n",
            " avg_train_loss: 0.70462241 \n",
            "BATCH: 823/1074\n",
            " train loss item: 0.7021949887275696\n",
            " f1:0.16798490\n",
            " avg_train_loss: 0.70462181 \n",
            "BATCH: 824/1074\n",
            " train loss item: 0.7098510265350342\n",
            " f1:0.16794338\n",
            " avg_train_loss: 0.70462310 \n",
            "BATCH: 825/1074\n",
            " train loss item: 0.6652966141700745\n",
            " f1:0.16790188\n",
            " avg_train_loss: 0.70461338 \n",
            "BATCH: 826/1074\n",
            " train loss item: 0.7026034593582153\n",
            " f1:0.16786040\n",
            " avg_train_loss: 0.70461288 \n",
            "BATCH: 827/1074\n",
            " train loss item: 0.6873553991317749\n",
            " f1:0.16781895\n",
            " avg_train_loss: 0.70460862 \n",
            "BATCH: 828/1074\n",
            " train loss item: 0.6795138120651245\n",
            " f1:0.16777751\n",
            " avg_train_loss: 0.70460243 \n",
            "BATCH: 829/1074\n",
            " train loss item: 0.6791425943374634\n",
            " f1:0.16773609\n",
            " avg_train_loss: 0.70459614 \n",
            "BATCH: 830/1074\n",
            " train loss item: 0.7123136520385742\n",
            " f1:0.16769470\n",
            " avg_train_loss: 0.70459805 \n",
            "BATCH: 831/1074\n",
            " train loss item: 0.6869566440582275\n",
            " f1:0.16765332\n",
            " avg_train_loss: 0.70459369 \n",
            "BATCH: 832/1074\n",
            " train loss item: 0.7127526998519897\n",
            " f1:0.16761197\n",
            " avg_train_loss: 0.70459571 \n",
            "BATCH: 833/1074\n",
            " train loss item: 0.7288848161697388\n",
            " f1:0.16757063\n",
            " avg_train_loss: 0.70460170 \n",
            "BATCH: 834/1074\n",
            " train loss item: 0.6950286626815796\n",
            " f1:0.16752932\n",
            " avg_train_loss: 0.70459934 \n",
            "BATCH: 835/1074\n",
            " train loss item: 0.6591619253158569\n",
            " f1:0.16748802\n",
            " avg_train_loss: 0.70458814 \n",
            "BATCH: 836/1074\n",
            " train loss item: 0.7155357599258423\n",
            " f1:0.16744675\n",
            " avg_train_loss: 0.70459083 \n",
            "BATCH: 837/1074\n",
            " train loss item: 0.6812145709991455\n",
            " f1:0.16740550\n",
            " avg_train_loss: 0.70458507 \n",
            "BATCH: 838/1074\n",
            " train loss item: 0.6749123334884644\n",
            " f1:0.16736426\n",
            " avg_train_loss: 0.70457777 \n",
            "BATCH: 839/1074\n",
            " train loss item: 0.7074747085571289\n",
            " f1:0.16732305\n",
            " avg_train_loss: 0.70457848 \n",
            "BATCH: 840/1074\n",
            " train loss item: 0.726543664932251\n",
            " f1:0.16728186\n",
            " avg_train_loss: 0.70458389 \n",
            "BATCH: 841/1074\n",
            " train loss item: 0.7006747126579285\n",
            " f1:0.16724069\n",
            " avg_train_loss: 0.70458292 \n",
            "BATCH: 842/1074\n",
            " train loss item: 0.706636369228363\n",
            " f1:0.16719954\n",
            " avg_train_loss: 0.70458343 \n",
            "BATCH: 843/1074\n",
            " train loss item: 0.7003111243247986\n",
            " f1:0.16715840\n",
            " avg_train_loss: 0.70458238 \n",
            "BATCH: 844/1074\n",
            " train loss item: 0.7001378536224365\n",
            " f1:0.16711729\n",
            " avg_train_loss: 0.70458129 \n",
            "BATCH: 845/1074\n",
            " train loss item: 0.6826927661895752\n",
            " f1:0.16707620\n",
            " avg_train_loss: 0.70457590 \n",
            "BATCH: 846/1074\n",
            " train loss item: 0.7055559754371643\n",
            " f1:0.16703513\n",
            " avg_train_loss: 0.70457614 \n",
            "BATCH: 847/1074\n",
            " train loss item: 0.6997199654579163\n",
            " f1:0.16699408\n",
            " avg_train_loss: 0.70457495 \n",
            "BATCH: 848/1074\n",
            " train loss item: 0.6940966844558716\n",
            " f1:0.16695305\n",
            " avg_train_loss: 0.70457238 \n",
            "BATCH: 849/1074\n",
            " train loss item: 0.6834064722061157\n",
            " f1:0.16691204\n",
            " avg_train_loss: 0.70456718 \n",
            "BATCH: 850/1074\n",
            " train loss item: 0.7045480012893677\n",
            " f1:0.16687105\n",
            " avg_train_loss: 0.70456717 \n",
            "BATCH: 851/1074\n",
            " train loss item: 0.699122428894043\n",
            " f1:0.16683008\n",
            " avg_train_loss: 0.70456584 \n",
            "BATCH: 852/1074\n",
            " train loss item: 0.6939405202865601\n",
            " f1:0.16678913\n",
            " avg_train_loss: 0.70456323 \n",
            "BATCH: 853/1074\n",
            " train loss item: 0.6890618801116943\n",
            " f1:0.16674820\n",
            " avg_train_loss: 0.70455942 \n",
            "BATCH: 854/1074\n",
            " train loss item: 0.7032992839813232\n",
            " f1:0.16670729\n",
            " avg_train_loss: 0.70455911 \n",
            "BATCH: 855/1074\n",
            " train loss item: 0.7029087543487549\n",
            " f1:0.16666640\n",
            " avg_train_loss: 0.70455871 \n",
            "BATCH: 856/1074\n",
            " train loss item: 0.706730306148529\n",
            " f1:0.16662553\n",
            " avg_train_loss: 0.70455924 \n",
            "BATCH: 857/1074\n",
            " train loss item: 0.6896408796310425\n",
            " f1:0.16658468\n",
            " avg_train_loss: 0.70455558 \n",
            "BATCH: 858/1074\n",
            " train loss item: 0.6822850704193115\n",
            " f1:0.16654385\n",
            " avg_train_loss: 0.70455013 \n",
            "BATCH: 859/1074\n",
            " train loss item: 0.7081955671310425\n",
            " f1:0.16650304\n",
            " avg_train_loss: 0.70455102 \n",
            "BATCH: 860/1074\n",
            " train loss item: 0.6833775043487549\n",
            " f1:0.16646225\n",
            " avg_train_loss: 0.70454583 \n",
            "BATCH: 861/1074\n",
            " train loss item: 0.6837056875228882\n",
            " f1:0.16642148\n",
            " avg_train_loss: 0.70454073 \n",
            "BATCH: 862/1074\n",
            " train loss item: 0.683606743812561\n",
            " f1:0.16638073\n",
            " avg_train_loss: 0.70453560 \n",
            "BATCH: 863/1074\n",
            " train loss item: 0.6900885105133057\n",
            " f1:0.16634000\n",
            " avg_train_loss: 0.70453207 \n",
            "BATCH: 864/1074\n",
            " train loss item: 0.6828639507293701\n",
            " f1:0.16629929\n",
            " avg_train_loss: 0.70452676 \n",
            "BATCH: 865/1074\n",
            " train loss item: 0.7010222673416138\n",
            " f1:0.16625860\n",
            " avg_train_loss: 0.70452591 \n",
            "BATCH: 866/1074\n",
            " train loss item: 0.6936030983924866\n",
            " f1:0.16621793\n",
            " avg_train_loss: 0.70452323 \n",
            "BATCH: 867/1074\n",
            " train loss item: 0.693608820438385\n",
            " f1:0.16617728\n",
            " avg_train_loss: 0.70452056 \n",
            "BATCH: 868/1074\n",
            " train loss item: 0.6974035501480103\n",
            " f1:0.16613665\n",
            " avg_train_loss: 0.70451882 \n",
            "BATCH: 869/1074\n",
            " train loss item: 0.701082706451416\n",
            " f1:0.16609604\n",
            " avg_train_loss: 0.70451798 \n",
            "BATCH: 870/1074\n",
            " train loss item: 0.6899546384811401\n",
            " f1:0.16605545\n",
            " avg_train_loss: 0.70451443 \n",
            "BATCH: 871/1074\n",
            " train loss item: 0.6900426745414734\n",
            " f1:0.16601488\n",
            " avg_train_loss: 0.70451089 \n",
            "BATCH: 872/1074\n",
            " train loss item: 0.6866913437843323\n",
            " f1:0.16597433\n",
            " avg_train_loss: 0.70450654 \n",
            "BATCH: 873/1074\n",
            " train loss item: 0.6866718530654907\n",
            " f1:0.16593380\n",
            " avg_train_loss: 0.70450218 \n",
            "BATCH: 874/1074\n",
            " train loss item: 0.6900193095207214\n",
            " f1:0.16589329\n",
            " avg_train_loss: 0.70449865 \n",
            "BATCH: 875/1074\n",
            " train loss item: 0.6789675951004028\n",
            " f1:0.16585280\n",
            " avg_train_loss: 0.70449241 \n",
            "BATCH: 876/1074\n",
            " train loss item: 0.6897153258323669\n",
            " f1:0.16581233\n",
            " avg_train_loss: 0.70448881 \n",
            "BATCH: 877/1074\n",
            " train loss item: 0.6978884935379028\n",
            " f1:0.16577187\n",
            " avg_train_loss: 0.70448720 \n",
            "BATCH: 878/1074\n",
            " train loss item: 0.6894049048423767\n",
            " f1:0.16573144\n",
            " avg_train_loss: 0.70448352 \n",
            "BATCH: 879/1074\n",
            " train loss item: 0.6802616119384766\n",
            " f1:0.16569103\n",
            " avg_train_loss: 0.70447761 \n",
            "BATCH: 880/1074\n",
            " train loss item: 0.703453004360199\n",
            " f1:0.16565064\n",
            " avg_train_loss: 0.70447736 \n",
            "BATCH: 881/1074\n",
            " train loss item: 0.6792173385620117\n",
            " f1:0.16561026\n",
            " avg_train_loss: 0.70447121 \n",
            "BATCH: 882/1074\n",
            " train loss item: 0.6732801198959351\n",
            " f1:0.16556991\n",
            " avg_train_loss: 0.70446361 \n",
            "BATCH: 883/1074\n",
            " train loss item: 0.6826291084289551\n",
            " f1:0.16552958\n",
            " avg_train_loss: 0.70445829 \n",
            "BATCH: 884/1074\n",
            " train loss item: 0.6812465190887451\n",
            " f1:0.16548926\n",
            " avg_train_loss: 0.70445263 \n",
            "BATCH: 885/1074\n",
            " train loss item: 0.6715126037597656\n",
            " f1:0.16544897\n",
            " avg_train_loss: 0.70444461 \n",
            "BATCH: 886/1074\n",
            " train loss item: 0.6962599754333496\n",
            " f1:0.16540869\n",
            " avg_train_loss: 0.70444262 \n",
            "BATCH: 887/1074\n",
            " train loss item: 0.6975538730621338\n",
            " f1:0.16536844\n",
            " avg_train_loss: 0.70444095 \n",
            "BATCH: 888/1074\n",
            " train loss item: 0.6987415552139282\n",
            " f1:0.16532820\n",
            " avg_train_loss: 0.70443956 \n",
            "BATCH: 889/1074\n",
            " train loss item: 0.6712499856948853\n",
            " f1:0.16528799\n",
            " avg_train_loss: 0.70443149 \n",
            "BATCH: 890/1074\n",
            " train loss item: 0.6853143572807312\n",
            " f1:0.16524779\n",
            " avg_train_loss: 0.70442684 \n",
            "BATCH: 891/1074\n",
            " train loss item: 0.7200524806976318\n",
            " f1:0.16520761\n",
            " avg_train_loss: 0.70443064 \n",
            "BATCH: 892/1074\n",
            " train loss item: 0.651548445224762\n",
            " f1:0.16516746\n",
            " avg_train_loss: 0.70441778 \n",
            "BATCH: 893/1074\n",
            " train loss item: 0.7037088871002197\n",
            " f1:0.16512732\n",
            " avg_train_loss: 0.70441761 \n",
            "BATCH: 894/1074\n",
            " train loss item: 0.7038260698318481\n",
            " f1:0.16508720\n",
            " avg_train_loss: 0.70441747 \n",
            "BATCH: 895/1074\n",
            " train loss item: 0.7546597719192505\n",
            " f1:0.16504710\n",
            " avg_train_loss: 0.70442967 \n",
            "BATCH: 896/1074\n",
            " train loss item: 0.6587176322937012\n",
            " f1:0.16500702\n",
            " avg_train_loss: 0.70441857 \n",
            "BATCH: 897/1074\n",
            " train loss item: 0.6395913362503052\n",
            " f1:0.16496696\n",
            " avg_train_loss: 0.70440283 \n",
            "BATCH: 898/1074\n",
            " train loss item: 0.7095083594322205\n",
            " f1:0.16492692\n",
            " avg_train_loss: 0.70440407 \n",
            "BATCH: 899/1074\n",
            " train loss item: 0.6859033107757568\n",
            " f1:0.16488690\n",
            " avg_train_loss: 0.70439958 \n",
            "BATCH: 900/1074\n",
            " train loss item: 0.7081684470176697\n",
            " f1:0.16484690\n",
            " avg_train_loss: 0.70440049 \n",
            "BATCH: 901/1074\n",
            " train loss item: 0.6760141849517822\n",
            " f1:0.16480692\n",
            " avg_train_loss: 0.70439361 \n",
            "BATCH: 902/1074\n",
            " train loss item: 0.6664719581604004\n",
            " f1:0.16476695\n",
            " avg_train_loss: 0.70438441 \n",
            "BATCH: 903/1074\n",
            " train loss item: 0.706951916217804\n",
            " f1:0.16472701\n",
            " avg_train_loss: 0.70438504 \n",
            "BATCH: 904/1074\n",
            " train loss item: 0.696505069732666\n",
            " f1:0.16468709\n",
            " avg_train_loss: 0.70438313 \n",
            "BATCH: 905/1074\n",
            " train loss item: 0.7162373661994934\n",
            " f1:0.16464718\n",
            " avg_train_loss: 0.70438600 \n",
            "BATCH: 906/1074\n",
            " train loss item: 0.6867178678512573\n",
            " f1:0.16460729\n",
            " avg_train_loss: 0.70438172 \n",
            "BATCH: 907/1074\n",
            " train loss item: 0.7038100957870483\n",
            " f1:0.16456743\n",
            " avg_train_loss: 0.70438158 \n",
            "BATCH: 908/1074\n",
            " train loss item: 0.6950211524963379\n",
            " f1:0.16452758\n",
            " avg_train_loss: 0.70437931 \n",
            "BATCH: 909/1074\n",
            " train loss item: 0.6876973509788513\n",
            " f1:0.16448775\n",
            " avg_train_loss: 0.70437528 \n",
            "BATCH: 910/1074\n",
            " train loss item: 0.7011615633964539\n",
            " f1:0.16444795\n",
            " avg_train_loss: 0.70437450 \n",
            "BATCH: 911/1074\n",
            " train loss item: 0.6943835616111755\n",
            " f1:0.16440816\n",
            " avg_train_loss: 0.70437208 \n",
            "BATCH: 912/1074\n",
            " train loss item: 0.6646788716316223\n",
            " f1:0.16436839\n",
            " avg_train_loss: 0.70436248 \n",
            "BATCH: 913/1074\n",
            " train loss item: 0.6883565187454224\n",
            " f1:0.16432864\n",
            " avg_train_loss: 0.70435861 \n",
            "BATCH: 914/1074\n",
            " train loss item: 0.7002246975898743\n",
            " f1:0.16428891\n",
            " avg_train_loss: 0.70435761 \n",
            "BATCH: 915/1074\n",
            " train loss item: 0.6704739332199097\n",
            " f1:0.16424919\n",
            " avg_train_loss: 0.70434942 \n",
            "BATCH: 916/1074\n",
            " train loss item: 0.7004437446594238\n",
            " f1:0.16420950\n",
            " avg_train_loss: 0.70434847 \n",
            "BATCH: 917/1074\n",
            " train loss item: 0.6819736957550049\n",
            " f1:0.16416983\n",
            " avg_train_loss: 0.70434307 \n",
            "BATCH: 918/1074\n",
            " train loss item: 0.6880743503570557\n",
            " f1:0.16413017\n",
            " avg_train_loss: 0.70433914 \n",
            "BATCH: 919/1074\n",
            " train loss item: 0.7142421007156372\n",
            " f1:0.16409054\n",
            " avg_train_loss: 0.70434153 \n",
            "BATCH: 920/1074\n",
            " train loss item: 0.6814191341400146\n",
            " f1:0.16405092\n",
            " avg_train_loss: 0.70433600 \n",
            "BATCH: 921/1074\n",
            " train loss item: 0.7011779546737671\n",
            " f1:0.16401132\n",
            " avg_train_loss: 0.70433523 \n",
            "BATCH: 922/1074\n",
            " train loss item: 0.6747236251831055\n",
            " f1:0.16397175\n",
            " avg_train_loss: 0.70432809 \n",
            "BATCH: 923/1074\n",
            " train loss item: 0.6608445644378662\n",
            " f1:0.16393219\n",
            " avg_train_loss: 0.70431760 \n",
            "BATCH: 924/1074\n",
            " train loss item: 0.6948325634002686\n",
            " f1:0.16389265\n",
            " avg_train_loss: 0.70431531 \n",
            "BATCH: 925/1074\n",
            " train loss item: 0.7028080224990845\n",
            " f1:0.16385313\n",
            " avg_train_loss: 0.70431495 \n",
            "BATCH: 926/1074\n",
            " train loss item: 0.6871917247772217\n",
            " f1:0.16381362\n",
            " avg_train_loss: 0.70431082 \n",
            "BATCH: 927/1074\n",
            " train loss item: 0.7037011981010437\n",
            " f1:0.16377414\n",
            " avg_train_loss: 0.70431067 \n",
            "BATCH: 928/1074\n",
            " train loss item: 0.6953810453414917\n",
            " f1:0.16373468\n",
            " avg_train_loss: 0.70430852 \n",
            "BATCH: 929/1074\n",
            " train loss item: 0.7201933860778809\n",
            " f1:0.16369523\n",
            " avg_train_loss: 0.70431235 \n",
            "BATCH: 930/1074\n",
            " train loss item: 0.7103450298309326\n",
            " f1:0.16365581\n",
            " avg_train_loss: 0.70431380 \n",
            "BATCH: 931/1074\n",
            " train loss item: 0.6877409219741821\n",
            " f1:0.16361640\n",
            " avg_train_loss: 0.70430981 \n",
            "BATCH: 932/1074\n",
            " train loss item: 0.6879819631576538\n",
            " f1:0.16357701\n",
            " avg_train_loss: 0.70430588 \n",
            "BATCH: 933/1074\n",
            " train loss item: 0.7070115804672241\n",
            " f1:0.16353764\n",
            " avg_train_loss: 0.70430653 \n",
            "BATCH: 934/1074\n",
            " train loss item: 0.7249691486358643\n",
            " f1:0.16349829\n",
            " avg_train_loss: 0.70431150 \n",
            "BATCH: 935/1074\n",
            " train loss item: 0.7001684904098511\n",
            " f1:0.16345896\n",
            " avg_train_loss: 0.70431050 \n",
            "BATCH: 936/1074\n",
            " train loss item: 0.6884819269180298\n",
            " f1:0.16341965\n",
            " avg_train_loss: 0.70430670 \n",
            "BATCH: 937/1074\n",
            " train loss item: 0.6773479580879211\n",
            " f1:0.16338036\n",
            " avg_train_loss: 0.70430022 \n",
            "BATCH: 938/1074\n",
            " train loss item: 0.6826604604721069\n",
            " f1:0.16334109\n",
            " avg_train_loss: 0.70429501 \n",
            "BATCH: 939/1074\n",
            " train loss item: 0.7067928314208984\n",
            " f1:0.16330183\n",
            " avg_train_loss: 0.70429561 \n",
            "BATCH: 940/1074\n",
            " train loss item: 0.6754347085952759\n",
            " f1:0.16326259\n",
            " avg_train_loss: 0.70428868 \n",
            "BATCH: 941/1074\n",
            " train loss item: 0.7016066312789917\n",
            " f1:0.16322338\n",
            " avg_train_loss: 0.70428804 \n",
            "BATCH: 942/1074\n",
            " train loss item: 0.7020416259765625\n",
            " f1:0.16318418\n",
            " avg_train_loss: 0.70428750 \n",
            "BATCH: 943/1074\n",
            " train loss item: 0.6589653491973877\n",
            " f1:0.16314500\n",
            " avg_train_loss: 0.70427661 \n",
            "BATCH: 944/1074\n",
            " train loss item: 0.6953407526016235\n",
            " f1:0.16310584\n",
            " avg_train_loss: 0.70427447 \n",
            "BATCH: 945/1074\n",
            " train loss item: 0.6866340637207031\n",
            " f1:0.16306669\n",
            " avg_train_loss: 0.70427024 \n",
            "BATCH: 946/1074\n",
            " train loss item: 0.6965856552124023\n",
            " f1:0.16302757\n",
            " avg_train_loss: 0.70426839 \n",
            "BATCH: 947/1074\n",
            " train loss item: 0.685972273349762\n",
            " f1:0.16298847\n",
            " avg_train_loss: 0.70426400 \n",
            "BATCH: 948/1074\n",
            " train loss item: 0.6616657376289368\n",
            " f1:0.16294938\n",
            " avg_train_loss: 0.70425379 \n",
            "BATCH: 949/1074\n",
            " train loss item: 0.6710103750228882\n",
            " f1:0.16291031\n",
            " avg_train_loss: 0.70424582 \n",
            "BATCH: 950/1074\n",
            " train loss item: 0.6499998569488525\n",
            " f1:0.16287126\n",
            " avg_train_loss: 0.70423282 \n",
            "BATCH: 951/1074\n",
            " train loss item: 0.6872502565383911\n",
            " f1:0.16283223\n",
            " avg_train_loss: 0.70422875 \n",
            "BATCH: 952/1074\n",
            " train loss item: 0.7196451425552368\n",
            " f1:0.16279322\n",
            " avg_train_loss: 0.70423244 \n",
            "BATCH: 953/1074\n",
            " train loss item: 0.7218305468559265\n",
            " f1:0.16275423\n",
            " avg_train_loss: 0.70423665 \n",
            "BATCH: 954/1074\n",
            " train loss item: 0.662251353263855\n",
            " f1:0.16271526\n",
            " avg_train_loss: 0.70422660 \n",
            "BATCH: 955/1074\n",
            " train loss item: 0.6629284620285034\n",
            " f1:0.16267630\n",
            " avg_train_loss: 0.70421671 \n",
            "BATCH: 956/1074\n",
            " train loss item: 0.5907230377197266\n",
            " f1:0.16263737\n",
            " avg_train_loss: 0.70418955 \n",
            "BATCH: 957/1074\n",
            " train loss item: 0.6907962560653687\n",
            " f1:0.16259845\n",
            " avg_train_loss: 0.70418634 \n",
            "BATCH: 958/1074\n",
            " train loss item: 0.7895269393920898\n",
            " f1:0.16255955\n",
            " avg_train_loss: 0.70420676 \n",
            "BATCH: 959/1074\n",
            " train loss item: 0.7689388990402222\n",
            " f1:0.16252067\n",
            " avg_train_loss: 0.70422224 \n",
            "BATCH: 960/1074\n",
            " train loss item: 0.6854333281517029\n",
            " f1:0.16248181\n",
            " avg_train_loss: 0.70421775 \n",
            "BATCH: 961/1074\n",
            " train loss item: 0.6508141756057739\n",
            " f1:0.16244296\n",
            " avg_train_loss: 0.70420498 \n",
            "BATCH: 962/1074\n",
            " train loss item: 0.668290376663208\n",
            " f1:0.16240414\n",
            " avg_train_loss: 0.70419640 \n",
            "BATCH: 963/1074\n",
            " train loss item: 0.6870982646942139\n",
            " f1:0.16236533\n",
            " avg_train_loss: 0.70419231 \n",
            "BATCH: 964/1074\n",
            " train loss item: 0.6719410419464111\n",
            " f1:0.16232654\n",
            " avg_train_loss: 0.70418461 \n",
            "BATCH: 965/1074\n",
            " train loss item: 0.7030284404754639\n",
            " f1:0.16228777\n",
            " avg_train_loss: 0.70418433 \n",
            "BATCH: 966/1074\n",
            " train loss item: 0.6793013215065002\n",
            " f1:0.16224902\n",
            " avg_train_loss: 0.70417839 \n",
            "BATCH: 967/1074\n",
            " train loss item: 0.7037161588668823\n",
            " f1:0.16221029\n",
            " avg_train_loss: 0.70417828 \n",
            "BATCH: 968/1074\n",
            " train loss item: 0.7039349675178528\n",
            " f1:0.16217158\n",
            " avg_train_loss: 0.70417822 \n",
            "BATCH: 969/1074\n",
            " train loss item: 0.6787112355232239\n",
            " f1:0.16213288\n",
            " avg_train_loss: 0.70417215 \n",
            "BATCH: 970/1074\n",
            " train loss item: 0.6698625683784485\n",
            " f1:0.16209421\n",
            " avg_train_loss: 0.70416396 \n",
            "BATCH: 971/1074\n",
            " train loss item: 0.6678008437156677\n",
            " f1:0.16205555\n",
            " avg_train_loss: 0.70415529 \n",
            "BATCH: 972/1074\n",
            " train loss item: 0.6859816312789917\n",
            " f1:0.16201691\n",
            " avg_train_loss: 0.70415096 \n",
            "BATCH: 973/1074\n",
            " train loss item: 0.7379728555679321\n",
            " f1:0.16197829\n",
            " avg_train_loss: 0.70415902 \n",
            "BATCH: 974/1074\n",
            " train loss item: 0.7363753318786621\n",
            " f1:0.16193968\n",
            " avg_train_loss: 0.70416670 \n",
            "BATCH: 975/1074\n",
            " train loss item: 0.7070262432098389\n",
            " f1:0.16190110\n",
            " avg_train_loss: 0.70416738 \n",
            "BATCH: 976/1074\n",
            " train loss item: 0.6787493824958801\n",
            " f1:0.16186253\n",
            " avg_train_loss: 0.70416132 \n",
            "BATCH: 977/1074\n",
            " train loss item: 0.6947991251945496\n",
            " f1:0.16182399\n",
            " avg_train_loss: 0.70415909 \n",
            "BATCH: 978/1074\n",
            " train loss item: 0.6944807767868042\n",
            " f1:0.16178546\n",
            " avg_train_loss: 0.70415679 \n",
            "BATCH: 979/1074\n",
            " train loss item: 0.7124696969985962\n",
            " f1:0.16174694\n",
            " avg_train_loss: 0.70415877 \n",
            "BATCH: 980/1074\n",
            " train loss item: 0.6826903820037842\n",
            " f1:0.16170845\n",
            " avg_train_loss: 0.70415366 \n",
            "BATCH: 981/1074\n",
            " train loss item: 0.7174026370048523\n",
            " f1:0.16166998\n",
            " avg_train_loss: 0.70415681 \n",
            "BATCH: 982/1074\n",
            " train loss item: 0.7054082155227661\n",
            " f1:0.16163152\n",
            " avg_train_loss: 0.70415711 \n",
            "BATCH: 983/1074\n",
            " train loss item: 0.6835702657699585\n",
            " f1:0.16159308\n",
            " avg_train_loss: 0.70415221 \n",
            "BATCH: 984/1074\n",
            " train loss item: 0.6990903615951538\n",
            " f1:0.16155466\n",
            " avg_train_loss: 0.70415101 \n",
            "BATCH: 985/1074\n",
            " train loss item: 0.6938705444335938\n",
            " f1:0.16151626\n",
            " avg_train_loss: 0.70414857 \n",
            "BATCH: 986/1074\n",
            " train loss item: 0.6850839853286743\n",
            " f1:0.16147788\n",
            " avg_train_loss: 0.70414404 \n",
            "BATCH: 987/1074\n",
            " train loss item: 0.7026873826980591\n",
            " f1:0.16143951\n",
            " avg_train_loss: 0.70414369 \n",
            "BATCH: 988/1074\n",
            " train loss item: 0.6936105489730835\n",
            " f1:0.16140117\n",
            " avg_train_loss: 0.70414119 \n",
            "BATCH: 989/1074\n",
            " train loss item: 0.6873190402984619\n",
            " f1:0.16136284\n",
            " avg_train_loss: 0.70413719 \n",
            "BATCH: 990/1074\n",
            " train loss item: 0.6966557502746582\n",
            " f1:0.16132453\n",
            " avg_train_loss: 0.70413542 \n",
            "BATCH: 991/1074\n",
            " train loss item: 0.6961067914962769\n",
            " f1:0.16128624\n",
            " avg_train_loss: 0.70413351 \n",
            "BATCH: 992/1074\n",
            " train loss item: 0.6915914416313171\n",
            " f1:0.16124796\n",
            " avg_train_loss: 0.70413053 \n",
            "BATCH: 993/1074\n",
            " train loss item: 0.6943449378013611\n",
            " f1:0.16120971\n",
            " avg_train_loss: 0.70412821 \n",
            "BATCH: 994/1074\n",
            " train loss item: 0.693225085735321\n",
            " f1:0.16128442\n",
            " avg_train_loss: 0.70412563 \n",
            "BATCH: 995/1074\n",
            " train loss item: 0.6921308636665344\n",
            " f1:0.16124617\n",
            " avg_train_loss: 0.70412278 \n",
            "BATCH: 996/1074\n",
            " train loss item: 0.7036740779876709\n",
            " f1:0.16120794\n",
            " avg_train_loss: 0.70412268 \n",
            "BATCH: 997/1074\n",
            " train loss item: 0.6917518973350525\n",
            " f1:0.16116973\n",
            " avg_train_loss: 0.70411974 \n",
            "BATCH: 998/1074\n",
            " train loss item: 0.6931799054145813\n",
            " f1:0.16113154\n",
            " avg_train_loss: 0.70411715 \n",
            "BATCH: 999/1074\n",
            " train loss item: 0.6948386430740356\n",
            " f1:0.16109337\n",
            " avg_train_loss: 0.70411495 \n",
            "BATCH: 1000/1074\n",
            " train loss item: 0.6873401999473572\n",
            " f1:0.16124820\n",
            " avg_train_loss: 0.70411098 \n",
            "BATCH: 1001/1074\n",
            " train loss item: 0.6804602146148682\n",
            " f1:0.16139217\n",
            " avg_train_loss: 0.70410538 \n",
            "BATCH: 1002/1074\n",
            " train loss item: 0.685336709022522\n",
            " f1:0.16152442\n",
            " avg_train_loss: 0.70410094 \n",
            "BATCH: 1003/1074\n",
            " train loss item: 0.7335550785064697\n",
            " f1:0.16163026\n",
            " avg_train_loss: 0.70410791 \n",
            "BATCH: 1004/1074\n",
            " train loss item: 0.786449134349823\n",
            " f1:0.16170469\n",
            " avg_train_loss: 0.70412739 \n",
            "BATCH: 1005/1074\n",
            " train loss item: 0.7004343271255493\n",
            " f1:0.16182415\n",
            " avg_train_loss: 0.70412652 \n",
            "BATCH: 1006/1074\n",
            " train loss item: 0.694675862789154\n",
            " f1:0.16194356\n",
            " avg_train_loss: 0.70412428 \n",
            "BATCH: 1007/1074\n",
            " train loss item: 0.6942959427833557\n",
            " f1:0.16204920\n",
            " avg_train_loss: 0.70412196 \n",
            "BATCH: 1008/1074\n",
            " train loss item: 0.6962190270423889\n",
            " f1:0.16201089\n",
            " avg_train_loss: 0.70412009 \n",
            "BATCH: 1009/1074\n",
            " train loss item: 0.6983491778373718\n",
            " f1:0.16197260\n",
            " avg_train_loss: 0.70411873 \n",
            "BATCH: 1010/1074\n",
            " train loss item: 0.6938884258270264\n",
            " f1:0.16193433\n",
            " avg_train_loss: 0.70411631 \n",
            "BATCH: 1011/1074\n",
            " train loss item: 0.702055811882019\n",
            " f1:0.16189607\n",
            " avg_train_loss: 0.70411582 \n",
            "BATCH: 1012/1074\n",
            " train loss item: 0.700279951095581\n",
            " f1:0.16185783\n",
            " avg_train_loss: 0.70411492 \n",
            "BATCH: 1013/1074\n",
            " train loss item: 0.6974773406982422\n",
            " f1:0.16193206\n",
            " avg_train_loss: 0.70411335 \n",
            "BATCH: 1014/1074\n",
            " train loss item: 0.6899755001068115\n",
            " f1:0.16206380\n",
            " avg_train_loss: 0.70411001 \n",
            "BATCH: 1015/1074\n",
            " train loss item: 0.6816298365592957\n",
            " f1:0.16220710\n",
            " avg_train_loss: 0.70410471 \n",
            "BATCH: 1016/1074\n",
            " train loss item: 0.7080599069595337\n",
            " f1:0.16231245\n",
            " avg_train_loss: 0.70410564 \n",
            "BATCH: 1017/1074\n",
            " train loss item: 0.6457182168960571\n",
            " f1:0.16247637\n",
            " avg_train_loss: 0.70409187 \n",
            "BATCH: 1018/1074\n",
            " train loss item: 0.7273349761962891\n",
            " f1:0.16258161\n",
            " avg_train_loss: 0.70409735 \n",
            "BATCH: 1019/1074\n",
            " train loss item: 0.6640135049819946\n",
            " f1:0.16272465\n",
            " avg_train_loss: 0.70408790 \n",
            "BATCH: 1020/1074\n",
            " train loss item: 0.6624887585639954\n",
            " f1:0.16286763\n",
            " avg_train_loss: 0.70407809 \n",
            "BATCH: 1021/1074\n",
            " train loss item: 0.6921869516372681\n",
            " f1:0.16299894\n",
            " avg_train_loss: 0.70407529 \n",
            "BATCH: 1022/1074\n",
            " train loss item: 0.7250428199768066\n",
            " f1:0.16311761\n",
            " avg_train_loss: 0.70408023 \n",
            "BATCH: 1023/1074\n",
            " train loss item: 0.7704595923423767\n",
            " f1:0.16320768\n",
            " avg_train_loss: 0.70409586 \n",
            "BATCH: 1024/1074\n",
            " train loss item: 0.6853158473968506\n",
            " f1:0.16333881\n",
            " avg_train_loss: 0.70409144 \n",
            "BATCH: 1025/1074\n",
            " train loss item: 0.6949807405471802\n",
            " f1:0.16345733\n",
            " avg_train_loss: 0.70408930 \n",
            "BATCH: 1026/1074\n",
            " train loss item: 0.6912227869033813\n",
            " f1:0.16358834\n",
            " avg_train_loss: 0.70408627 \n",
            "BATCH: 1027/1074\n",
            " train loss item: 0.6931775808334351\n",
            " f1:0.16354984\n",
            " avg_train_loss: 0.70408370 \n",
            "BATCH: 1028/1074\n",
            " train loss item: 0.6907527446746826\n",
            " f1:0.16351136\n",
            " avg_train_loss: 0.70408056 \n",
            "BATCH: 1029/1074\n",
            " train loss item: 0.6868084073066711\n",
            " f1:0.16347289\n",
            " avg_train_loss: 0.70407650 \n",
            "BATCH: 1030/1074\n",
            " train loss item: 0.6935497522354126\n",
            " f1:0.16343445\n",
            " avg_train_loss: 0.70407402 \n",
            "BATCH: 1031/1074\n",
            " train loss item: 0.6903012990951538\n",
            " f1:0.16339602\n",
            " avg_train_loss: 0.70407079 \n",
            "BATCH: 1032/1074\n",
            " train loss item: 0.6881346106529236\n",
            " f1:0.16335761\n",
            " avg_train_loss: 0.70406704 \n",
            "BATCH: 1033/1074\n",
            " train loss item: 0.6908649802207947\n",
            " f1:0.16331922\n",
            " avg_train_loss: 0.70406394 \n",
            "BATCH: 1034/1074\n",
            " train loss item: 0.6986046433448792\n",
            " f1:0.16328084\n",
            " avg_train_loss: 0.70406265 \n",
            "BATCH: 1035/1074\n",
            " train loss item: 0.6833099126815796\n",
            " f1:0.16324249\n",
            " avg_train_loss: 0.70405778 \n",
            "BATCH: 1036/1074\n",
            " train loss item: 0.6856342554092407\n",
            " f1:0.16320415\n",
            " avg_train_loss: 0.70405345 \n",
            "BATCH: 1037/1074\n",
            " train loss item: 0.6875284910202026\n",
            " f1:0.16316583\n",
            " avg_train_loss: 0.70404957 \n",
            "BATCH: 1038/1074\n",
            " train loss item: 0.7298029661178589\n",
            " f1:0.16312753\n",
            " avg_train_loss: 0.70405562 \n",
            "BATCH: 1039/1074\n",
            " train loss item: 0.6970719695091248\n",
            " f1:0.16308924\n",
            " avg_train_loss: 0.70405398 \n",
            "BATCH: 1040/1074\n",
            " train loss item: 0.6967122554779053\n",
            " f1:0.16305098\n",
            " avg_train_loss: 0.70405226 \n",
            "BATCH: 1041/1074\n",
            " train loss item: 0.6482865810394287\n",
            " f1:0.16301273\n",
            " avg_train_loss: 0.70403918 \n",
            "BATCH: 1042/1074\n",
            " train loss item: 0.7093613147735596\n",
            " f1:0.16297450\n",
            " avg_train_loss: 0.70404042 \n",
            "BATCH: 1043/1074\n",
            " train loss item: 0.6856645941734314\n",
            " f1:0.16293629\n",
            " avg_train_loss: 0.70403611 \n",
            "BATCH: 1044/1074\n",
            " train loss item: 0.6855325102806091\n",
            " f1:0.16289809\n",
            " avg_train_loss: 0.70403178 \n",
            "BATCH: 1045/1074\n",
            " train loss item: 0.6577111482620239\n",
            " f1:0.16285992\n",
            " avg_train_loss: 0.70402092 \n",
            "BATCH: 1046/1074\n",
            " train loss item: 0.6517876386642456\n",
            " f1:0.16282176\n",
            " avg_train_loss: 0.70400868 \n",
            "BATCH: 1047/1074\n",
            " train loss item: 0.6866735219955444\n",
            " f1:0.16278362\n",
            " avg_train_loss: 0.70400462 \n",
            "BATCH: 1048/1074\n",
            " train loss item: 0.6894420385360718\n",
            " f1:0.16274550\n",
            " avg_train_loss: 0.70400121 \n",
            "BATCH: 1049/1074\n",
            " train loss item: 0.63116455078125\n",
            " f1:0.16270739\n",
            " avg_train_loss: 0.70398416 \n",
            "BATCH: 1050/1074\n",
            " train loss item: 0.734767735004425\n",
            " f1:0.16266930\n",
            " avg_train_loss: 0.70399136 \n",
            "BATCH: 1051/1074\n",
            " train loss item: 0.6972401142120361\n",
            " f1:0.16263124\n",
            " avg_train_loss: 0.70398978 \n",
            "BATCH: 1052/1074\n",
            " train loss item: 0.6928932666778564\n",
            " f1:0.16259318\n",
            " avg_train_loss: 0.70398719 \n",
            "BATCH: 1053/1074\n",
            " train loss item: 0.7403627038002014\n",
            " f1:0.16255515\n",
            " avg_train_loss: 0.70399570 \n",
            "BATCH: 1054/1074\n",
            " train loss item: 0.6504377126693726\n",
            " f1:0.16251714\n",
            " avg_train_loss: 0.70398317 \n",
            "BATCH: 1055/1074\n",
            " train loss item: 0.7123262882232666\n",
            " f1:0.16247914\n",
            " avg_train_loss: 0.70398512 \n",
            "BATCH: 1056/1074\n",
            " train loss item: 0.6593402624130249\n",
            " f1:0.16244116\n",
            " avg_train_loss: 0.70397469 \n",
            "BATCH: 1057/1074\n",
            " train loss item: 0.7029228210449219\n",
            " f1:0.16240319\n",
            " avg_train_loss: 0.70397444 \n",
            "BATCH: 1058/1074\n",
            " train loss item: 0.69443678855896\n",
            " f1:0.16236525\n",
            " avg_train_loss: 0.70397221 \n",
            "BATCH: 1059/1074\n",
            " train loss item: 0.6730388402938843\n",
            " f1:0.16232732\n",
            " avg_train_loss: 0.70396499 \n",
            "BATCH: 1060/1074\n",
            " train loss item: 0.6996173858642578\n",
            " f1:0.16228941\n",
            " avg_train_loss: 0.70396397 \n",
            "BATCH: 1061/1074\n",
            " train loss item: 0.6885955333709717\n",
            " f1:0.16225152\n",
            " avg_train_loss: 0.70396038 \n",
            "BATCH: 1062/1074\n",
            " train loss item: 0.6942257285118103\n",
            " f1:0.16221365\n",
            " avg_train_loss: 0.70395811 \n",
            "BATCH: 1063/1074\n",
            " train loss item: 0.700383722782135\n",
            " f1:0.16217579\n",
            " avg_train_loss: 0.70395728 \n",
            "BATCH: 1064/1074\n",
            " train loss item: 0.6822034120559692\n",
            " f1:0.16213795\n",
            " avg_train_loss: 0.70395220 \n",
            "BATCH: 1065/1074\n",
            " train loss item: 0.7074182033538818\n",
            " f1:0.16210013\n",
            " avg_train_loss: 0.70395301 \n",
            "BATCH: 1066/1074\n",
            " train loss item: 0.7070434093475342\n",
            " f1:0.16206233\n",
            " avg_train_loss: 0.70395373 \n",
            "BATCH: 1067/1074\n",
            " train loss item: 0.688534677028656\n",
            " f1:0.16202454\n",
            " avg_train_loss: 0.70395014 \n",
            "BATCH: 1068/1074\n",
            " train loss item: 0.6940039396286011\n",
            " f1:0.16198678\n",
            " avg_train_loss: 0.70394782 \n",
            "BATCH: 1069/1074\n",
            " train loss item: 0.68914794921875\n",
            " f1:0.16194903\n",
            " avg_train_loss: 0.70394437 \n",
            "BATCH: 1070/1074\n",
            " train loss item: 0.6848499774932861\n",
            " f1:0.16191129\n",
            " avg_train_loss: 0.70393992 \n",
            "BATCH: 1071/1074\n",
            " train loss item: 0.6891683340072632\n",
            " f1:0.16187358\n",
            " avg_train_loss: 0.70393648 \n",
            "BATCH: 1072/1074\n",
            " train loss item: 0.714168906211853\n",
            " f1:0.16183588\n",
            " avg_train_loss: 0.70393886 \n",
            "BATCH: 1073/1074\n",
            " train loss item: 0.6980984210968018\n",
            " f1:0.16179820\n",
            " avg_train_loss: 0.70393750 \n",
            "BATCH: 1074/1074\n",
            " train loss item: 0.6844395399093628\n",
            " f1:0.16176054\n",
            " avg_train_loss: 0.70393296 \n",
            "\n",
            "\n",
            "\n",
            "VALIDATION FOR EPOCH 4\n",
            "BATCH: 1/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277597 \n",
            "BATCH: 2/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278847 \n",
            "BATCH: 3/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278318 \n",
            "BATCH: 4/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278678 \n",
            "BATCH: 5/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279037 \n",
            "BATCH: 6/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279101 \n",
            "BATCH: 7/358\n",
            "val loss item: 0.6902827620506287\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278869 \n",
            "BATCH: 8/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278342 \n",
            "BATCH: 9/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278111 \n",
            "BATCH: 10/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277881 \n",
            "BATCH: 11/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277357 \n",
            "BATCH: 12/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277715 \n",
            "BATCH: 13/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277192 \n",
            "BATCH: 14/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276963 \n",
            "BATCH: 15/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276735 \n",
            "BATCH: 16/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275922 \n",
            "BATCH: 17/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276279 \n",
            "BATCH: 18/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277221 \n",
            "BATCH: 19/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277577 \n",
            "BATCH: 20/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278515 \n",
            "BATCH: 21/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278578 \n",
            "BATCH: 22/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278932 \n",
            "BATCH: 23/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278994 \n",
            "BATCH: 24/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279928 \n",
            "BATCH: 25/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69280569 \n",
            "BATCH: 26/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69280050 \n",
            "BATCH: 27/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69280691 \n",
            "BATCH: 28/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279883 \n",
            "BATCH: 29/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279365 \n",
            "BATCH: 30/358\n",
            "val loss item: 0.6743330955505371\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277693 \n",
            "BATCH: 31/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278045 \n",
            "BATCH: 32/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277531 \n",
            "BATCH: 33/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277305 \n",
            "BATCH: 34/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276793 \n",
            "BATCH: 35/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277432 \n",
            "BATCH: 36/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278357 \n",
            "BATCH: 37/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278419 \n",
            "BATCH: 38/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277907 \n",
            "BATCH: 39/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277969 \n",
            "BATCH: 40/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276886 \n",
            "BATCH: 41/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277521 \n",
            "BATCH: 42/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277298 \n",
            "BATCH: 43/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277646 \n",
            "BATCH: 44/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277423 \n",
            "BATCH: 45/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276916 \n",
            "BATCH: 46/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277548 \n",
            "BATCH: 47/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278464 \n",
            "BATCH: 48/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278809 \n",
            "BATCH: 49/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278870 \n",
            "BATCH: 50/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278931 \n",
            "BATCH: 51/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278425 \n",
            "BATCH: 52/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277919 \n",
            "BATCH: 53/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277981 \n",
            "BATCH: 54/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278325 \n",
            "BATCH: 55/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277821 \n",
            "BATCH: 56/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277600 \n",
            "BATCH: 57/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277944 \n",
            "BATCH: 58/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278005 \n",
            "BATCH: 59/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277503 \n",
            "BATCH: 60/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278409 \n",
            "BATCH: 61/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278188 \n",
            "BATCH: 62/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277688 \n",
            "BATCH: 63/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276907 \n",
            "BATCH: 64/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276969 \n",
            "BATCH: 65/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276191 \n",
            "BATCH: 66/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276812 \n",
            "BATCH: 67/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276595 \n",
            "BATCH: 68/358\n",
            "val loss item: 0.6775230169296265\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275260 \n",
            "BATCH: 69/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275323 \n",
            "BATCH: 70/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275943 \n",
            "BATCH: 71/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275727 \n",
            "BATCH: 72/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275511 \n",
            "BATCH: 73/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276686 \n",
            "BATCH: 74/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277026 \n",
            "BATCH: 75/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277364 \n",
            "BATCH: 76/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276870 \n",
            "BATCH: 77/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276377 \n",
            "BATCH: 78/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276993 \n",
            "BATCH: 79/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277054 \n",
            "BATCH: 80/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277667 \n",
            "BATCH: 81/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278004 \n",
            "BATCH: 82/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278064 \n",
            "BATCH: 83/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278123 \n",
            "BATCH: 84/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278734 \n",
            "BATCH: 85/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278518 \n",
            "BATCH: 86/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278027 \n",
            "BATCH: 87/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278362 \n",
            "BATCH: 88/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278421 \n",
            "BATCH: 89/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278480 \n",
            "BATCH: 90/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278813 \n",
            "BATCH: 91/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279420 \n",
            "BATCH: 92/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278384 \n",
            "BATCH: 93/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278443 \n",
            "BATCH: 94/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278229 \n",
            "BATCH: 95/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278015 \n",
            "BATCH: 96/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277256 \n",
            "BATCH: 97/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277043 \n",
            "BATCH: 98/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276559 \n",
            "BATCH: 99/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275531 \n",
            "BATCH: 100/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275864 \n",
            "BATCH: 101/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275654 \n",
            "BATCH: 102/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275714 \n",
            "BATCH: 103/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275775 \n",
            "BATCH: 104/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276107 \n",
            "BATCH: 105/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275626 \n",
            "BATCH: 106/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275957 \n",
            "BATCH: 107/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276287 \n",
            "BATCH: 108/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276617 \n",
            "BATCH: 109/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277216 \n",
            "BATCH: 110/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276467 \n",
            "BATCH: 111/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276258 \n",
            "BATCH: 112/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275511 \n",
            "BATCH: 113/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275303 \n",
            "BATCH: 114/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275363 \n",
            "BATCH: 115/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275155 \n",
            "BATCH: 116/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274144 \n",
            "BATCH: 117/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274205 \n",
            "BATCH: 118/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273999 \n",
            "BATCH: 119/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274595 \n",
            "BATCH: 120/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274656 \n",
            "BATCH: 121/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274450 \n",
            "BATCH: 122/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274777 \n",
            "BATCH: 123/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275371 \n",
            "BATCH: 124/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275697 \n",
            "BATCH: 125/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275491 \n",
            "BATCH: 126/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275551 \n",
            "BATCH: 127/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274814 \n",
            "BATCH: 128/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275405 \n",
            "BATCH: 129/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274669 \n",
            "BATCH: 130/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274199 \n",
            "BATCH: 131/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275054 \n",
            "BATCH: 132/358\n",
            "val loss item: 0.6902827620506287\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274849 \n",
            "BATCH: 133/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275438 \n",
            "BATCH: 134/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274705 \n",
            "BATCH: 135/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274765 \n",
            "BATCH: 136/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274825 \n",
            "BATCH: 137/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275675 \n",
            "BATCH: 138/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275471 \n",
            "BATCH: 139/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275004 \n",
            "BATCH: 140/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274801 \n",
            "BATCH: 141/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275123 \n",
            "BATCH: 142/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275970 \n",
            "BATCH: 143/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276028 \n",
            "BATCH: 144/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276873 \n",
            "BATCH: 145/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276930 \n",
            "BATCH: 146/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276726 \n",
            "BATCH: 147/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276262 \n",
            "BATCH: 148/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276581 \n",
            "BATCH: 149/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276639 \n",
            "BATCH: 150/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277218 \n",
            "BATCH: 151/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277535 \n",
            "BATCH: 152/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277072 \n",
            "BATCH: 153/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277909 \n",
            "BATCH: 154/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276926 \n",
            "BATCH: 155/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275945 \n",
            "BATCH: 156/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274966 \n",
            "BATCH: 157/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275025 \n",
            "BATCH: 158/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275083 \n",
            "BATCH: 159/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274883 \n",
            "BATCH: 160/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275200 \n",
            "BATCH: 161/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275517 \n",
            "BATCH: 162/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274801 \n",
            "BATCH: 163/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275117 \n",
            "BATCH: 164/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275175 \n",
            "BATCH: 165/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274719 \n",
            "BATCH: 166/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274777 \n",
            "BATCH: 167/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275093 \n",
            "BATCH: 168/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275921 \n",
            "BATCH: 169/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275722 \n",
            "BATCH: 170/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275523 \n",
            "BATCH: 171/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275068 \n",
            "BATCH: 172/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274614 \n",
            "BATCH: 173/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275440 \n",
            "BATCH: 174/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274731 \n",
            "BATCH: 175/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275300 \n",
            "BATCH: 176/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274336 \n",
            "BATCH: 177/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274140 \n",
            "BATCH: 178/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274708 \n",
            "BATCH: 179/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275020 \n",
            "BATCH: 180/358\n",
            "val loss item: 0.6775230765342712\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273806 \n",
            "BATCH: 181/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273610 \n",
            "BATCH: 182/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69272907 \n",
            "BATCH: 183/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273220 \n",
            "BATCH: 184/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274040 \n",
            "BATCH: 185/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273084 \n",
            "BATCH: 186/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273650 \n",
            "BATCH: 187/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273708 \n",
            "BATCH: 188/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273766 \n",
            "BATCH: 189/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273824 \n",
            "BATCH: 190/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274135 \n",
            "BATCH: 191/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274445 \n",
            "BATCH: 192/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274250 \n",
            "BATCH: 193/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275063 \n",
            "BATCH: 194/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275120 \n",
            "BATCH: 195/358\n",
            "val loss item: 0.7094223499298096\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276434 \n",
            "BATCH: 196/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276490 \n",
            "BATCH: 197/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276295 \n",
            "BATCH: 198/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275849 \n",
            "BATCH: 199/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276406 \n",
            "BATCH: 200/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276211 \n",
            "BATCH: 201/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276017 \n",
            "BATCH: 202/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276573 \n",
            "BATCH: 203/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277128 \n",
            "BATCH: 204/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276683 \n",
            "BATCH: 205/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276988 \n",
            "BATCH: 206/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277292 \n",
            "BATCH: 207/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277845 \n",
            "BATCH: 208/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278148 \n",
            "BATCH: 209/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278202 \n",
            "BATCH: 210/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69279001 \n",
            "BATCH: 211/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69278557 \n",
            "BATCH: 212/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277867 \n",
            "BATCH: 213/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277425 \n",
            "BATCH: 214/358\n",
            "val loss item: 0.6870928406715393\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276984 \n",
            "BATCH: 215/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276543 \n",
            "BATCH: 216/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276598 \n",
            "BATCH: 217/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275912 \n",
            "BATCH: 218/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275720 \n",
            "BATCH: 219/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275775 \n",
            "BATCH: 220/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275831 \n",
            "BATCH: 221/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276378 \n",
            "BATCH: 222/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276925 \n",
            "BATCH: 223/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276488 \n",
            "BATCH: 224/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277279 \n",
            "BATCH: 225/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69277333 \n",
            "BATCH: 226/358\n",
            "val loss item: 0.6807129383087158\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276406 \n",
            "BATCH: 227/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275479 \n",
            "BATCH: 228/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275779 \n",
            "BATCH: 229/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275834 \n",
            "BATCH: 230/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276134 \n",
            "BATCH: 231/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276188 \n",
            "BATCH: 232/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276487 \n",
            "BATCH: 233/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275809 \n",
            "BATCH: 234/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276351 \n",
            "BATCH: 235/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276405 \n",
            "BATCH: 236/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276216 \n",
            "BATCH: 237/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276270 \n",
            "BATCH: 238/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69276324 \n",
            "BATCH: 239/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275893 \n",
            "BATCH: 240/358\n",
            "val loss item: 0.6775230169296265\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274733 \n",
            "BATCH: 241/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69275031 \n",
            "BATCH: 242/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274601 \n",
            "BATCH: 243/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69274656 \n",
            "BATCH: 244/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273985 \n",
            "BATCH: 245/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273799 \n",
            "BATCH: 246/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273854 \n",
            "BATCH: 247/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273186 \n",
            "BATCH: 248/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69273242 \n",
            "BATCH: 249/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69272815 \n",
            "BATCH: 250/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69272872 \n",
            "BATCH: 251/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69272687 \n",
            "BATCH: 252/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69271781 \n",
            "BATCH: 253/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69272078 \n",
            "BATCH: 254/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69271414 \n",
            "BATCH: 255/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69271231 \n",
            "BATCH: 256/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270809 \n",
            "BATCH: 257/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270866 \n",
            "BATCH: 258/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270924 \n",
            "BATCH: 259/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69271938 \n",
            "BATCH: 260/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69271755 \n",
            "BATCH: 261/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69272529 \n",
            "BATCH: 262/358\n",
            "val loss item: 0.6807129383087158\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69271630 \n",
            "BATCH: 263/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270732 \n",
            "BATCH: 264/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270312 \n",
            "BATCH: 265/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270846 \n",
            "BATCH: 266/358\n",
            "val loss item: 0.6775230169296265\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269713 \n",
            "BATCH: 267/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269771 \n",
            "BATCH: 268/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269353 \n",
            "BATCH: 269/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270361 \n",
            "BATCH: 270/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270181 \n",
            "BATCH: 271/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269290 \n",
            "BATCH: 272/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270059 \n",
            "BATCH: 273/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269642 \n",
            "BATCH: 274/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69268990 \n",
            "BATCH: 275/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269994 \n",
            "BATCH: 276/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269579 \n",
            "BATCH: 277/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269872 \n",
            "BATCH: 278/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269930 \n",
            "BATCH: 279/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269987 \n",
            "BATCH: 280/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270044 \n",
            "BATCH: 281/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269865 \n",
            "BATCH: 282/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269687 \n",
            "BATCH: 283/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270685 \n",
            "BATCH: 284/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270506 \n",
            "BATCH: 285/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269859 \n",
            "BATCH: 286/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269915 \n",
            "BATCH: 287/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270207 \n",
            "BATCH: 288/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269561 \n",
            "BATCH: 289/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269618 \n",
            "BATCH: 290/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269207 \n",
            "BATCH: 291/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69268797 \n",
            "BATCH: 292/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269088 \n",
            "BATCH: 293/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270078 \n",
            "BATCH: 294/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69270135 \n",
            "BATCH: 295/358\n",
            "val loss item: 0.6775230169296265\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269026 \n",
            "BATCH: 296/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69268850 \n",
            "BATCH: 297/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69269140 \n",
            "BATCH: 298/358\n",
            "val loss item: 0.6775230169296265\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69268035 \n",
            "BATCH: 299/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69267395 \n",
            "BATCH: 300/358\n",
            "val loss item: 0.6775230169296265\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69266293 \n",
            "BATCH: 301/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265887 \n",
            "BATCH: 302/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265019 \n",
            "BATCH: 303/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265311 \n",
            "BATCH: 304/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264907 \n",
            "BATCH: 305/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264504 \n",
            "BATCH: 306/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265258 \n",
            "BATCH: 307/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265317 \n",
            "BATCH: 308/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265146 \n",
            "BATCH: 309/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265436 \n",
            "BATCH: 310/358\n",
            "val loss item: 0.7094223499298096\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69266647 \n",
            "BATCH: 311/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69267166 \n",
            "BATCH: 312/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69266764 \n",
            "BATCH: 313/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69267282 \n",
            "BATCH: 314/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69266650 \n",
            "BATCH: 315/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69266708 \n",
            "BATCH: 316/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69266307 \n",
            "BATCH: 317/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265906 \n",
            "BATCH: 318/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265277 \n",
            "BATCH: 319/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264420 \n",
            "BATCH: 320/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263793 \n",
            "BATCH: 321/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263624 \n",
            "BATCH: 322/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264598 \n",
            "BATCH: 323/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264657 \n",
            "BATCH: 324/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264032 \n",
            "BATCH: 325/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263179 \n",
            "BATCH: 326/358\n",
            "val loss item: 0.7062324285507202\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264151 \n",
            "BATCH: 327/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263527 \n",
            "BATCH: 328/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263359 \n",
            "BATCH: 329/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263874 \n",
            "BATCH: 330/358\n",
            "val loss item: 0.6807129979133606\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263024 \n",
            "BATCH: 331/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69262857 \n",
            "BATCH: 332/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69262917 \n",
            "BATCH: 333/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263204 \n",
            "BATCH: 334/358\n",
            "val loss item: 0.6998525857925415\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263717 \n",
            "BATCH: 335/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264455 \n",
            "BATCH: 336/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264288 \n",
            "BATCH: 337/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263895 \n",
            "BATCH: 338/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263954 \n",
            "BATCH: 339/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264238 \n",
            "BATCH: 340/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264523 \n",
            "BATCH: 341/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264581 \n",
            "BATCH: 342/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263964 \n",
            "BATCH: 343/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263572 \n",
            "BATCH: 344/358\n",
            "val loss item: 0.68390291929245\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69262956 \n",
            "BATCH: 345/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263016 \n",
            "BATCH: 346/358\n",
            "val loss item: 0.6870929002761841\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69262626 \n",
            "BATCH: 347/358\n",
            "val loss item: 0.6839029788970947\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69262012 \n",
            "BATCH: 348/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69262745 \n",
            "BATCH: 349/358\n",
            "val loss item: 0.6902828216552734\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69262580 \n",
            "BATCH: 350/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263312 \n",
            "BATCH: 351/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263594 \n",
            "BATCH: 352/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263653 \n",
            "BATCH: 353/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69263712 \n",
            "BATCH: 354/358\n",
            "val loss item: 0.7030425071716309\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264440 \n",
            "BATCH: 355/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264722 \n",
            "BATCH: 356/358\n",
            "val loss item: 0.6934727430343628\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69264779 \n",
            "BATCH: 357/358\n",
            "val loss item: 0.6966626644134521\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265060 \n",
            "BATCH: 358/358\n",
            "val loss item: 0.6977259516716003\n",
            " f1_val:0.00000000\n",
            " avg_val_loss: 0.69265414 \n",
            "SAVED\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss_data_BERTSOL, model_BERTSOL = bert_train(BERT_EPOCHS, bert_sol, bert_trainLoader, bert_valLoader,\n",
        "                                              optimizer_fn(bert_sol, sol_LR,weight_decay=0.000),\n",
        "                                              bert_sol_ckpt, bert_sol_best,\n",
        "                                              BERT_SOL_ATTEMPT[bert_sol_attempt],\n",
        "                                              save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0haksGsq8Fs"
      },
      "source": [
        "#### Export loss data dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export(loss_data_BERTBUPO,base_path+f\"BERT/TRAIN_LOSS_DATA/bertbupo_loss_data_{MAX_LENGTH}_{bert_bupo_attempt}.pkl\")\n",
        "export(loss_data_BERTSOL,base_path+f\"BERT/TRAIN_LOSS_DATA/bertsol_loss_data_{MAX_LENGTH}_{bert_sol_attempt}.pkl\")"
      ],
      "metadata": {
        "id": "fBcHaOfhwmT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJlQATwUq8Ft"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYj0Mb_iq8Ft"
      },
      "outputs": [],
      "source": [
        "def test_bert(model,testLoader):\n",
        "    test_loss = dict()\n",
        "    test_loss['avg_loss'] = 0\n",
        "    test_loss['len'] = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for index, batch in enumerate(testLoader):\n",
        "            test_loss[index+1]={'f1':[], 'loss':[]}\n",
        "            print(\"=\"*10)\n",
        "            print(f\"BATCH: {index+1}\")\n",
        "            print(\"=\"*10)\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device, dtype= torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype= torch.long)\n",
        "            token_type_ids = batch['token_type_ids'].to(device, dtype= torch.long)\n",
        "            targets = batch['targets'].to(device, dtype= torch.float)\n",
        "            output= model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "            loss= loss_fn(output,targets)\n",
        "            test_loss[index+1]['loss'].append(loss.item())\n",
        "            test_loss['avg_loss']= (test_loss['avg_loss']*test_loss['len'] + test_loss[index+1]['loss'][-1])/ (test_loss['len']+1)\n",
        "            test_loss['len']+=1\n",
        "\n",
        "            sigmoid_op= nn.Sigmoid()(output)\n",
        "            f1= get_f1(sigmoid_op, targets)\n",
        "            test_loss[index+1]['f1'].append(f1)\n",
        "\n",
        "            print(\"f1: {:.8f}\\nloss_item:{:.8f}\\ntest_loss_avg: {:.8f}\\n\".format(f1,loss.item(),test_loss['avg_loss']))\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-Cu0Ldzq8Fu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1e45fc-553d-454b-8d3c-4f39ede055b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========\n",
            "BATCH: 1\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.73635006\n",
            "\n",
            "==========\n",
            "BATCH: 2\n",
            "==========\n",
            "f1: 0.85714286\n",
            "loss_item:0.61460358\n",
            "test_loss_avg: 0.67547682\n",
            "\n",
            "==========\n",
            "BATCH: 3\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330212\n",
            "test_loss_avg: 0.67141859\n",
            "\n",
            "==========\n",
            "BATCH: 4\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.68765146\n",
            "\n",
            "==========\n",
            "BATCH: 5\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.68765146\n",
            "\n",
            "==========\n",
            "BATCH: 6\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.70794254\n",
            "\n",
            "==========\n",
            "BATCH: 7\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200079\n",
            "test_loss_avg: 0.70852229\n",
            "\n",
            "==========\n",
            "BATCH: 8\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.70895709\n",
            "\n",
            "==========\n",
            "BATCH: 9\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.71200075\n",
            "\n",
            "==========\n",
            "BATCH: 10\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.71443568\n",
            "\n",
            "==========\n",
            "BATCH: 11\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.71642790\n",
            "\n",
            "==========\n",
            "BATCH: 12\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.71402986\n",
            "\n",
            "==========\n",
            "BATCH: 13\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.71387378\n",
            "\n",
            "==========\n",
            "BATCH: 14\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72069693\n",
            "\n",
            "==========\n",
            "BATCH: 15\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.71849390\n",
            "\n",
            "==========\n",
            "BATCH: 16\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72417540\n",
            "\n",
            "==========\n",
            "BATCH: 17\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72918849\n",
            "\n",
            "==========\n",
            "BATCH: 18\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72823362\n",
            "\n",
            "==========\n",
            "BATCH: 19\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72866080\n",
            "\n",
            "==========\n",
            "BATCH: 20\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72661033\n",
            "\n",
            "==========\n",
            "BATCH: 21\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72707413\n",
            "\n",
            "==========\n",
            "BATCH: 22\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72860255\n",
            "\n",
            "==========\n",
            "BATCH: 23\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72893940\n",
            "\n",
            "==========\n",
            "BATCH: 24\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72518996\n",
            "\n",
            "==========\n",
            "BATCH: 25\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72661033\n",
            "\n",
            "==========\n",
            "BATCH: 26\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72511191\n",
            "\n",
            "==========\n",
            "BATCH: 27\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72372449\n",
            "\n",
            "==========\n",
            "BATCH: 28\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72330578\n",
            "\n",
            "==========\n",
            "BATCH: 29\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72207632\n",
            "\n",
            "==========\n",
            "BATCH: 30\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72255211\n",
            "\n",
            "==========\n",
            "BATCH: 31\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.71985536\n",
            "\n",
            "==========\n",
            "BATCH: 32\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.71808808\n",
            "\n",
            "==========\n",
            "BATCH: 33\n",
            "==========\n",
            "f1: 0.22222222\n",
            "loss_item:0.85809654\n",
            "test_loss_avg: 0.72233076\n",
            "\n",
            "==========\n",
            "BATCH: 34\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72202693\n",
            "\n",
            "==========\n",
            "BATCH: 35\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72452325\n",
            "\n",
            "==========\n",
            "BATCH: 36\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72349903\n",
            "\n",
            "==========\n",
            "BATCH: 37\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72384636\n",
            "\n",
            "==========\n",
            "BATCH: 38\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72289386\n",
            "\n",
            "==========\n",
            "BATCH: 39\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200079\n",
            "test_loss_avg: 0.72261455\n",
            "\n",
            "==========\n",
            "BATCH: 40\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72174047\n",
            "\n",
            "==========\n",
            "BATCH: 41\n",
            "==========\n",
            "f1: 0.85714286\n",
            "loss_item:0.61460358\n",
            "test_loss_avg: 0.71912738\n",
            "\n",
            "==========\n",
            "BATCH: 42\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.71953744\n",
            "\n",
            "==========\n",
            "BATCH: 43\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.71936217\n",
            "\n",
            "==========\n",
            "BATCH: 44\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.71808808\n",
            "\n",
            "==========\n",
            "BATCH: 45\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.71903499\n",
            "\n",
            "==========\n",
            "BATCH: 46\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.71888207\n",
            "\n",
            "==========\n",
            "BATCH: 47\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72080794\n",
            "\n",
            "==========\n",
            "BATCH: 48\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72011718\n",
            "\n",
            "==========\n",
            "BATCH: 49\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72044847\n",
            "\n",
            "==========\n",
            "BATCH: 50\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.71930554\n",
            "\n",
            "==========\n",
            "BATCH: 51\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.71963975\n",
            "\n",
            "==========\n",
            "BATCH: 52\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.71949284\n",
            "\n",
            "==========\n",
            "BATCH: 53\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72118917\n",
            "\n",
            "==========\n",
            "BATCH: 54\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72192084\n",
            "\n",
            "==========\n",
            "BATCH: 55\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72262590\n",
            "\n",
            "==========\n",
            "BATCH: 56\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72417540\n",
            "\n",
            "==========\n",
            "BATCH: 57\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72438899\n",
            "\n",
            "==========\n",
            "BATCH: 58\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72585466\n",
            "\n",
            "==========\n",
            "BATCH: 59\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72479445\n",
            "\n",
            "==========\n",
            "BATCH: 60\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72498704\n",
            "\n",
            "==========\n",
            "BATCH: 61\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72477415\n",
            "\n",
            "==========\n",
            "BATCH: 62\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72456813\n",
            "\n",
            "==========\n",
            "BATCH: 63\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72552814\n",
            "\n",
            "==========\n",
            "BATCH: 64\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72569723\n",
            "\n",
            "==========\n",
            "BATCH: 65\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72586112\n",
            "\n",
            "==========\n",
            "BATCH: 66\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72491326\n",
            "\n",
            "==========\n",
            "BATCH: 67\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72472053\n",
            "\n",
            "==========\n",
            "BATCH: 68\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72524963\n",
            "\n",
            "==========\n",
            "BATCH: 69\n",
            "==========\n",
            "f1: 0.85714286\n",
            "loss_item:0.61460352\n",
            "test_loss_avg: 0.72364607\n",
            "\n",
            "==========\n",
            "BATCH: 70\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72313186\n",
            "\n",
            "==========\n",
            "BATCH: 71\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72263214\n",
            "\n",
            "==========\n",
            "BATCH: 72\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72349903\n",
            "\n",
            "==========\n",
            "BATCH: 73\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72300797\n",
            "\n",
            "==========\n",
            "BATCH: 74\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72220113\n",
            "\n",
            "==========\n",
            "BATCH: 75\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72271444\n",
            "\n",
            "==========\n",
            "BATCH: 76\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72321424\n",
            "\n",
            "==========\n",
            "BATCH: 77\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72370106\n",
            "\n",
            "==========\n",
            "BATCH: 78\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895291\n",
            "test_loss_avg: 0.72261455\n",
            "\n",
            "==========\n",
            "BATCH: 79\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72248020\n",
            "\n",
            "==========\n",
            "BATCH: 80\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72295794\n",
            "\n",
            "==========\n",
            "BATCH: 81\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72312327\n",
            "\n",
            "==========\n",
            "BATCH: 82\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72328457\n",
            "\n",
            "==========\n",
            "BATCH: 83\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72314862\n",
            "\n",
            "==========\n",
            "BATCH: 84\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72388553\n",
            "\n",
            "==========\n",
            "BATCH: 85\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72403217\n",
            "\n",
            "==========\n",
            "BATCH: 86\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72360914\n",
            "\n",
            "==========\n",
            "BATCH: 87\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72459522\n",
            "\n",
            "==========\n",
            "BATCH: 88\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72472880\n",
            "\n",
            "==========\n",
            "BATCH: 89\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72513296\n",
            "\n",
            "==========\n",
            "BATCH: 90\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72498705\n",
            "\n",
            "==========\n",
            "BATCH: 91\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72484434\n",
            "\n",
            "==========\n",
            "BATCH: 92\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72523407\n",
            "\n",
            "==========\n",
            "BATCH: 93\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200079\n",
            "test_loss_avg: 0.72509177\n",
            "\n",
            "==========\n",
            "BATCH: 94\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72469347\n",
            "\n",
            "==========\n",
            "BATCH: 95\n",
            "==========\n",
            "f1: 0.85714286\n",
            "loss_item:0.61460352\n",
            "test_loss_avg: 0.72353463\n",
            "\n",
            "==========\n",
            "BATCH: 96\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72341449\n",
            "\n",
            "==========\n",
            "BATCH: 97\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72279477\n",
            "\n",
            "==========\n",
            "BATCH: 98\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72318155\n",
            "\n",
            "==========\n",
            "BATCH: 99\n",
            "==========\n",
            "f1: 0.31578947\n",
            "loss_item:0.83374727\n",
            "test_loss_avg: 0.72429838\n",
            "\n",
            "==========\n",
            "BATCH: 100\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72466239\n",
            "\n",
            "==========\n",
            "BATCH: 101\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72429594\n",
            "\n",
            "==========\n",
            "BATCH: 102\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200079\n",
            "test_loss_avg: 0.72417540\n",
            "\n",
            "==========\n",
            "BATCH: 103\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72476640\n",
            "\n",
            "==========\n",
            "BATCH: 104\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72487779\n",
            "\n",
            "==========\n",
            "BATCH: 105\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72475515\n",
            "\n",
            "==========\n",
            "BATCH: 106\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72486453\n",
            "\n",
            "==========\n",
            "BATCH: 107\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72406162\n",
            "\n",
            "==========\n",
            "BATCH: 108\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72372449\n",
            "\n",
            "==========\n",
            "BATCH: 109\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72406371\n",
            "\n",
            "==========\n",
            "BATCH: 110\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72417540\n",
            "\n",
            "==========\n",
            "BATCH: 111\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72362699\n",
            "\n",
            "==========\n",
            "BATCH: 112\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72395800\n",
            "\n",
            "==========\n",
            "BATCH: 113\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72363670\n",
            "\n",
            "==========\n",
            "BATCH: 114\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72332104\n",
            "\n",
            "==========\n",
            "BATCH: 115\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72364607\n",
            "\n",
            "==========\n",
            "BATCH: 116\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72333577\n",
            "\n",
            "==========\n",
            "BATCH: 117\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72344700\n",
            "\n",
            "==========\n",
            "BATCH: 118\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72376270\n",
            "\n",
            "==========\n",
            "BATCH: 119\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72386848\n",
            "\n",
            "==========\n",
            "BATCH: 120\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72437831\n",
            "\n",
            "==========\n",
            "BATCH: 121\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72367232\n",
            "\n",
            "==========\n",
            "BATCH: 122\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72377623\n",
            "\n",
            "==========\n",
            "BATCH: 123\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72427438\n",
            "\n",
            "==========\n",
            "BATCH: 124\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72476450\n",
            "\n",
            "==========\n",
            "BATCH: 125\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72446759\n",
            "\n",
            "==========\n",
            "BATCH: 126\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72436865\n",
            "\n",
            "==========\n",
            "BATCH: 127\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72427126\n",
            "\n",
            "==========\n",
            "BATCH: 128\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72455586\n",
            "\n",
            "==========\n",
            "BATCH: 129\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72426978\n",
            "\n",
            "==========\n",
            "BATCH: 130\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72436270\n",
            "\n",
            "==========\n",
            "BATCH: 131\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72464008\n",
            "\n",
            "==========\n",
            "BATCH: 132\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72435986\n",
            "\n",
            "==========\n",
            "BATCH: 133\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72445002\n",
            "\n",
            "==========\n",
            "BATCH: 134\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72435711\n",
            "\n",
            "==========\n",
            "BATCH: 135\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72408522\n",
            "\n",
            "==========\n",
            "BATCH: 136\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72345924\n",
            "\n",
            "==========\n",
            "BATCH: 137\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72337561\n",
            "\n",
            "==========\n",
            "BATCH: 138\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72311674\n",
            "\n",
            "==========\n",
            "BATCH: 139\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72321194\n",
            "\n",
            "==========\n",
            "BATCH: 140\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72347971\n",
            "\n",
            "==========\n",
            "BATCH: 141\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72374368\n",
            "\n",
            "==========\n",
            "BATCH: 142\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72383245\n",
            "\n",
            "==========\n",
            "BATCH: 143\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72409026\n",
            "\n",
            "==========\n",
            "BATCH: 144\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72400631\n",
            "\n",
            "==========\n",
            "BATCH: 145\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200079\n",
            "test_loss_avg: 0.72392351\n",
            "\n",
            "==========\n",
            "BATCH: 146\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72417540\n",
            "\n",
            "==========\n",
            "BATCH: 147\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72425822\n",
            "\n",
            "==========\n",
            "BATCH: 148\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72450445\n",
            "\n",
            "==========\n",
            "BATCH: 149\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72474736\n",
            "\n",
            "==========\n",
            "BATCH: 150\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72482472\n",
            "\n",
            "==========\n",
            "BATCH: 151\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72441728\n",
            "\n",
            "==========\n",
            "BATCH: 152\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72401521\n",
            "\n",
            "==========\n",
            "BATCH: 153\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72441412\n",
            "\n",
            "==========\n",
            "BATCH: 154\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72417540\n",
            "\n",
            "==========\n",
            "BATCH: 155\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72393976\n",
            "\n",
            "==========\n",
            "BATCH: 156\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72339498\n",
            "\n",
            "==========\n",
            "BATCH: 157\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72285713\n",
            "\n",
            "==========\n",
            "BATCH: 158\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72294253\n",
            "\n",
            "==========\n",
            "BATCH: 159\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330212\n",
            "test_loss_avg: 0.72256743\n",
            "\n",
            "==========\n",
            "BATCH: 160\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72295794\n",
            "\n",
            "==========\n",
            "BATCH: 161\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72273864\n",
            "\n",
            "==========\n",
            "BATCH: 162\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72237175\n",
            "\n",
            "==========\n",
            "BATCH: 163\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72230812\n",
            "\n",
            "==========\n",
            "BATCH: 164\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72239375\n",
            "\n",
            "==========\n",
            "BATCH: 165\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72247833\n",
            "\n",
            "==========\n",
            "BATCH: 166\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72241521\n",
            "\n",
            "==========\n",
            "BATCH: 167\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72264446\n",
            "\n",
            "==========\n",
            "BATCH: 168\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72243617\n",
            "\n",
            "==========\n",
            "BATCH: 169\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72237442\n",
            "\n",
            "==========\n",
            "BATCH: 170\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72274309\n",
            "\n",
            "==========\n",
            "BATCH: 171\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72239548\n",
            "\n",
            "==========\n",
            "BATCH: 172\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72219348\n",
            "\n",
            "==========\n",
            "BATCH: 173\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72199382\n",
            "\n",
            "==========\n",
            "BATCH: 174\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72193639\n",
            "\n",
            "==========\n",
            "BATCH: 175\n",
            "==========\n",
            "f1: 0.31578947\n",
            "loss_item:0.83374727\n",
            "test_loss_avg: 0.72257531\n",
            "\n",
            "==========\n",
            "BATCH: 176\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72251522\n",
            "\n",
            "==========\n",
            "BATCH: 177\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72231825\n",
            "\n",
            "==========\n",
            "BATCH: 178\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72239708\n",
            "\n",
            "==========\n",
            "BATCH: 179\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72247503\n",
            "\n",
            "==========\n",
            "BATCH: 180\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72228157\n",
            "\n",
            "==========\n",
            "BATCH: 181\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72276287\n",
            "\n",
            "==========\n",
            "BATCH: 182\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72270374\n",
            "\n",
            "==========\n",
            "BATCH: 183\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72291137\n",
            "\n",
            "==========\n",
            "BATCH: 184\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72298440\n",
            "\n",
            "==========\n",
            "BATCH: 185\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72305665\n",
            "\n",
            "==========\n",
            "BATCH: 186\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72299721\n",
            "\n",
            "==========\n",
            "BATCH: 187\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72306862\n",
            "\n",
            "==========\n",
            "BATCH: 188\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72275071\n",
            "\n",
            "==========\n",
            "BATCH: 189\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72243617\n",
            "\n",
            "==========\n",
            "BATCH: 190\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72199678\n",
            "\n",
            "==========\n",
            "BATCH: 191\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72194445\n",
            "\n",
            "==========\n",
            "BATCH: 192\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72151220\n",
            "\n",
            "==========\n",
            "BATCH: 193\n",
            "==========\n",
            "f1: 0.31578947\n",
            "loss_item:0.83374727\n",
            "test_loss_avg: 0.72209373\n",
            "\n",
            "==========\n",
            "BATCH: 194\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72229273\n",
            "\n",
            "==========\n",
            "BATCH: 195\n",
            "==========\n",
            "f1: 0.22222222\n",
            "loss_item:0.85809654\n",
            "test_loss_avg: 0.72298916\n",
            "\n",
            "==========\n",
            "BATCH: 196\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72268463\n",
            "\n",
            "==========\n",
            "BATCH: 197\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72238320\n",
            "\n",
            "==========\n",
            "BATCH: 198\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72233076\n",
            "\n",
            "==========\n",
            "BATCH: 199\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72203413\n",
            "\n",
            "==========\n",
            "BATCH: 200\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72186222\n",
            "\n",
            "==========\n",
            "BATCH: 201\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72169202\n",
            "\n",
            "==========\n",
            "BATCH: 202\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72152350\n",
            "\n",
            "==========\n",
            "BATCH: 203\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72159654\n",
            "\n",
            "==========\n",
            "BATCH: 204\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72119142\n",
            "\n",
            "==========\n",
            "BATCH: 205\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72114659\n",
            "\n",
            "==========\n",
            "BATCH: 206\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72133859\n",
            "\n",
            "==========\n",
            "BATCH: 207\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72129348\n",
            "\n",
            "==========\n",
            "BATCH: 208\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72171706\n",
            "\n",
            "==========\n",
            "BATCH: 209\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72178708\n",
            "\n",
            "==========\n",
            "BATCH: 210\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72185642\n",
            "\n",
            "==========\n",
            "BATCH: 211\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72192511\n",
            "\n",
            "==========\n",
            "BATCH: 212\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72222287\n",
            "\n",
            "==========\n",
            "BATCH: 213\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72194624\n",
            "\n",
            "==========\n",
            "BATCH: 214\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72212733\n",
            "\n",
            "==========\n",
            "BATCH: 215\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72253324\n",
            "\n",
            "==========\n",
            "BATCH: 216\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72237175\n",
            "\n",
            "==========\n",
            "BATCH: 217\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72243617\n",
            "\n",
            "==========\n",
            "BATCH: 218\n",
            "==========\n",
            "f1: 0.93333333\n",
            "loss_item:0.56590503\n",
            "test_loss_avg: 0.72171814\n",
            "\n",
            "==========\n",
            "BATCH: 219\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200079\n",
            "test_loss_avg: 0.72167376\n",
            "\n",
            "==========\n",
            "BATCH: 220\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72162980\n",
            "\n",
            "==========\n",
            "BATCH: 221\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72158622\n",
            "\n",
            "==========\n",
            "BATCH: 222\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72132368\n",
            "\n",
            "==========\n",
            "BATCH: 223\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72139107\n",
            "\n",
            "==========\n",
            "BATCH: 224\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72156655\n",
            "\n",
            "==========\n",
            "BATCH: 225\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72141582\n",
            "\n",
            "==========\n",
            "BATCH: 226\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72158964\n",
            "\n",
            "==========\n",
            "BATCH: 227\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72133286\n",
            "\n",
            "==========\n",
            "BATCH: 228\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72150553\n",
            "\n",
            "==========\n",
            "BATCH: 229\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72146402\n",
            "\n",
            "==========\n",
            "BATCH: 230\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72152874\n",
            "\n",
            "==========\n",
            "BATCH: 231\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72127668\n",
            "\n",
            "==========\n",
            "BATCH: 232\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72123670\n",
            "\n",
            "==========\n",
            "BATCH: 233\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72098805\n",
            "\n",
            "==========\n",
            "BATCH: 234\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72105370\n",
            "\n",
            "==========\n",
            "BATCH: 235\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72132602\n",
            "\n",
            "==========\n",
            "BATCH: 236\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72118333\n",
            "\n",
            "==========\n",
            "BATCH: 237\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72083637\n",
            "\n",
            "==========\n",
            "BATCH: 238\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72069693\n",
            "\n",
            "==========\n",
            "BATCH: 239\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72086431\n",
            "\n",
            "==========\n",
            "BATCH: 240\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72103029\n",
            "\n",
            "==========\n",
            "BATCH: 241\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72109385\n",
            "\n",
            "==========\n",
            "BATCH: 242\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72115690\n",
            "\n",
            "==========\n",
            "BATCH: 243\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72111922\n",
            "\n",
            "==========\n",
            "BATCH: 244\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72088226\n",
            "\n",
            "==========\n",
            "BATCH: 245\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72124355\n",
            "\n",
            "==========\n",
            "BATCH: 246\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72100802\n",
            "\n",
            "==========\n",
            "BATCH: 247\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72107013\n",
            "\n",
            "==========\n",
            "BATCH: 248\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72132811\n",
            "\n",
            "==========\n",
            "BATCH: 249\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72168180\n",
            "\n",
            "==========\n",
            "BATCH: 250\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72193527\n",
            "\n",
            "==========\n",
            "BATCH: 251\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72170167\n",
            "\n",
            "==========\n",
            "BATCH: 252\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72166318\n",
            "\n",
            "==========\n",
            "BATCH: 253\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72172123\n",
            "\n",
            "==========\n",
            "BATCH: 254\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72177882\n",
            "\n",
            "==========\n",
            "BATCH: 255\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72164499\n",
            "\n",
            "==========\n",
            "BATCH: 256\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72189266\n",
            "\n",
            "==========\n",
            "BATCH: 257\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72175942\n",
            "\n",
            "==========\n",
            "BATCH: 258\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72200473\n",
            "\n",
            "==========\n",
            "BATCH: 259\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72215413\n",
            "\n",
            "==========\n",
            "BATCH: 260\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72230238\n",
            "\n",
            "==========\n",
            "BATCH: 261\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72226291\n",
            "\n",
            "==========\n",
            "BATCH: 262\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72203787\n",
            "\n",
            "==========\n",
            "BATCH: 263\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72190712\n",
            "\n",
            "==========\n",
            "BATCH: 264\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504860\n",
            "test_loss_avg: 0.72214630\n",
            "\n",
            "==========\n",
            "BATCH: 265\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72238366\n",
            "\n",
            "==========\n",
            "BATCH: 266\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72207001\n",
            "\n",
            "==========\n",
            "BATCH: 267\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72203230\n",
            "\n",
            "==========\n",
            "BATCH: 268\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72190401\n",
            "\n",
            "==========\n",
            "BATCH: 269\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72186720\n",
            "\n",
            "==========\n",
            "BATCH: 270\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72183066\n",
            "\n",
            "==========\n",
            "BATCH: 271\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72188423\n",
            "\n",
            "==========\n",
            "BATCH: 272\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72193742\n",
            "\n",
            "==========\n",
            "BATCH: 273\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72199021\n",
            "\n",
            "==========\n",
            "BATCH: 274\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635012\n",
            "test_loss_avg: 0.72204262\n",
            "\n",
            "==========\n",
            "BATCH: 275\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72174047\n",
            "\n",
            "==========\n",
            "BATCH: 276\n",
            "==========\n",
            "f1: 0.85714286\n",
            "loss_item:0.61460364\n",
            "test_loss_avg: 0.72135230\n",
            "\n",
            "==========\n",
            "BATCH: 277\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72123063\n",
            "\n",
            "==========\n",
            "BATCH: 278\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72128502\n",
            "\n",
            "==========\n",
            "BATCH: 279\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72142629\n",
            "\n",
            "==========\n",
            "BATCH: 280\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72156655\n",
            "\n",
            "==========\n",
            "BATCH: 281\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72144586\n",
            "\n",
            "==========\n",
            "BATCH: 282\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72141236\n",
            "\n",
            "==========\n",
            "BATCH: 283\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72129307\n",
            "\n",
            "==========\n",
            "BATCH: 284\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72126035\n",
            "\n",
            "==========\n",
            "BATCH: 285\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72114242\n",
            "\n",
            "==========\n",
            "BATCH: 286\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330212\n",
            "test_loss_avg: 0.72094018\n",
            "\n",
            "==========\n",
            "BATCH: 287\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72107872\n",
            "\n",
            "==========\n",
            "BATCH: 288\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72113174\n",
            "\n",
            "==========\n",
            "BATCH: 289\n",
            "==========\n",
            "f1: 0.31578947\n",
            "loss_item:0.83374727\n",
            "test_loss_avg: 0.72152142\n",
            "\n",
            "==========\n",
            "BATCH: 290\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72165651\n",
            "\n",
            "==========\n",
            "BATCH: 291\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72162333\n",
            "\n",
            "==========\n",
            "BATCH: 292\n",
            "==========\n",
            "f1: 0.81481481\n",
            "loss_item:0.63895285\n",
            "test_loss_avg: 0.72134021\n",
            "\n",
            "==========\n",
            "BATCH: 293\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72147454\n",
            "\n",
            "==========\n",
            "BATCH: 294\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72177360\n",
            "\n",
            "==========\n",
            "BATCH: 295\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72182301\n",
            "\n",
            "==========\n",
            "BATCH: 296\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72195435\n",
            "\n",
            "==========\n",
            "BATCH: 297\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72200282\n",
            "\n",
            "==========\n",
            "BATCH: 298\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72205097\n",
            "\n",
            "==========\n",
            "BATCH: 299\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72201736\n",
            "\n",
            "==========\n",
            "BATCH: 300\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72214630\n",
            "\n",
            "==========\n",
            "BATCH: 301\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72203170\n",
            "\n",
            "==========\n",
            "BATCH: 302\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72183723\n",
            "\n",
            "==========\n",
            "BATCH: 303\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72180476\n",
            "\n",
            "==========\n",
            "BATCH: 304\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72185261\n",
            "\n",
            "==========\n",
            "BATCH: 305\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72190014\n",
            "\n",
            "==========\n",
            "BATCH: 306\n",
            "==========\n",
            "f1: 0.22222222\n",
            "loss_item:0.85809654\n",
            "test_loss_avg: 0.72234523\n",
            "\n",
            "==========\n",
            "BATCH: 307\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72239085\n",
            "\n",
            "==========\n",
            "BATCH: 308\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72243617\n",
            "\n",
            "==========\n",
            "BATCH: 309\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72240240\n",
            "\n",
            "==========\n",
            "BATCH: 310\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72252594\n",
            "\n",
            "==========\n",
            "BATCH: 311\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72280527\n",
            "\n",
            "==========\n",
            "BATCH: 312\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72269259\n",
            "\n",
            "==========\n",
            "BATCH: 313\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72258064\n",
            "\n",
            "==========\n",
            "BATCH: 314\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72246940\n",
            "\n",
            "==========\n",
            "BATCH: 315\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72235887\n",
            "\n",
            "==========\n",
            "BATCH: 316\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72240315\n",
            "\n",
            "==========\n",
            "BATCH: 317\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72244714\n",
            "\n",
            "==========\n",
            "BATCH: 318\n",
            "==========\n",
            "f1: 0.96774194\n",
            "loss_item:0.54155570\n",
            "test_loss_avg: 0.72187830\n",
            "\n",
            "==========\n",
            "BATCH: 319\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72192367\n",
            "\n",
            "==========\n",
            "BATCH: 320\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72189266\n",
            "\n",
            "==========\n",
            "BATCH: 321\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72178599\n",
            "\n",
            "==========\n",
            "BATCH: 322\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200079\n",
            "test_loss_avg: 0.72175560\n",
            "\n",
            "==========\n",
            "BATCH: 323\n",
            "==========\n",
            "f1: 0.40000000\n",
            "loss_item:0.80939794\n",
            "test_loss_avg: 0.72202694\n",
            "\n",
            "==========\n",
            "BATCH: 324\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72199599\n",
            "\n",
            "==========\n",
            "BATCH: 325\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72211508\n",
            "\n",
            "==========\n",
            "BATCH: 326\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72208405\n",
            "\n",
            "==========\n",
            "BATCH: 327\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72212768\n",
            "\n",
            "==========\n",
            "BATCH: 328\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72231951\n",
            "\n",
            "==========\n",
            "BATCH: 329\n",
            "==========\n",
            "f1: 0.89655172\n",
            "loss_item:0.59025425\n",
            "test_loss_avg: 0.72191810\n",
            "\n",
            "==========\n",
            "BATCH: 330\n",
            "==========\n",
            "f1: 0.85714286\n",
            "loss_item:0.61460358\n",
            "test_loss_avg: 0.72159290\n",
            "\n",
            "==========\n",
            "BATCH: 331\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72156392\n",
            "\n",
            "==========\n",
            "BATCH: 332\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765152\n",
            "test_loss_avg: 0.72146178\n",
            "\n",
            "==========\n",
            "BATCH: 333\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72128713\n",
            "\n",
            "==========\n",
            "BATCH: 334\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72118642\n",
            "\n",
            "==========\n",
            "BATCH: 335\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72115900\n",
            "\n",
            "==========\n",
            "BATCH: 336\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72113174\n",
            "\n",
            "==========\n",
            "BATCH: 337\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72103239\n",
            "\n",
            "==========\n",
            "BATCH: 338\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72100567\n",
            "\n",
            "==========\n",
            "BATCH: 339\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72112276\n",
            "\n",
            "==========\n",
            "BATCH: 340\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72116755\n",
            "\n",
            "==========\n",
            "BATCH: 341\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72128348\n",
            "\n",
            "==========\n",
            "BATCH: 342\n",
            "==========\n",
            "f1: 0.47619048\n",
            "loss_item:0.78504866\n",
            "test_loss_avg: 0.72146993\n",
            "\n",
            "==========\n",
            "BATCH: 343\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72158430\n",
            "\n",
            "==========\n",
            "BATCH: 344\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72141487\n",
            "\n",
            "==========\n",
            "BATCH: 345\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069939\n",
            "test_loss_avg: 0.72152874\n",
            "\n",
            "==========\n",
            "BATCH: 346\n",
            "==========\n",
            "f1: 0.54545455\n",
            "loss_item:0.76069933\n",
            "test_loss_avg: 0.72164195\n",
            "\n",
            "==========\n",
            "BATCH: 347\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72154400\n",
            "\n",
            "==========\n",
            "BATCH: 348\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72144660\n",
            "\n",
            "==========\n",
            "BATCH: 349\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765146\n",
            "test_loss_avg: 0.72134977\n",
            "\n",
            "==========\n",
            "BATCH: 350\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72139263\n",
            "\n",
            "==========\n",
            "BATCH: 351\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72129650\n",
            "\n",
            "==========\n",
            "BATCH: 352\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72133927\n",
            "\n",
            "==========\n",
            "BATCH: 353\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72138179\n",
            "\n",
            "==========\n",
            "BATCH: 354\n",
            "==========\n",
            "f1: 0.72000000\n",
            "loss_item:0.68765140\n",
            "test_loss_avg: 0.72128650\n",
            "\n",
            "==========\n",
            "BATCH: 355\n",
            "==========\n",
            "f1: 0.60869565\n",
            "loss_item:0.73635006\n",
            "test_loss_avg: 0.72132894\n",
            "\n",
            "==========\n",
            "BATCH: 356\n",
            "==========\n",
            "f1: 0.76923077\n",
            "loss_item:0.66330218\n",
            "test_loss_avg: 0.72116594\n",
            "\n",
            "==========\n",
            "BATCH: 357\n",
            "==========\n",
            "f1: 0.66666667\n",
            "loss_item:0.71200073\n",
            "test_loss_avg: 0.72114027\n",
            "\n",
            "==========\n",
            "BATCH: 358\n",
            "==========\n",
            "f1: 0.58823529\n",
            "loss_item:0.74446648\n",
            "test_loss_avg: 0.72120543\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_data_BUPO= test_bert(bert_bupo,bert_testLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpN_PNaaq8Fu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd1f9b6-97b2-47bb-cd48-09c8dd8e48af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========\n",
            "BATCH: 1\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69028282\n",
            "\n",
            "==========\n",
            "BATCH: 2\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70623243\n",
            "test_loss_avg: 0.69825763\n",
            "\n",
            "==========\n",
            "BATCH: 3\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69878928\n",
            "\n",
            "==========\n",
            "BATCH: 4\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69666266\n",
            "\n",
            "==========\n",
            "BATCH: 5\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69666266\n",
            "\n",
            "==========\n",
            "BATCH: 6\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69400439\n",
            "\n",
            "==========\n",
            "BATCH: 7\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69392844\n",
            "\n",
            "==========\n",
            "BATCH: 8\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69387148\n",
            "\n",
            "==========\n",
            "BATCH: 9\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69347274\n",
            "\n",
            "==========\n",
            "BATCH: 10\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028276\n",
            "test_loss_avg: 0.69315374\n",
            "\n",
            "==========\n",
            "BATCH: 11\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69289275\n",
            "\n",
            "==========\n",
            "BATCH: 12\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69320691\n",
            "\n",
            "==========\n",
            "BATCH: 13\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69322736\n",
            "\n",
            "==========\n",
            "BATCH: 14\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69233347\n",
            "\n",
            "==========\n",
            "BATCH: 15\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69262209\n",
            "\n",
            "==========\n",
            "BATCH: 16\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69187777\n",
            "\n",
            "==========\n",
            "BATCH: 17\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69122102\n",
            "\n",
            "==========\n",
            "BATCH: 18\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69134611\n",
            "\n",
            "==========\n",
            "BATCH: 19\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69129015\n",
            "\n",
            "==========\n",
            "BATCH: 20\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69155878\n",
            "\n",
            "==========\n",
            "BATCH: 21\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69149802\n",
            "\n",
            "==========\n",
            "BATCH: 22\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69129778\n",
            "\n",
            "==========\n",
            "BATCH: 23\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69125365\n",
            "\n",
            "==========\n",
            "BATCH: 24\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69174486\n",
            "\n",
            "==========\n",
            "BATCH: 25\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69155878\n",
            "\n",
            "==========\n",
            "BATCH: 26\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69175508\n",
            "\n",
            "==========\n",
            "BATCH: 27\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69193684\n",
            "\n",
            "==========\n",
            "BATCH: 28\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69199170\n",
            "\n",
            "==========\n",
            "BATCH: 29\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69215277\n",
            "\n",
            "==========\n",
            "BATCH: 30\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69209043\n",
            "\n",
            "==========\n",
            "BATCH: 31\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69244373\n",
            "\n",
            "==========\n",
            "BATCH: 32\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69267525\n",
            "\n",
            "==========\n",
            "BATCH: 33\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.67433310\n",
            "test_loss_avg: 0.69211943\n",
            "\n",
            "==========\n",
            "BATCH: 34\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69215923\n",
            "\n",
            "==========\n",
            "BATCH: 35\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69183220\n",
            "\n",
            "==========\n",
            "BATCH: 36\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69196638\n",
            "\n",
            "==========\n",
            "BATCH: 37\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69192088\n",
            "\n",
            "==========\n",
            "BATCH: 38\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69204566\n",
            "\n",
            "==========\n",
            "BATCH: 39\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69208225\n",
            "\n",
            "==========\n",
            "BATCH: 40\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69219676\n",
            "\n",
            "==========\n",
            "BATCH: 41\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70623243\n",
            "test_loss_avg: 0.69253910\n",
            "\n",
            "==========\n",
            "BATCH: 42\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69248538\n",
            "\n",
            "==========\n",
            "BATCH: 43\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69250834\n",
            "\n",
            "==========\n",
            "BATCH: 44\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69267525\n",
            "\n",
            "==========\n",
            "BATCH: 45\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69255120\n",
            "\n",
            "==========\n",
            "BATCH: 46\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69257123\n",
            "\n",
            "==========\n",
            "BATCH: 47\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69231893\n",
            "\n",
            "==========\n",
            "BATCH: 48\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69240942\n",
            "\n",
            "==========\n",
            "BATCH: 49\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69236602\n",
            "\n",
            "==========\n",
            "BATCH: 50\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69251576\n",
            "\n",
            "==========\n",
            "BATCH: 51\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69247197\n",
            "\n",
            "==========\n",
            "BATCH: 52\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69249122\n",
            "\n",
            "==========\n",
            "BATCH: 53\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071294\n",
            "test_loss_avg: 0.69226899\n",
            "\n",
            "==========\n",
            "BATCH: 54\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69217313\n",
            "\n",
            "==========\n",
            "BATCH: 55\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69208077\n",
            "\n",
            "==========\n",
            "BATCH: 56\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69187777\n",
            "\n",
            "==========\n",
            "BATCH: 57\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69184979\n",
            "\n",
            "==========\n",
            "BATCH: 58\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69165777\n",
            "\n",
            "==========\n",
            "BATCH: 59\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69179667\n",
            "\n",
            "==========\n",
            "BATCH: 60\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69177144\n",
            "\n",
            "==========\n",
            "BATCH: 61\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69179933\n",
            "\n",
            "==========\n",
            "BATCH: 62\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69182632\n",
            "\n",
            "==========\n",
            "BATCH: 63\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69170055\n",
            "\n",
            "==========\n",
            "BATCH: 64\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69167840\n",
            "\n",
            "==========\n",
            "BATCH: 65\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69165693\n",
            "\n",
            "==========\n",
            "BATCH: 66\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69178111\n",
            "\n",
            "==========\n",
            "BATCH: 67\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69180635\n",
            "\n",
            "==========\n",
            "BATCH: 68\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69173704\n",
            "\n",
            "==========\n",
            "BATCH: 69\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70623243\n",
            "test_loss_avg: 0.69194712\n",
            "\n",
            "==========\n",
            "BATCH: 70\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69201448\n",
            "\n",
            "==========\n",
            "BATCH: 71\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69207995\n",
            "\n",
            "==========\n",
            "BATCH: 72\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69196638\n",
            "\n",
            "==========\n",
            "BATCH: 73\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69203071\n",
            "\n",
            "==========\n",
            "BATCH: 74\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69213641\n",
            "\n",
            "==========\n",
            "BATCH: 75\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69206917\n",
            "\n",
            "==========\n",
            "BATCH: 76\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69200369\n",
            "\n",
            "==========\n",
            "BATCH: 77\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69193991\n",
            "\n",
            "==========\n",
            "BATCH: 78\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69208225\n",
            "\n",
            "==========\n",
            "BATCH: 79\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69209985\n",
            "\n",
            "==========\n",
            "BATCH: 80\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69203727\n",
            "\n",
            "==========\n",
            "BATCH: 81\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69201561\n",
            "\n",
            "==========\n",
            "BATCH: 82\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69199448\n",
            "\n",
            "==========\n",
            "BATCH: 83\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69201229\n",
            "\n",
            "==========\n",
            "BATCH: 84\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69191575\n",
            "\n",
            "==========\n",
            "BATCH: 85\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69189654\n",
            "\n",
            "==========\n",
            "BATCH: 86\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69195196\n",
            "\n",
            "==========\n",
            "BATCH: 87\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69182277\n",
            "\n",
            "==========\n",
            "BATCH: 88\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69180527\n",
            "\n",
            "==========\n",
            "BATCH: 89\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69175232\n",
            "\n",
            "==========\n",
            "BATCH: 90\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69177144\n",
            "\n",
            "==========\n",
            "BATCH: 91\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69179014\n",
            "\n",
            "==========\n",
            "BATCH: 92\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69173908\n",
            "\n",
            "==========\n",
            "BATCH: 93\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69175772\n",
            "\n",
            "==========\n",
            "BATCH: 94\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69180990\n",
            "\n",
            "==========\n",
            "BATCH: 95\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70623243\n",
            "test_loss_avg: 0.69196172\n",
            "\n",
            "==========\n",
            "BATCH: 96\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69197746\n",
            "\n",
            "==========\n",
            "BATCH: 97\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69205864\n",
            "\n",
            "==========\n",
            "BATCH: 98\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69200797\n",
            "\n",
            "==========\n",
            "BATCH: 99\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.67752302\n",
            "test_loss_avg: 0.69186166\n",
            "\n",
            "==========\n",
            "BATCH: 100\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69181397\n",
            "\n",
            "==========\n",
            "BATCH: 101\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69186198\n",
            "\n",
            "==========\n",
            "BATCH: 102\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69187777\n",
            "\n",
            "==========\n",
            "BATCH: 103\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69180035\n",
            "\n",
            "==========\n",
            "BATCH: 104\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69178575\n",
            "\n",
            "==========\n",
            "BATCH: 105\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69180182\n",
            "\n",
            "==========\n",
            "BATCH: 106\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69178749\n",
            "\n",
            "==========\n",
            "BATCH: 107\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69189268\n",
            "\n",
            "==========\n",
            "BATCH: 108\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69193684\n",
            "\n",
            "==========\n",
            "BATCH: 109\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69189240\n",
            "\n",
            "==========\n",
            "BATCH: 110\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69187777\n",
            "\n",
            "==========\n",
            "BATCH: 111\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69194962\n",
            "\n",
            "==========\n",
            "BATCH: 112\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69190625\n",
            "\n",
            "==========\n",
            "BATCH: 113\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69194835\n",
            "\n",
            "==========\n",
            "BATCH: 114\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69198970\n",
            "\n",
            "==========\n",
            "BATCH: 115\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69194712\n",
            "\n",
            "==========\n",
            "BATCH: 116\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69198777\n",
            "\n",
            "==========\n",
            "BATCH: 117\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69197320\n",
            "\n",
            "==========\n",
            "BATCH: 118\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69193184\n",
            "\n",
            "==========\n",
            "BATCH: 119\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69191798\n",
            "\n",
            "==========\n",
            "BATCH: 120\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69185119\n",
            "\n",
            "==========\n",
            "BATCH: 121\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69194368\n",
            "\n",
            "==========\n",
            "BATCH: 122\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028276\n",
            "test_loss_avg: 0.69193007\n",
            "\n",
            "==========\n",
            "BATCH: 123\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69186480\n",
            "\n",
            "==========\n",
            "BATCH: 124\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69180060\n",
            "\n",
            "==========\n",
            "BATCH: 125\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69183949\n",
            "\n",
            "==========\n",
            "BATCH: 126\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69185245\n",
            "\n",
            "==========\n",
            "BATCH: 127\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69186521\n",
            "\n",
            "==========\n",
            "BATCH: 128\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69182793\n",
            "\n",
            "==========\n",
            "BATCH: 129\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69186541\n",
            "\n",
            "==========\n",
            "BATCH: 130\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69185323\n",
            "\n",
            "==========\n",
            "BATCH: 131\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69181690\n",
            "\n",
            "==========\n",
            "BATCH: 132\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69185361\n",
            "\n",
            "==========\n",
            "BATCH: 133\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69184180\n",
            "\n",
            "==========\n",
            "BATCH: 134\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69185397\n",
            "\n",
            "==========\n",
            "BATCH: 135\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69188959\n",
            "\n",
            "==========\n",
            "BATCH: 136\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69197159\n",
            "\n",
            "==========\n",
            "BATCH: 137\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69198255\n",
            "\n",
            "==========\n",
            "BATCH: 138\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69201646\n",
            "\n",
            "==========\n",
            "BATCH: 139\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69200399\n",
            "\n",
            "==========\n",
            "BATCH: 140\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69196891\n",
            "\n",
            "==========\n",
            "BATCH: 141\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69193433\n",
            "\n",
            "==========\n",
            "BATCH: 142\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69192270\n",
            "\n",
            "==========\n",
            "BATCH: 143\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69188893\n",
            "\n",
            "==========\n",
            "BATCH: 144\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69189993\n",
            "\n",
            "==========\n",
            "BATCH: 145\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69191077\n",
            "\n",
            "==========\n",
            "BATCH: 146\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69187777\n",
            "\n",
            "==========\n",
            "BATCH: 147\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69186692\n",
            "\n",
            "==========\n",
            "BATCH: 148\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69183467\n",
            "\n",
            "==========\n",
            "BATCH: 149\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69180284\n",
            "\n",
            "==========\n",
            "BATCH: 150\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69179271\n",
            "\n",
            "==========\n",
            "BATCH: 151\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69184609\n",
            "\n",
            "==========\n",
            "BATCH: 152\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69189876\n",
            "\n",
            "==========\n",
            "BATCH: 153\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69184650\n",
            "\n",
            "==========\n",
            "BATCH: 154\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69187777\n",
            "\n",
            "==========\n",
            "BATCH: 155\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69190864\n",
            "\n",
            "==========\n",
            "BATCH: 156\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69198001\n",
            "\n",
            "==========\n",
            "BATCH: 157\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69205048\n",
            "\n",
            "==========\n",
            "BATCH: 158\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69203929\n",
            "\n",
            "==========\n",
            "BATCH: 159\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69208843\n",
            "\n",
            "==========\n",
            "BATCH: 160\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69203727\n",
            "\n",
            "==========\n",
            "BATCH: 161\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69206600\n",
            "\n",
            "==========\n",
            "BATCH: 162\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69211406\n",
            "\n",
            "==========\n",
            "BATCH: 163\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69212240\n",
            "\n",
            "==========\n",
            "BATCH: 164\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69211118\n",
            "\n",
            "==========\n",
            "BATCH: 165\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69210010\n",
            "\n",
            "==========\n",
            "BATCH: 166\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69210837\n",
            "\n",
            "==========\n",
            "BATCH: 167\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69207834\n",
            "\n",
            "==========\n",
            "BATCH: 168\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69210562\n",
            "\n",
            "==========\n",
            "BATCH: 169\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69211371\n",
            "\n",
            "==========\n",
            "BATCH: 170\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69206542\n",
            "\n",
            "==========\n",
            "BATCH: 171\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69211095\n",
            "\n",
            "==========\n",
            "BATCH: 172\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69213742\n",
            "\n",
            "==========\n",
            "BATCH: 173\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69216358\n",
            "\n",
            "==========\n",
            "BATCH: 174\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69217110\n",
            "\n",
            "==========\n",
            "BATCH: 175\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.67752302\n",
            "test_loss_avg: 0.69208740\n",
            "\n",
            "==========\n",
            "BATCH: 176\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347268\n",
            "test_loss_avg: 0.69209527\n",
            "\n",
            "==========\n",
            "BATCH: 177\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69212107\n",
            "\n",
            "==========\n",
            "BATCH: 178\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69211074\n",
            "\n",
            "==========\n",
            "BATCH: 179\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69210053\n",
            "\n",
            "==========\n",
            "BATCH: 180\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69212588\n",
            "\n",
            "==========\n",
            "BATCH: 181\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69206282\n",
            "\n",
            "==========\n",
            "BATCH: 182\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69207057\n",
            "\n",
            "==========\n",
            "BATCH: 183\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69204337\n",
            "\n",
            "==========\n",
            "BATCH: 184\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69203380\n",
            "\n",
            "==========\n",
            "BATCH: 185\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69202434\n",
            "\n",
            "==========\n",
            "BATCH: 186\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69203212\n",
            "\n",
            "==========\n",
            "BATCH: 187\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69202277\n",
            "\n",
            "==========\n",
            "BATCH: 188\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69206442\n",
            "\n",
            "==========\n",
            "BATCH: 189\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69210562\n",
            "\n",
            "==========\n",
            "BATCH: 190\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69216319\n",
            "\n",
            "==========\n",
            "BATCH: 191\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69217004\n",
            "\n",
            "==========\n",
            "BATCH: 192\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69222667\n",
            "\n",
            "==========\n",
            "BATCH: 193\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.67752302\n",
            "test_loss_avg: 0.69215049\n",
            "\n",
            "==========\n",
            "BATCH: 194\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69212442\n",
            "\n",
            "==========\n",
            "BATCH: 195\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.67433310\n",
            "test_loss_avg: 0.69203318\n",
            "\n",
            "==========\n",
            "BATCH: 196\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69207307\n",
            "\n",
            "==========\n",
            "BATCH: 197\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69211256\n",
            "\n",
            "==========\n",
            "BATCH: 198\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69211943\n",
            "\n",
            "==========\n",
            "BATCH: 199\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69215829\n",
            "\n",
            "==========\n",
            "BATCH: 200\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69218081\n",
            "\n",
            "==========\n",
            "BATCH: 201\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69220311\n",
            "\n",
            "==========\n",
            "BATCH: 202\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69222519\n",
            "\n",
            "==========\n",
            "BATCH: 203\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69221562\n",
            "\n",
            "==========\n",
            "BATCH: 204\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69226869\n",
            "\n",
            "==========\n",
            "BATCH: 205\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69227457\n",
            "\n",
            "==========\n",
            "BATCH: 206\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69224941\n",
            "\n",
            "==========\n",
            "BATCH: 207\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69225532\n",
            "\n",
            "==========\n",
            "BATCH: 208\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69219983\n",
            "\n",
            "==========\n",
            "BATCH: 209\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69219066\n",
            "\n",
            "==========\n",
            "BATCH: 210\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69218157\n",
            "\n",
            "==========\n",
            "BATCH: 211\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69217258\n",
            "\n",
            "==========\n",
            "BATCH: 212\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69213357\n",
            "\n",
            "==========\n",
            "BATCH: 213\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69216981\n",
            "\n",
            "==========\n",
            "BATCH: 214\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69214608\n",
            "\n",
            "==========\n",
            "BATCH: 215\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69209291\n",
            "\n",
            "==========\n",
            "BATCH: 216\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69211406\n",
            "\n",
            "==========\n",
            "BATCH: 217\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69210562\n",
            "\n",
            "==========\n",
            "BATCH: 218\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.71261227\n",
            "test_loss_avg: 0.69219969\n",
            "\n",
            "==========\n",
            "BATCH: 219\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69220550\n",
            "\n",
            "==========\n",
            "BATCH: 220\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69221126\n",
            "\n",
            "==========\n",
            "BATCH: 221\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347268\n",
            "test_loss_avg: 0.69221697\n",
            "\n",
            "==========\n",
            "BATCH: 222\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69225137\n",
            "\n",
            "==========\n",
            "BATCH: 223\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69224254\n",
            "\n",
            "==========\n",
            "BATCH: 224\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69221955\n",
            "\n",
            "==========\n",
            "BATCH: 225\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69223930\n",
            "\n",
            "==========\n",
            "BATCH: 226\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69221652\n",
            "\n",
            "==========\n",
            "BATCH: 227\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69225016\n",
            "\n",
            "==========\n",
            "BATCH: 228\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69222754\n",
            "\n",
            "==========\n",
            "BATCH: 229\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69223298\n",
            "\n",
            "==========\n",
            "BATCH: 230\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69222450\n",
            "\n",
            "==========\n",
            "BATCH: 231\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69225752\n",
            "\n",
            "==========\n",
            "BATCH: 232\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69226276\n",
            "\n",
            "==========\n",
            "BATCH: 233\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69229534\n",
            "\n",
            "==========\n",
            "BATCH: 234\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69228674\n",
            "\n",
            "==========\n",
            "BATCH: 235\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69225106\n",
            "\n",
            "==========\n",
            "BATCH: 236\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69226975\n",
            "\n",
            "==========\n",
            "BATCH: 237\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69231521\n",
            "\n",
            "==========\n",
            "BATCH: 238\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69233348\n",
            "\n",
            "==========\n",
            "BATCH: 239\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69231155\n",
            "\n",
            "==========\n",
            "BATCH: 240\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69228980\n",
            "\n",
            "==========\n",
            "BATCH: 241\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69228148\n",
            "\n",
            "==========\n",
            "BATCH: 242\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69227322\n",
            "\n",
            "==========\n",
            "BATCH: 243\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69227815\n",
            "\n",
            "==========\n",
            "BATCH: 244\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69230920\n",
            "\n",
            "==========\n",
            "BATCH: 245\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69226187\n",
            "\n",
            "==========\n",
            "BATCH: 246\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69229272\n",
            "\n",
            "==========\n",
            "BATCH: 247\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69228458\n",
            "\n",
            "==========\n",
            "BATCH: 248\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69225079\n",
            "\n",
            "==========\n",
            "BATCH: 249\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071294\n",
            "test_loss_avg: 0.69220445\n",
            "\n",
            "==========\n",
            "BATCH: 250\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69217125\n",
            "\n",
            "==========\n",
            "BATCH: 251\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69220185\n",
            "\n",
            "==========\n",
            "BATCH: 252\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69220689\n",
            "\n",
            "==========\n",
            "BATCH: 253\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69219929\n",
            "\n",
            "==========\n",
            "BATCH: 254\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69219174\n",
            "\n",
            "==========\n",
            "BATCH: 255\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69220927\n",
            "\n",
            "==========\n",
            "BATCH: 256\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390298\n",
            "test_loss_avg: 0.69217683\n",
            "\n",
            "==========\n",
            "BATCH: 257\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69219428\n",
            "\n",
            "==========\n",
            "BATCH: 258\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69216215\n",
            "\n",
            "==========\n",
            "BATCH: 259\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69214257\n",
            "\n",
            "==========\n",
            "BATCH: 260\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69212315\n",
            "\n",
            "==========\n",
            "BATCH: 261\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69212832\n",
            "\n",
            "==========\n",
            "BATCH: 262\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69215780\n",
            "\n",
            "==========\n",
            "BATCH: 263\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69217493\n",
            "\n",
            "==========\n",
            "BATCH: 264\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69214360\n",
            "\n",
            "==========\n",
            "BATCH: 265\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69211250\n",
            "\n",
            "==========\n",
            "BATCH: 266\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69215359\n",
            "\n",
            "==========\n",
            "BATCH: 267\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69215853\n",
            "\n",
            "==========\n",
            "BATCH: 268\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69217534\n",
            "\n",
            "==========\n",
            "BATCH: 269\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69218016\n",
            "\n",
            "==========\n",
            "BATCH: 270\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69218495\n",
            "\n",
            "==========\n",
            "BATCH: 271\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69217793\n",
            "\n",
            "==========\n",
            "BATCH: 272\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69217096\n",
            "\n",
            "==========\n",
            "BATCH: 273\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69216405\n",
            "\n",
            "==========\n",
            "BATCH: 274\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69215718\n",
            "\n",
            "==========\n",
            "BATCH: 275\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69219676\n",
            "\n",
            "==========\n",
            "BATCH: 276\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70623243\n",
            "test_loss_avg: 0.69224762\n",
            "\n",
            "==========\n",
            "BATCH: 277\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69226356\n",
            "\n",
            "==========\n",
            "BATCH: 278\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69225643\n",
            "\n",
            "==========\n",
            "BATCH: 279\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69223793\n",
            "\n",
            "==========\n",
            "BATCH: 280\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69221955\n",
            "\n",
            "==========\n",
            "BATCH: 281\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69223536\n",
            "\n",
            "==========\n",
            "BATCH: 282\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69223975\n",
            "\n",
            "==========\n",
            "BATCH: 283\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69225538\n",
            "\n",
            "==========\n",
            "BATCH: 284\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69225966\n",
            "\n",
            "==========\n",
            "BATCH: 285\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69227511\n",
            "\n",
            "==========\n",
            "BATCH: 286\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69230161\n",
            "\n",
            "==========\n",
            "BATCH: 287\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69228346\n",
            "\n",
            "==========\n",
            "BATCH: 288\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69227651\n",
            "\n",
            "==========\n",
            "BATCH: 289\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.67752308\n",
            "test_loss_avg: 0.69222546\n",
            "\n",
            "==========\n",
            "BATCH: 290\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69220776\n",
            "\n",
            "==========\n",
            "BATCH: 291\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69221211\n",
            "\n",
            "==========\n",
            "BATCH: 292\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70304251\n",
            "test_loss_avg: 0.69224920\n",
            "\n",
            "==========\n",
            "BATCH: 293\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69223160\n",
            "\n",
            "==========\n",
            "BATCH: 294\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69219242\n",
            "\n",
            "==========\n",
            "BATCH: 295\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69218595\n",
            "\n",
            "==========\n",
            "BATCH: 296\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69216875\n",
            "\n",
            "==========\n",
            "BATCH: 297\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69216240\n",
            "\n",
            "==========\n",
            "BATCH: 298\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69215609\n",
            "\n",
            "==========\n",
            "BATCH: 299\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69216049\n",
            "\n",
            "==========\n",
            "BATCH: 300\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69214360\n",
            "\n",
            "==========\n",
            "BATCH: 301\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69215861\n",
            "\n",
            "==========\n",
            "BATCH: 302\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69218409\n",
            "\n",
            "==========\n",
            "BATCH: 303\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69218834\n",
            "\n",
            "==========\n",
            "BATCH: 304\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69218207\n",
            "\n",
            "==========\n",
            "BATCH: 305\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69217585\n",
            "\n",
            "==========\n",
            "BATCH: 306\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.67433310\n",
            "test_loss_avg: 0.69211754\n",
            "\n",
            "==========\n",
            "BATCH: 307\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69211156\n",
            "\n",
            "==========\n",
            "BATCH: 308\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028276\n",
            "test_loss_avg: 0.69210562\n",
            "\n",
            "==========\n",
            "BATCH: 309\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69211005\n",
            "\n",
            "==========\n",
            "BATCH: 310\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69209386\n",
            "\n",
            "==========\n",
            "BATCH: 311\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69205727\n",
            "\n",
            "==========\n",
            "BATCH: 312\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69207203\n",
            "\n",
            "==========\n",
            "BATCH: 313\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69208670\n",
            "\n",
            "==========\n",
            "BATCH: 314\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69210127\n",
            "\n",
            "==========\n",
            "BATCH: 315\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69211575\n",
            "\n",
            "==========\n",
            "BATCH: 316\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69210995\n",
            "\n",
            "==========\n",
            "BATCH: 317\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69210419\n",
            "\n",
            "==========\n",
            "BATCH: 318\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.71580219\n",
            "test_loss_avg: 0.69217871\n",
            "\n",
            "==========\n",
            "BATCH: 319\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028276\n",
            "test_loss_avg: 0.69217277\n",
            "\n",
            "==========\n",
            "BATCH: 320\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69217683\n",
            "\n",
            "==========\n",
            "BATCH: 321\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69219080\n",
            "\n",
            "==========\n",
            "BATCH: 322\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69219478\n",
            "\n",
            "==========\n",
            "BATCH: 323\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68071300\n",
            "test_loss_avg: 0.69215924\n",
            "\n",
            "==========\n",
            "BATCH: 324\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69216329\n",
            "\n",
            "==========\n",
            "BATCH: 325\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69214769\n",
            "\n",
            "==========\n",
            "BATCH: 326\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69215175\n",
            "\n",
            "==========\n",
            "BATCH: 327\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69214604\n",
            "\n",
            "==========\n",
            "BATCH: 328\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69212091\n",
            "\n",
            "==========\n",
            "BATCH: 329\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70942235\n",
            "test_loss_avg: 0.69217349\n",
            "\n",
            "==========\n",
            "BATCH: 330\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.70623243\n",
            "test_loss_avg: 0.69221610\n",
            "\n",
            "==========\n",
            "BATCH: 331\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69221989\n",
            "\n",
            "==========\n",
            "BATCH: 332\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69223328\n",
            "\n",
            "==========\n",
            "BATCH: 333\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69225616\n",
            "\n",
            "==========\n",
            "BATCH: 334\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69226935\n",
            "\n",
            "==========\n",
            "BATCH: 335\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69227294\n",
            "\n",
            "==========\n",
            "BATCH: 336\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69227651\n",
            "\n",
            "==========\n",
            "BATCH: 337\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69228953\n",
            "\n",
            "==========\n",
            "BATCH: 338\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69229303\n",
            "\n",
            "==========\n",
            "BATCH: 339\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709284\n",
            "test_loss_avg: 0.69227769\n",
            "\n",
            "==========\n",
            "BATCH: 340\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69227182\n",
            "\n",
            "==========\n",
            "BATCH: 341\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69225663\n",
            "\n",
            "==========\n",
            "BATCH: 342\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68390292\n",
            "test_loss_avg: 0.69223221\n",
            "\n",
            "==========\n",
            "BATCH: 343\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69221722\n",
            "\n",
            "==========\n",
            "BATCH: 344\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69223942\n",
            "\n",
            "==========\n",
            "BATCH: 345\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69222450\n",
            "\n",
            "==========\n",
            "BATCH: 346\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68709290\n",
            "test_loss_avg: 0.69220967\n",
            "\n",
            "==========\n",
            "BATCH: 347\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69222250\n",
            "\n",
            "==========\n",
            "BATCH: 348\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69223526\n",
            "\n",
            "==========\n",
            "BATCH: 349\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69224795\n",
            "\n",
            "==========\n",
            "BATCH: 350\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69224233\n",
            "\n",
            "==========\n",
            "BATCH: 351\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69225493\n",
            "\n",
            "==========\n",
            "BATCH: 352\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69224933\n",
            "\n",
            "==========\n",
            "BATCH: 353\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69224376\n",
            "\n",
            "==========\n",
            "BATCH: 354\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69666266\n",
            "test_loss_avg: 0.69225624\n",
            "\n",
            "==========\n",
            "BATCH: 355\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69028282\n",
            "test_loss_avg: 0.69225068\n",
            "\n",
            "==========\n",
            "BATCH: 356\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69985259\n",
            "test_loss_avg: 0.69227203\n",
            "\n",
            "==========\n",
            "BATCH: 357\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.69347274\n",
            "test_loss_avg: 0.69227540\n",
            "\n",
            "==========\n",
            "BATCH: 358\n",
            "==========\n",
            "f1: 0.00000000\n",
            "loss_item:0.68921947\n",
            "test_loss_avg: 0.69226686\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_data_SOL= test_bert(bert_sol, bert_testLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b-4KmzDq8Fu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5jcQMwbq8Fu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DZKu5pSq8Fu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm2TKKq3q8Fu"
      },
      "outputs": [],
      "source": [
        "export(test_data_BUPO,base_path+f\"BERT/TEST_DATA/bupo_test_data_{MAX_LENGTH}_{bert_bupo_attempt}.pkl\")\n",
        "export(test_data_SOL,base_path+f\"BERT/TEST_DATA/sol_test_data_{MAX_LENGTH}_{bert_sol_attempt}.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M97OXP5jq8Fu"
      },
      "source": [
        "## ENTER INPUT MANUALLY TO CHECK SARCASM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cilmi3b1q8Fv"
      },
      "outputs": [],
      "source": [
        "def check_sarcasm(txt,model):\n",
        "    print(\"Text: \", txt)\n",
        "    cleanTxt= clean_text(txt)\n",
        "    tokenized_txt= word_tokenize(cleanTxt)\n",
        "    length_diff= MAX_LENGTH-tokenized_txt.__len__()\n",
        "\n",
        "    if length_diff >= 0 :\n",
        "        tokenized_txt.extend([PAD_TOKEN]*length_diff)\n",
        "    else: #truncate if length_diff is negative, i.e, tokenized_text should not be longer than max length\n",
        "        tokenized_txt = tokenized_txt[:length_diff]\n",
        "    #print(\"tokenized text: \", tokenized_txt)\n",
        "    embeds= glove_emb.get_vecs_by_tokens(tokenized_txt)\n",
        "    embeds= embeds.view(1,MAX_LENGTH,input_size)\n",
        "    output = model(embeds)\n",
        "    sig = nn.Sigmoid()(output)\n",
        "    label = 'sarcastic' if sig>=0.5 else 'not_sarcastic'\n",
        "    print(\"pred val: \", sig.item())\n",
        "    print(\"Prediction: \", label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YhE9spuq8Fv",
        "outputId": "7e07e0f5-37f9-4150-b296-fa35528221b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text:  Yeah sure \n",
            "pred val:  0.9623934030532837\n",
            "Prediction:  sarcastic\n"
          ]
        }
      ],
      "source": [
        "check_sarcasm('', lstm_BD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i2IAzpMq8Fv",
        "outputId": "1051365d-aca8-4c29-e6f7-91d072426626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[8,\n",
              " 13,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 3,\n",
              " 9,\n",
              " 15,\n",
              " 12,\n",
              " 6,\n",
              " 14,\n",
              " 10,\n",
              " 8,\n",
              " 17,\n",
              " 9,\n",
              " 11,\n",
              " 10,\n",
              " 9,\n",
              " 6,\n",
              " 13,\n",
              " 11,\n",
              " 14,\n",
              " 6,\n",
              " 11,\n",
              " 9,\n",
              " 12,\n",
              " 7,\n",
              " 10,\n",
              " 12,\n",
              " 19,\n",
              " 7,\n",
              " 4,\n",
              " 6,\n",
              " 14,\n",
              " 10,\n",
              " 7,\n",
              " 18,\n",
              " 12,\n",
              " 17,\n",
              " 7,\n",
              " 10,\n",
              " 13,\n",
              " 11,\n",
              " 9,\n",
              " 11,\n",
              " 7,\n",
              " 10,\n",
              " 7,\n",
              " 14,\n",
              " 8,\n",
              " 15,\n",
              " 8,\n",
              " 12,\n",
              " 4,\n",
              " 14,\n",
              " 7,\n",
              " 9,\n",
              " 9,\n",
              " 12,\n",
              " 9,\n",
              " 12,\n",
              " 4,\n",
              " 8,\n",
              " 12,\n",
              " 7,\n",
              " 14,\n",
              " 7,\n",
              " 12,\n",
              " 11,\n",
              " 14,\n",
              " 10,\n",
              " 9,\n",
              " 10,\n",
              " 9,\n",
              " 13,\n",
              " 11,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 16,\n",
              " 14,\n",
              " 12,\n",
              " 14,\n",
              " 13,\n",
              " 12,\n",
              " 10,\n",
              " 6,\n",
              " 12,\n",
              " 13,\n",
              " 13,\n",
              " 12,\n",
              " 9,\n",
              " 5,\n",
              " 15,\n",
              " 8,\n",
              " 9,\n",
              " 19,\n",
              " 8,\n",
              " 12,\n",
              " 7,\n",
              " 14,\n",
              " 9,\n",
              " 9,\n",
              " 13,\n",
              " 6,\n",
              " 21,\n",
              " 12,\n",
              " 10,\n",
              " 8,\n",
              " 9,\n",
              " 7,\n",
              " 12,\n",
              " 10,\n",
              " 7,\n",
              " 9,\n",
              " 10,\n",
              " 10,\n",
              " 15,\n",
              " 11,\n",
              " 6,\n",
              " 13,\n",
              " 4,\n",
              " 13,\n",
              " 8,\n",
              " 11,\n",
              " 12,\n",
              " 17,\n",
              " 12,\n",
              " 17,\n",
              " 10,\n",
              " 13,\n",
              " 5,\n",
              " 14,\n",
              " 14,\n",
              " 14,\n",
              " 18,\n",
              " 10,\n",
              " 17,\n",
              " 12,\n",
              " 7,\n",
              " 14,\n",
              " 11,\n",
              " 7,\n",
              " 18,\n",
              " 16,\n",
              " 11,\n",
              " 12,\n",
              " 12,\n",
              " 12,\n",
              " 16,\n",
              " 8,\n",
              " 14,\n",
              " 8,\n",
              " 14,\n",
              " 13,\n",
              " 10,\n",
              " 9,\n",
              " 13,\n",
              " 5,\n",
              " 6,\n",
              " 10,\n",
              " 6,\n",
              " 18,\n",
              " 12,\n",
              " 12,\n",
              " 6,\n",
              " 9,\n",
              " 9,\n",
              " 7,\n",
              " 7,\n",
              " 6,\n",
              " 17,\n",
              " 5,\n",
              " 10,\n",
              " 9,\n",
              " 13,\n",
              " 12,\n",
              " 11,\n",
              " 8,\n",
              " 14,\n",
              " 7,\n",
              " 8,\n",
              " 16,\n",
              " 22,\n",
              " 10,\n",
              " 10,\n",
              " 14,\n",
              " 5,\n",
              " 21,\n",
              " 9,\n",
              " 10,\n",
              " 13,\n",
              " 11,\n",
              " 4,\n",
              " 10,\n",
              " 15,\n",
              " 12,\n",
              " 10,\n",
              " 14,\n",
              " 15,\n",
              " 10,\n",
              " 12,\n",
              " 10,\n",
              " 16,\n",
              " 12,\n",
              " 9,\n",
              " 12,\n",
              " 14,\n",
              " 14,\n",
              " 7,\n",
              " 19,\n",
              " 10,\n",
              " 11,\n",
              " 15,\n",
              " 10,\n",
              " 9,\n",
              " 9,\n",
              " 6,\n",
              " 7,\n",
              " 6,\n",
              " 9,\n",
              " 12,\n",
              " 11,\n",
              " 13,\n",
              " 10,\n",
              " 5,\n",
              " 13,\n",
              " 8,\n",
              " 12,\n",
              " 3,\n",
              " 5,\n",
              " 7,\n",
              " 11,\n",
              " 13,\n",
              " 8,\n",
              " 13,\n",
              " 10,\n",
              " 20,\n",
              " 9,\n",
              " 13,\n",
              " 9,\n",
              " 11,\n",
              " 10,\n",
              " 6,\n",
              " 11,\n",
              " 12,\n",
              " 11,\n",
              " 11,\n",
              " 11,\n",
              " 13,\n",
              " 12,\n",
              " 13,\n",
              " 9,\n",
              " 11,\n",
              " 10,\n",
              " 8,\n",
              " 8,\n",
              " 9,\n",
              " 5,\n",
              " 6,\n",
              " 13,\n",
              " 6,\n",
              " 10,\n",
              " 11,\n",
              " 11,\n",
              " 11,\n",
              " 10,\n",
              " 15,\n",
              " 10,\n",
              " 3,\n",
              " 8,\n",
              " 9,\n",
              " 8,\n",
              " 12,\n",
              " 17,\n",
              " 10,\n",
              " 4,\n",
              " 11,\n",
              " 8,\n",
              " 8,\n",
              " 11,\n",
              " 10,\n",
              " 16,\n",
              " 11,\n",
              " 8,\n",
              " 5,\n",
              " 9,\n",
              " 11,\n",
              " 15,\n",
              " 20,\n",
              " 9,\n",
              " 11,\n",
              " 8,\n",
              " 10,\n",
              " 7,\n",
              " 15,\n",
              " 9,\n",
              " 13,\n",
              " 9,\n",
              " 5,\n",
              " 8,\n",
              " 8,\n",
              " 5,\n",
              " 12,\n",
              " 14,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 11,\n",
              " 11,\n",
              " 14,\n",
              " 7,\n",
              " 9,\n",
              " 11,\n",
              " 8,\n",
              " 10,\n",
              " 17,\n",
              " 10,\n",
              " 7,\n",
              " 11,\n",
              " 12,\n",
              " 12,\n",
              " 12,\n",
              " 11,\n",
              " 9,\n",
              " 11,\n",
              " 9,\n",
              " 14,\n",
              " 13,\n",
              " 13,\n",
              " 13,\n",
              " 12,\n",
              " 11,\n",
              " 8,\n",
              " 6,\n",
              " 7,\n",
              " 12,\n",
              " 9,\n",
              " 11,\n",
              " 13,\n",
              " 12,\n",
              " 17,\n",
              " 14,\n",
              " 11,\n",
              " 13,\n",
              " 6,\n",
              " 11,\n",
              " 9,\n",
              " 14,\n",
              " 12,\n",
              " 9,\n",
              " 9,\n",
              " 5,\n",
              " 13,\n",
              " 17,\n",
              " 9,\n",
              " 11,\n",
              " 8,\n",
              " 10,\n",
              " 8,\n",
              " 9,\n",
              " 13,\n",
              " 18,\n",
              " 3,\n",
              " 7,\n",
              " 14,\n",
              " 9,\n",
              " 12,\n",
              " 11,\n",
              " 10,\n",
              " 15,\n",
              " 11,\n",
              " 9,\n",
              " 12,\n",
              " 7,\n",
              " 11,\n",
              " 13,\n",
              " 9,\n",
              " 17,\n",
              " 12,\n",
              " 12,\n",
              " 18,\n",
              " 5,\n",
              " 7,\n",
              " 12,\n",
              " 15,\n",
              " 10,\n",
              " 14,\n",
              " 7,\n",
              " 12,\n",
              " 13,\n",
              " 7,\n",
              " 15,\n",
              " 5,\n",
              " 13,\n",
              " 10,\n",
              " 14,\n",
              " 10,\n",
              " 10,\n",
              " 13,\n",
              " 8,\n",
              " 9,\n",
              " 8,\n",
              " 11,\n",
              " 10,\n",
              " 10,\n",
              " 14,\n",
              " 13,\n",
              " 12,\n",
              " 13,\n",
              " 12,\n",
              " 14,\n",
              " 11,\n",
              " 14,\n",
              " 10,\n",
              " 10,\n",
              " 11,\n",
              " 9,\n",
              " 11,\n",
              " 14,\n",
              " 14,\n",
              " 13,\n",
              " 10,\n",
              " 13,\n",
              " 7,\n",
              " 6,\n",
              " 8,\n",
              " 18,\n",
              " 9,\n",
              " 9,\n",
              " 13,\n",
              " 10,\n",
              " 8,\n",
              " 16,\n",
              " 13,\n",
              " 7,\n",
              " 6,\n",
              " 6,\n",
              " 14,\n",
              " 8,\n",
              " 8,\n",
              " 6,\n",
              " 11,\n",
              " 9,\n",
              " 15,\n",
              " 12,\n",
              " 11,\n",
              " 13,\n",
              " 11,\n",
              " 10,\n",
              " 10,\n",
              " 7,\n",
              " 16,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 8,\n",
              " 13,\n",
              " 12,\n",
              " 13,\n",
              " 9,\n",
              " 11,\n",
              " 14,\n",
              " 6,\n",
              " 7,\n",
              " 7,\n",
              " 16,\n",
              " 10,\n",
              " 14,\n",
              " 11,\n",
              " 10,\n",
              " 6,\n",
              " 10,\n",
              " 10,\n",
              " 17,\n",
              " 10,\n",
              " 15,\n",
              " 13,\n",
              " 10,\n",
              " 9,\n",
              " 11,\n",
              " 9,\n",
              " 11,\n",
              " 7,\n",
              " 5,\n",
              " 11,\n",
              " 8,\n",
              " 12,\n",
              " 10,\n",
              " 2,\n",
              " 7,\n",
              " 7,\n",
              " 9,\n",
              " 9,\n",
              " 4,\n",
              " 8,\n",
              " 10,\n",
              " 3,\n",
              " 8,\n",
              " 11,\n",
              " 9,\n",
              " 16,\n",
              " 7,\n",
              " 8,\n",
              " 5,\n",
              " 12,\n",
              " 10,\n",
              " 14,\n",
              " 8,\n",
              " 9,\n",
              " 8,\n",
              " 11,\n",
              " 4,\n",
              " 5,\n",
              " 11,\n",
              " 13,\n",
              " 10,\n",
              " 12,\n",
              " 11,\n",
              " 10,\n",
              " 8,\n",
              " 7,\n",
              " 9,\n",
              " 10,\n",
              " 13,\n",
              " 8,\n",
              " 10,\n",
              " 8,\n",
              " 12,\n",
              " 12,\n",
              " 11,\n",
              " 14,\n",
              " 6,\n",
              " 17,\n",
              " 4,\n",
              " 7,\n",
              " 9,\n",
              " 5,\n",
              " 12,\n",
              " 19,\n",
              " 6,\n",
              " 12,\n",
              " 7,\n",
              " 10,\n",
              " 12,\n",
              " 9,\n",
              " 14,\n",
              " 9,\n",
              " 10,\n",
              " 7,\n",
              " 10,\n",
              " 10,\n",
              " 12,\n",
              " 17,\n",
              " 10,\n",
              " 6,\n",
              " 8,\n",
              " 9,\n",
              " 6,\n",
              " 19,\n",
              " 15,\n",
              " 16,\n",
              " 12,\n",
              " 20,\n",
              " 10,\n",
              " 8,\n",
              " 8,\n",
              " 11,\n",
              " 7,\n",
              " 9,\n",
              " 9,\n",
              " 9,\n",
              " 11,\n",
              " 11,\n",
              " 6,\n",
              " 5,\n",
              " 10,\n",
              " 7,\n",
              " 13,\n",
              " 8,\n",
              " 12,\n",
              " 16,\n",
              " 15,\n",
              " 22,\n",
              " 11,\n",
              " 8,\n",
              " 12,\n",
              " 11,\n",
              " 9,\n",
              " 13,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 15,\n",
              " 9,\n",
              " 12,\n",
              " 8,\n",
              " 14,\n",
              " 12,\n",
              " 11,\n",
              " 9,\n",
              " 14,\n",
              " 10,\n",
              " 11,\n",
              " 7,\n",
              " 10,\n",
              " 11,\n",
              " 7,\n",
              " 11,\n",
              " 13,\n",
              " 8,\n",
              " 12,\n",
              " 16,\n",
              " 12,\n",
              " 5,\n",
              " 10,\n",
              " 10,\n",
              " 9,\n",
              " 8,\n",
              " 21,\n",
              " 12,\n",
              " 19,\n",
              " 12,\n",
              " 7,\n",
              " 9,\n",
              " 4,\n",
              " 11,\n",
              " 12,\n",
              " 11,\n",
              " 8,\n",
              " 9,\n",
              " 16,\n",
              " 7,\n",
              " 13,\n",
              " 10,\n",
              " 9,\n",
              " 11,\n",
              " 9,\n",
              " 15,\n",
              " 7,\n",
              " 11,\n",
              " 15,\n",
              " 5,\n",
              " 16,\n",
              " 7,\n",
              " 8,\n",
              " 4,\n",
              " 7,\n",
              " 11,\n",
              " 11,\n",
              " 15,\n",
              " 4,\n",
              " 14,\n",
              " 14,\n",
              " 9,\n",
              " 12,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 12,\n",
              " 5,\n",
              " 13,\n",
              " 14,\n",
              " 12,\n",
              " 9,\n",
              " 6,\n",
              " 10,\n",
              " 9,\n",
              " 7,\n",
              " 8,\n",
              " 12,\n",
              " 7,\n",
              " 7,\n",
              " 12,\n",
              " 13,\n",
              " 12,\n",
              " 11,\n",
              " 10,\n",
              " 11,\n",
              " 6,\n",
              " 10,\n",
              " 12,\n",
              " 11,\n",
              " 14,\n",
              " 14,\n",
              " 18,\n",
              " 12,\n",
              " 14,\n",
              " 10,\n",
              " 11,\n",
              " 10,\n",
              " 10,\n",
              " 5,\n",
              " 5,\n",
              " 8,\n",
              " 15,\n",
              " 11,\n",
              " 15,\n",
              " 9,\n",
              " 7,\n",
              " 13,\n",
              " 4,\n",
              " 10,\n",
              " 12,\n",
              " 7,\n",
              " 7,\n",
              " 21,\n",
              " 12,\n",
              " 7,\n",
              " 11,\n",
              " 8,\n",
              " 12,\n",
              " 16,\n",
              " 9,\n",
              " 11,\n",
              " 11,\n",
              " 13,\n",
              " 13,\n",
              " 7,\n",
              " 18,\n",
              " 10,\n",
              " 15,\n",
              " 11,\n",
              " 13,\n",
              " 7,\n",
              " 12,\n",
              " 9,\n",
              " 7,\n",
              " 12,\n",
              " 9,\n",
              " 15,\n",
              " 8,\n",
              " 15,\n",
              " 5,\n",
              " 12,\n",
              " 8,\n",
              " 6,\n",
              " 8,\n",
              " 13,\n",
              " 9,\n",
              " 10,\n",
              " 7,\n",
              " 13,\n",
              " 12,\n",
              " 15,\n",
              " 9,\n",
              " 4,\n",
              " 10,\n",
              " 13,\n",
              " 15,\n",
              " 9,\n",
              " 14,\n",
              " 11,\n",
              " 7,\n",
              " 15,\n",
              " 8,\n",
              " 15,\n",
              " 10,\n",
              " 10,\n",
              " 10,\n",
              " 9,\n",
              " 8,\n",
              " 8,\n",
              " 17,\n",
              " 12,\n",
              " 8,\n",
              " 3,\n",
              " 9,\n",
              " 11,\n",
              " 7,\n",
              " 17,\n",
              " 13,\n",
              " 9,\n",
              " 6,\n",
              " 10,\n",
              " 14,\n",
              " 6,\n",
              " 7,\n",
              " 10,\n",
              " 4,\n",
              " 10,\n",
              " 6,\n",
              " 12,\n",
              " 16,\n",
              " 12,\n",
              " 13,\n",
              " 8,\n",
              " 16,\n",
              " 17,\n",
              " 10,\n",
              " 16,\n",
              " 13,\n",
              " 10,\n",
              " 17,\n",
              " 11,\n",
              " 9,\n",
              " 10,\n",
              " 16,\n",
              " 6,\n",
              " 10,\n",
              " 10,\n",
              " 11,\n",
              " 9,\n",
              " 14,\n",
              " 12,\n",
              " 11,\n",
              " 8,\n",
              " 9,\n",
              " 14,\n",
              " 12,\n",
              " 10,\n",
              " 14,\n",
              " 12,\n",
              " 9,\n",
              " 13,\n",
              " 10,\n",
              " 12,\n",
              " 13,\n",
              " 7,\n",
              " 11,\n",
              " 14,\n",
              " 8,\n",
              " 10,\n",
              " 9,\n",
              " 13,\n",
              " 5,\n",
              " 11,\n",
              " 10,\n",
              " 9,\n",
              " 11,\n",
              " 13,\n",
              " 13,\n",
              " 11,\n",
              " 3,\n",
              " 14,\n",
              " 13,\n",
              " 10,\n",
              " 11,\n",
              " 13,\n",
              " 10,\n",
              " 8,\n",
              " 6,\n",
              " 11,\n",
              " 11,\n",
              " 14,\n",
              " 10,\n",
              " 16,\n",
              " 3,\n",
              " 9,\n",
              " 10,\n",
              " 7,\n",
              " 17,\n",
              " 11,\n",
              " 7,\n",
              " 10,\n",
              " 16,\n",
              " 10,\n",
              " 10,\n",
              " 14,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 11,\n",
              " 5,\n",
              " 10,\n",
              " 9,\n",
              " 13,\n",
              " 14,\n",
              " 9,\n",
              " 17,\n",
              " 13,\n",
              " 11,\n",
              " 8,\n",
              " 18,\n",
              " 15,\n",
              " 10,\n",
              " 12,\n",
              " 11,\n",
              " 8,\n",
              " 12,\n",
              " 13,\n",
              " 12,\n",
              " 9,\n",
              " 12,\n",
              " 11,\n",
              " 15,\n",
              " 13,\n",
              " 5,\n",
              " 11,\n",
              " 8,\n",
              " 10,\n",
              " 11,\n",
              " 7,\n",
              " 8,\n",
              " 10,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 11,\n",
              " 10,\n",
              " 11,\n",
              " 13,\n",
              " 6,\n",
              " 11,\n",
              " 15,\n",
              " 14,\n",
              " 10,\n",
              " 13,\n",
              " 14,\n",
              " 9,\n",
              " 11,\n",
              " 10,\n",
              " 11,\n",
              " 11,\n",
              " 7,\n",
              " 5,\n",
              " 8,\n",
              " 13,\n",
              " 10,\n",
              " 11,\n",
              " 10,\n",
              " 7,\n",
              " 12,\n",
              " 9,\n",
              " 9,\n",
              " 6,\n",
              " 9,\n",
              " 11,\n",
              " 14,\n",
              " 12,\n",
              " 9,\n",
              " 12,\n",
              " 7,\n",
              " 10,\n",
              " 12,\n",
              " 15,\n",
              " 6,\n",
              " 13,\n",
              " 6,\n",
              " 12,\n",
              " 12,\n",
              " 6,\n",
              " 10,\n",
              " 13,\n",
              " 10,\n",
              " 10,\n",
              " 8,\n",
              " 3,\n",
              " 9,\n",
              " 9,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 9,\n",
              " 16,\n",
              " 8,\n",
              " 8,\n",
              " 4,\n",
              " 14,\n",
              " 5,\n",
              " 9,\n",
              " 5,\n",
              " 11,\n",
              " 7,\n",
              " 11,\n",
              " 6,\n",
              " 5,\n",
              " 6,\n",
              " 12,\n",
              " 8,\n",
              " 5,\n",
              " 9,\n",
              " 10,\n",
              " 15,\n",
              " 11,\n",
              " 12,\n",
              " 14,\n",
              " 12,\n",
              " 19,\n",
              " 13,\n",
              " 9,\n",
              " 12,\n",
              " 17,\n",
              " 3,\n",
              " 6,\n",
              " 10,\n",
              " 6,\n",
              " 5,\n",
              " 8,\n",
              " 6,\n",
              " 4,\n",
              " 10,\n",
              " 12,\n",
              " 10,\n",
              " 17,\n",
              " 7,\n",
              " 9,\n",
              " 9,\n",
              " 12,\n",
              " ...]"
            ]
          },
          "execution_count": 872,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lengths=[]\n",
        "for comm in training_examples.comment:\n",
        "    txt = clean_text(comm)\n",
        "    toktext= word_tokenize(txt)\n",
        "    lengths.append(len(toktext))\n",
        "lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G3witQdq8Fv",
        "outputId": "c599d3f8-ffca-4882-b0ec-287ca931a95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "welp\n",
            "152\n",
            "welp\n",
            "41\n"
          ]
        }
      ],
      "source": [
        "for l in lengths:\n",
        "    if l>=40:\n",
        "        print(\"welp\")\n",
        "        print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0yVk1WOq8Fw",
        "outputId": "514ac8be-8e93-4dea-d65d-e0e4ff1e9c40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10.517523323666095"
            ]
          },
          "execution_count": 616,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum (lengths)/ len (lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovkhW3b8q8Fw"
      },
      "outputs": [],
      "source": [
        "from statistics import stdev, mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe88WOemq8Fw",
        "outputId": "232b63d4-d385-4b89-8198-35e643c37f1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3.510285407616222, 10.517523323666095)"
            ]
          },
          "execution_count": 658,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stdev(lengths) , mean(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrjxSkwiq8Fw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJW6-5zmq8Fw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGmvfM1Qq8Fw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSYh_sZTq8Fw"
      },
      "outputs": [],
      "source": [
        "rnn = nn.RNN(200, 128, 2, batch_first=True, bidirectional= True)\n",
        "lstm = nn.LSTM(200,128,2, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgGIOBZFq8Fw",
        "outputId": "57050db1-f5f0-4027-ff80-b292eff366fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "loss: 0.6931674480438232\n",
            "\n",
            "\n",
            "loss: 0.6924201250076294\n",
            "\n",
            "\n",
            "loss: 0.6927540302276611\n",
            "\n",
            "\n",
            "loss: 0.6928267478942871\n",
            "\n",
            "\n",
            "loss: 0.6928661584854126\n",
            "\n",
            "\n",
            "loss: 0.6939104596773783\n",
            "\n",
            "\n",
            "loss: 0.6928463748523167\n",
            "\n",
            "\n",
            "loss: 0.69243473559618\n",
            "\n",
            "\n",
            "loss: 0.6929150289959378\n",
            "\n",
            "\n",
            "loss: 0.6937695145606995\n",
            "\n",
            "\n",
            "loss: 0.6944961818781766\n",
            "\n",
            "\n",
            "loss: 0.6930368989706039\n",
            "\n",
            "\n",
            "loss: 0.693761027776278\n",
            "\n",
            "\n",
            "loss: 0.693546942302159\n",
            "\n",
            "\n",
            "loss: 0.6935274243354798\n",
            "\n",
            "\n",
            "loss: 0.6939952932298183\n",
            "\n",
            "\n",
            "loss: 0.6941820663564345\n",
            "\n",
            "\n",
            "loss: 0.6937335663371615\n",
            "\n",
            "\n",
            "loss: 0.6939332328344646\n",
            "\n",
            "\n",
            "loss: 0.6939005613327026\n",
            "\n",
            "\n",
            "loss: 0.6938808191390264\n",
            "\n",
            "\n",
            "loss: 0.6940848935734142\n",
            "\n",
            "\n",
            "loss: 0.6939385533332825\n",
            "\n",
            "\n",
            "loss: 0.6937598288059235\n",
            "\n",
            "\n",
            "loss: 0.6937420845031739\n",
            "\n",
            "\n",
            "loss: 0.6935031345257393\n",
            "\n",
            "\n",
            "loss: 0.693388322989146\n",
            "\n",
            "\n",
            "loss: 0.6933895817824773\n",
            "\n",
            "\n",
            "loss: 0.6933965662430074\n",
            "\n",
            "\n",
            "loss: 0.6932682196299236\n",
            "\n",
            "\n",
            "loss: 0.6933671082219771\n",
            "\n",
            "\n",
            "loss: 0.6934192162007095\n",
            "\n",
            "\n",
            "loss: 0.6936304334438209\n",
            "\n",
            "\n",
            "loss: 0.6936924054342158\n",
            "\n",
            "\n",
            "loss: 0.6936828630311149\n",
            "\n",
            "\n",
            "loss: 0.6935723490185208\n",
            "\n",
            "\n",
            "loss: 0.6935752920202307\n",
            "\n",
            "\n",
            "loss: 0.693510933926231\n",
            "\n",
            "\n",
            "loss: 0.6935917490567917\n",
            "\n",
            "\n",
            "loss: 0.6935716345906258\n",
            "\n",
            "\n",
            "loss: 0.6934700899007844\n",
            "\n",
            "\n",
            "loss: 0.6934709080627987\n",
            "\n",
            "\n",
            "loss: 0.693704738173374\n",
            "\n",
            "\n",
            "loss: 0.6935804188251496\n",
            "\n",
            "\n",
            "loss: 0.6939258893330893\n",
            "\n",
            "\n",
            "loss: 0.6935398954412213\n",
            "\n",
            "\n",
            "loss: 0.6934910911194823\n",
            "\n",
            "\n",
            "loss: 0.6934516429901124\n",
            "\n",
            "\n",
            "loss: 0.69314765200323\n",
            "\n",
            "\n",
            "loss: 0.6935564577579499\n",
            "\n",
            "\n",
            "loss: 0.6934736300917234\n",
            "\n",
            "\n",
            "loss: 0.6934724530348412\n",
            "\n",
            "\n",
            "loss: 0.6934042471759725\n",
            "\n",
            "\n",
            "loss: 0.6931314854710191\n",
            "\n",
            "\n",
            "loss: 0.6932513583790173\n",
            "\n",
            "\n",
            "loss: 0.6932730685387339\n",
            "\n",
            "\n",
            "loss: 0.6932346935857807\n",
            "\n",
            "\n",
            "loss: 0.6931415775726583\n",
            "\n",
            "\n",
            "loss: 0.6930470951532914\n",
            "\n",
            "\n",
            "loss: 0.6930402984221777\n",
            "\n",
            "\n",
            "loss: 0.6930019151969036\n",
            "\n",
            "\n",
            "loss: 0.6930785419479495\n",
            "\n",
            "\n",
            "loss: 0.6929919937300305\n",
            "\n",
            "\n",
            "loss: 0.6930085765197874\n",
            "\n",
            "\n",
            "loss: 0.6930376520523659\n",
            "\n",
            "\n",
            "loss: 0.6930412460457196\n",
            "\n",
            "\n",
            "loss: 0.6930441571705378\n",
            "\n",
            "\n",
            "loss: 0.6931502678815057\n",
            "\n",
            "\n",
            "loss: 0.6932326771210934\n",
            "\n",
            "\n",
            "loss: 0.6929904563086374\n",
            "\n",
            "\n",
            "loss: 0.6930780939652887\n",
            "\n",
            "\n",
            "loss: 0.6930851514140766\n",
            "\n",
            "\n",
            "loss: 0.6932650918829931\n",
            "\n",
            "\n",
            "loss: 0.6932628605816815\n",
            "\n",
            "\n",
            "loss: 0.6932753984133402\n",
            "\n",
            "\n",
            "loss: 0.6930489767538873\n",
            "\n",
            "\n",
            "loss: 0.6931236827528321\n",
            "\n",
            "\n",
            "loss: 0.6931604047616322\n",
            "\n",
            "\n",
            "loss: 0.6931598661821099\n",
            "\n",
            "\n",
            "loss: 0.6931655272841453\n",
            "\n",
            "\n",
            "loss: 0.6934531345779512\n",
            "\n",
            "\n",
            "loss: 0.6936597838634397\n",
            "\n",
            "\n",
            "loss: 0.6936556991324366\n",
            "\n",
            "\n",
            "loss: 0.693754718417213\n",
            "\n",
            "\n",
            "loss: 0.6937243615879731\n",
            "\n",
            "\n",
            "loss: 0.6936274205529411\n",
            "\n",
            "\n",
            "loss: 0.6936194526738133\n",
            "\n",
            "\n",
            "loss: 0.6935973756692625\n",
            "\n",
            "\n",
            "loss: 0.6935706399799731\n",
            "\n",
            "\n",
            "loss: 0.6935455355379315\n",
            "\n",
            "\n",
            "loss: 0.6935207902730165\n",
            "\n",
            "\n",
            "loss: 0.6935234328974846\n",
            "\n",
            "\n",
            "loss: 0.6935215425747695\n",
            "\n",
            "\n",
            "loss: 0.6936846464238267\n",
            "\n",
            "\n",
            "loss: 0.6937151507327429\n",
            "\n",
            "\n",
            "loss: 0.6937076840549705\n",
            "\n",
            "\n",
            "loss: 0.693658801083712\n",
            "\n",
            "\n",
            "loss: 0.6936556605660184\n",
            "\n",
            "\n",
            "loss: 0.6936967186253479\n",
            "\n",
            "\n",
            "loss: 0.6936821323633192\n",
            "\n",
            "\n",
            "loss: 0.6936758921878171\n",
            "\n",
            "\n",
            "loss: 0.6936055345862518\n",
            "\n",
            "\n",
            "loss: 0.6935914482885193\n",
            "\n",
            "\n",
            "loss: 0.6936579570174216\n",
            "\n",
            "\n",
            "loss: 0.6936538741702124\n",
            "\n",
            "\n",
            "loss: 0.6936656236648558\n",
            "\n",
            "\n",
            "loss: 0.6936008428858819\n",
            "\n",
            "\n",
            "loss: 0.6935860923043004\n",
            "\n",
            "\n",
            "loss: 0.6939232207219534\n",
            "\n",
            "\n",
            "loss: 0.6938137422908436\n",
            "\n",
            "\n",
            "loss: 0.6937278079556989\n",
            "\n",
            "\n",
            "loss: 0.6937796654445784\n",
            "\n",
            "\n",
            "loss: 0.6938133308317809\n",
            "\n",
            "\n",
            "loss: 0.6938842221310264\n",
            "\n",
            "\n",
            "loss: 0.6938764074574346\n",
            "\n",
            "\n",
            "loss: 0.6938677897741055\n",
            "\n",
            "\n",
            "loss: 0.6938672666875725\n",
            "\n",
            "\n",
            "loss: 0.6938738853244458\n",
            "\n",
            "\n",
            "loss: 0.6938695907592773\n",
            "\n",
            "\n",
            "loss: 0.693842294315497\n",
            "\n",
            "\n",
            "loss: 0.6938490173048224\n",
            "\n",
            "\n",
            "loss: 0.6938458974244164\n",
            "\n",
            "\n",
            "loss: 0.6938316846281531\n",
            "\n",
            "\n",
            "loss: 0.6937003462545333\n",
            "\n",
            "\n",
            "loss: 0.6936988277435302\n",
            "\n",
            "\n",
            "loss: 0.6936173789084903\n",
            "\n",
            "\n",
            "loss: 0.693649134767337\n",
            "\n",
            "\n",
            "loss: 0.6936296471394597\n",
            "\n",
            "\n",
            "loss: 0.6937278268873228\n",
            "\n",
            "\n",
            "loss: 0.6937606660219338\n",
            "\n",
            "\n",
            "loss: 0.6938056586352922\n",
            "\n",
            "\n",
            "loss: 0.6938033528400188\n",
            "\n",
            "\n",
            "loss: 0.6937347023110639\n",
            "\n",
            "\n",
            "loss: 0.6937284060378571\n",
            "\n",
            "\n",
            "loss: 0.6937268658920569\n",
            "\n",
            "\n",
            "loss: 0.6937135672744581\n",
            "\n",
            "\n",
            "loss: 0.6937715811450984\n",
            "\n",
            "\n",
            "loss: 0.6937903532947317\n",
            "\n",
            "\n",
            "loss: 0.6937965600610635\n",
            "\n",
            "\n",
            "loss: 0.6936852203948155\n",
            "\n",
            "\n",
            "loss: 0.6937634120596214\n",
            "\n",
            "\n",
            "loss: 0.6937789190822922\n",
            "\n",
            "\n",
            "loss: 0.6937925031968761\n",
            "\n",
            "\n",
            "loss: 0.6936892440749537\n",
            "\n",
            "\n",
            "loss: 0.6936750288667347\n",
            "\n",
            "\n",
            "loss: 0.6936603286494945\n",
            "\n",
            "\n",
            "loss: 0.6936599210816984\n",
            "\n",
            "\n",
            "loss: 0.6936700162855352\n",
            "\n",
            "\n",
            "loss: 0.6935876011848446\n",
            "\n",
            "\n",
            "loss: 0.6936181302865343\n",
            "\n",
            "\n",
            "loss: 0.6935858726501462\n",
            "\n",
            "\n",
            "loss: 0.6935831418162894\n",
            "\n",
            "\n",
            "loss: 0.6935688997405802\n",
            "\n",
            "\n",
            "loss: 0.693650192254549\n",
            "\n",
            "\n",
            "loss: 0.6936482121867514\n",
            "\n",
            "\n",
            "loss: 0.6936366989826541\n",
            "\n",
            "\n",
            "loss: 0.6936039605717746\n",
            "\n",
            "\n",
            "loss: 0.6934870409814614\n",
            "\n",
            "\n",
            "loss: 0.6934303453883282\n",
            "\n",
            "\n",
            "loss: 0.6934445984661576\n",
            "\n",
            "\n",
            "loss: 0.6934439536207208\n",
            "\n",
            "\n",
            "loss: 0.6934440080766321\n",
            "\n",
            "\n",
            "loss: 0.6934300982878979\n",
            "\n",
            "\n",
            "loss: 0.6933962859031627\n",
            "\n",
            "\n",
            "loss: 0.6934106035666029\n",
            "\n",
            "\n",
            "loss: 0.6934329454439229\n",
            "\n",
            "\n",
            "loss: 0.6934998160350819\n",
            "\n",
            "\n",
            "loss: 0.6935044534149621\n",
            "\n",
            "\n",
            "loss: 0.6935088796728459\n",
            "\n",
            "\n",
            "loss: 0.6935282942126777\n",
            "\n",
            "\n",
            "loss: 0.6935271488295659\n",
            "\n",
            "\n",
            "loss: 0.6935217702804608\n",
            "\n",
            "\n",
            "loss: 0.6934800082548503\n",
            "\n",
            "\n",
            "loss: 0.6934879716785473\n",
            "\n",
            "\n",
            "loss: 0.6934089200837269\n",
            "\n",
            "\n",
            "loss: 0.6934103644029658\n",
            "\n",
            "\n",
            "loss: 0.6934496103707003\n",
            "\n",
            "\n",
            "loss: 0.6935093252176646\n",
            "\n",
            "\n",
            "loss: 0.6935416916229202\n"
          ]
        }
      ],
      "source": [
        "train_loss1 =0\n",
        "for index, batch in enumerate(trainLoader):\n",
        "    text = batch['text']\n",
        "    inputs= batch['embedding']\n",
        "    label = batch['label'].float()\n",
        "    h0= torch.zeros(num_layers,16,128) # *2, because Bidirectional= True\n",
        "    output,_  = lstm (inputs)\n",
        "    out= output.mean(dim=1)\n",
        "    lin =( nn.Linear(hidden_size,1) (out) ) # *2, because Bidirectional= True\n",
        "    sig = nn.Sigmoid()(lin)\n",
        "    #print (f\"output: {output.shape}\\nout: {out.shape}\\nlin: {lin.shape}\\nsig: {sig.shape}\")\n",
        "\n",
        "    #f1= f1_score(y_true=(label.numpy()==1.),y_pred= (sig.flatten().detach().numpy() > 0.5 ))\n",
        "\n",
        "    criterion= nn.BCEWithLogitsLoss()(lin.flatten(), label)\n",
        "    optimizer.zero_grad()\n",
        "    criterion.backward()\n",
        "    optimizer.step()\n",
        "    train_loss1=train_loss1 + ((1/(index+1))*(criterion.item()-train_loss1))\n",
        "\n",
        "    #print(\"f1: \", f1)\n",
        "    #criterion2= nn.BCEWithLogitsLoss()(sig.flatten(), label)\n",
        "    print(\"\\n\\nloss:\", train_loss1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJxAPDjmq8Fx",
        "outputId": "aa337519-e0ca-49c9-a455-c1ce4957b41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2465,  0.3413,  0.2725,  ...,  0.3813, -0.8614, -0.7881],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.6991, -0.0561, -0.3917,  ...,  0.1031,  0.1671, -0.1672],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
            "1\n",
            "50\n"
          ]
        }
      ],
      "source": [
        "for i in train_ds:\n",
        "    #print(i['text'])\n",
        "    print(i['embedding'])\n",
        "    print(i['label'])\n",
        "    print(i['length'])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QJn1Wycq8Fx",
        "outputId": "cf3e29f3-f79f-408d-ca88-b837f7be4e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([16, 50, 200])\n",
            "torch.Size([16])\n",
            "torch.Size([13, 50, 200])\n",
            "torch.Size([13])\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(trainLoader):\n",
        "    text= batch['text']\n",
        "    inputs= batch['embedding']\n",
        "    labels= batch['label']\n",
        "    print(inputs.shape)\n",
        "    print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af9u2QYAq8Fx"
      },
      "outputs": [],
      "source": [
        "\"\"\"t = training_examples.comment.values[0]\n",
        "print(t)\n",
        "\"\"\"\n",
        "for i, batch in enumerate (bert_trainLoader) :\n",
        "    output = bert(batch['input_ids'], batch['attention_mask'],batch['token_type_ids'])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4q2zezbq8Fx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joFHVJeiq8Fy",
        "outputId": "63c6891e-1edf-43ba-b9cb-726f7d0f5b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of last hidden state: torch.Size([32, 50, 768])\n",
            " type of last hidden state: <class 'torch.Tensor'>\n",
            "last hidden state: tensor([[[ 2.1008e-01,  2.0528e-01, -1.4847e-01,  ..., -5.3590e-01,\n",
            "           4.2048e-01,  2.7359e-01],\n",
            "         [ 1.0002e+00,  4.1562e-02, -2.7572e-01,  ..., -8.5971e-01,\n",
            "           4.9414e-01, -3.6458e-01],\n",
            "         [ 4.6754e-01, -4.8304e-01, -9.1953e-02,  ..., -8.7756e-01,\n",
            "           9.0378e-01, -1.3082e-01],\n",
            "         ...,\n",
            "         [ 1.0677e-01, -8.5856e-01, -1.3957e-01,  ...,  3.6273e-01,\n",
            "           5.2746e-01, -2.3511e-01],\n",
            "         [ 3.8209e-01, -5.2103e-02,  1.6364e-02,  ..., -2.6334e-01,\n",
            "           3.0532e-01, -1.5548e-03],\n",
            "         [ 4.9810e-01,  7.9557e-03,  4.3164e-02,  ..., -2.6970e-01,\n",
            "           2.5408e-01,  1.9860e-02]],\n",
            "\n",
            "        [[-5.9768e-01,  1.7950e-01, -2.0828e-01,  ..., -4.4094e-01,\n",
            "           2.8299e-01,  9.0861e-01],\n",
            "         [ 4.5615e-01, -2.4149e-01,  3.0020e-03,  ...,  1.2696e-01,\n",
            "           5.2834e-01,  5.6157e-02],\n",
            "         [ 3.9882e-01, -6.0239e-01,  3.0984e-01,  ...,  1.0672e-01,\n",
            "          -3.5830e-01,  2.5975e-01],\n",
            "         ...,\n",
            "         [ 3.3780e-01, -1.7308e-01,  1.6457e-01,  ...,  3.3213e-02,\n",
            "          -1.6684e-01,  1.5242e-01],\n",
            "         [ 3.2737e-01, -1.8516e-01,  1.6065e-01,  ...,  9.4583e-03,\n",
            "          -1.5629e-01,  1.9401e-01],\n",
            "         [ 2.5044e-01, -1.6344e-01,  1.5810e-01,  ...,  6.4392e-02,\n",
            "          -2.0145e-01,  1.3012e-01]],\n",
            "\n",
            "        [[-2.8598e-01,  1.4456e-02,  5.6552e-02,  ..., -3.4352e-01,\n",
            "          -1.5251e-01,  1.8009e-01],\n",
            "         [ 3.4474e-01, -6.1410e-01,  2.0272e-01,  ..., -5.7820e-01,\n",
            "           7.9265e-02, -1.9771e-01],\n",
            "         [-1.6459e-01, -1.2269e-01, -1.3902e-01,  ..., -7.7720e-01,\n",
            "          -3.5327e-01, -2.0034e-01],\n",
            "         ...,\n",
            "         [ 9.8724e-02, -2.1614e-01,  2.1267e-01,  ..., -5.4895e-02,\n",
            "           8.5158e-03, -5.4150e-04],\n",
            "         [ 3.2965e-03, -5.4691e-01, -3.6725e-01,  ..., -1.7426e-01,\n",
            "           3.4314e-02, -2.4477e-01],\n",
            "         [-5.3488e-02, -4.7498e-01, -5.1290e-01,  ..., -1.5896e-01,\n",
            "          -1.1705e-02, -2.7569e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.4540e-01,  4.3532e-02, -1.5263e-01,  ..., -1.6401e-01,\n",
            "           1.3079e-01,  6.4017e-02],\n",
            "         [ 1.1451e-01,  2.6388e-02, -5.6629e-02,  ...,  5.7179e-01,\n",
            "           5.9083e-01,  3.5776e-01],\n",
            "         [ 5.4174e-01, -1.1679e-01, -3.1221e-01,  ...,  4.1923e-01,\n",
            "           6.4285e-01, -8.7305e-02],\n",
            "         ...,\n",
            "         [-3.4530e-01, -5.5465e-01, -4.0498e-01,  ...,  5.7313e-01,\n",
            "           3.7932e-01, -4.2157e-01],\n",
            "         [-2.0110e-01, -6.6158e-02, -4.2682e-02,  ...,  3.4893e-01,\n",
            "           8.2896e-02, -4.6332e-02],\n",
            "         [-7.9667e-02, -1.2613e-01,  1.4653e-01,  ...,  3.1945e-01,\n",
            "           1.1273e-01, -1.7358e-01]],\n",
            "\n",
            "        [[-5.1595e-01, -7.7584e-02, -2.9248e-01,  ..., -6.4900e-01,\n",
            "           2.8751e-01,  2.0538e-01],\n",
            "         [-3.3158e-01, -5.1478e-01, -1.3732e-01,  ..., -4.6023e-01,\n",
            "           1.0187e+00, -2.0837e-01],\n",
            "         [ 3.5053e-02, -2.3717e-01, -1.7723e-01,  ..., -9.5049e-01,\n",
            "          -2.1213e-02, -3.5468e-01],\n",
            "         ...,\n",
            "         [-2.9136e-01, -1.4139e-01,  4.4569e-01,  ..., -1.6129e-01,\n",
            "           2.6986e-01, -2.2288e-01],\n",
            "         [-3.1349e-01, -2.9573e-01,  3.3053e-01,  ...,  8.4915e-02,\n",
            "           3.3582e-01, -3.6331e-01],\n",
            "         [-2.7864e-01, -1.1149e-01,  3.9131e-01,  ..., -1.0200e-02,\n",
            "           2.5705e-01, -2.4975e-01]],\n",
            "\n",
            "        [[-4.1400e-01,  1.4952e-01,  6.1106e-02,  ..., -5.6709e-01,\n",
            "           3.2079e-01,  4.3859e-02],\n",
            "         [ 1.8934e-01,  2.1137e-01,  2.9948e-01,  ..., -8.3885e-01,\n",
            "           3.6380e-01, -4.6225e-01],\n",
            "         [-1.0355e-01,  4.5097e-01, -2.7926e-01,  ..., -9.5024e-01,\n",
            "          -1.8997e-01, -2.1213e-01],\n",
            "         ...,\n",
            "         [ 1.8047e-01,  1.4699e-01,  4.0174e-01,  ..., -2.9283e-01,\n",
            "           6.8139e-02, -7.4605e-02],\n",
            "         [ 7.2907e-02,  1.0501e-01,  4.2132e-01,  ..., -2.4687e-01,\n",
            "           3.2011e-02, -8.2694e-02],\n",
            "         [ 9.2091e-02,  8.2251e-02,  4.1939e-01,  ..., -2.2621e-01,\n",
            "           3.4394e-02, -1.2638e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "\n",
            "size of pooler output: torch.Size([32, 768])\n",
            " type of pooler output: <class 'torch.Tensor'>\n",
            "pooler output: tensor([[-0.9545, -0.4786, -0.6812,  ..., -0.6353, -0.7574,  0.9447],\n",
            "        [-0.8891, -0.5594, -0.8818,  ..., -0.6589, -0.7555,  0.8811],\n",
            "        [-0.8581, -0.3693,  0.0166,  ...,  0.3174, -0.6147,  0.8445],\n",
            "        ...,\n",
            "        [-0.7929, -0.3144, -0.0178,  ..., -0.0038, -0.5409,  0.8143],\n",
            "        [-0.8720, -0.4289, -0.9032,  ..., -0.8038, -0.5726,  0.8638],\n",
            "        [-0.8716, -0.4685, -0.9413,  ..., -0.8646, -0.6478,  0.9139]],\n",
            "       grad_fn=<TanhBackward0>)\n",
            "\n",
            "\n",
            "size of hidden states: 13\n",
            " type of hidden states: <class 'tuple'>\n",
            "hidden states: (tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
            "         [-0.3317, -0.2377, -0.5622,  ..., -0.8902, -0.0910, -0.5556],\n",
            "         [ 0.2996, -0.4072, -0.3423,  ..., -1.2813,  0.4760,  0.3997],\n",
            "         ...,\n",
            "         [ 0.4184, -0.4752, -0.1869,  ..., -0.1893, -0.2621,  0.1460],\n",
            "         [ 0.4448, -0.6527, -0.1576,  ..., -0.1649, -0.0288,  0.1678],\n",
            "         [ 0.5421, -0.5476, -0.4449,  ..., -0.0049, -0.2638,  0.1874]],\n",
            "\n",
            "        [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
            "         [ 0.8655,  0.5076,  0.0013,  ..., -0.0600,  0.7299, -0.4495],\n",
            "         [-0.9303, -0.4235,  0.5873,  ..., -1.1611,  0.5457, -0.0214],\n",
            "         ...,\n",
            "         [ 0.4184, -0.4752, -0.1869,  ..., -0.1893, -0.2621,  0.1460],\n",
            "         [ 0.4448, -0.6527, -0.1576,  ..., -0.1649, -0.0288,  0.1678],\n",
            "         [ 0.5421, -0.5476, -0.4449,  ..., -0.0049, -0.2638,  0.1874]],\n",
            "\n",
            "        [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
            "         [ 0.7955,  0.9768,  0.0525,  ..., -0.1027,  0.6043, -0.4444],\n",
            "         [-0.2293, -0.6354, -1.2705,  ..., -0.0499,  0.2276, -0.4999],\n",
            "         ...,\n",
            "         [ 0.4184, -0.4752, -0.1869,  ..., -0.1893, -0.2621,  0.1460],\n",
            "         [ 0.4448, -0.6527, -0.1576,  ..., -0.1649, -0.0288,  0.1678],\n",
            "         [ 0.5421, -0.5476, -0.4449,  ..., -0.0049, -0.2638,  0.1874]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
            "         [ 0.2714, -0.0430, -0.0760,  ...,  0.6592,  0.0249, -0.6766],\n",
            "         [ 0.2024,  0.4469, -0.0687,  ...,  0.4889,  0.3223,  0.2605],\n",
            "         ...,\n",
            "         [ 0.4184, -0.4752, -0.1869,  ..., -0.1893, -0.2621,  0.1460],\n",
            "         [ 0.4448, -0.6527, -0.1576,  ..., -0.1649, -0.0288,  0.1678],\n",
            "         [ 0.5421, -0.5476, -0.4449,  ..., -0.0049, -0.2638,  0.1874]],\n",
            "\n",
            "        [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
            "         [ 0.7796,  0.5359,  0.0490,  ..., -0.2750,  0.1788, -0.2276],\n",
            "         [-0.4053,  0.1127, -0.5541,  ...,  0.3581, -0.3911,  0.1235],\n",
            "         ...,\n",
            "         [ 0.4184, -0.4752, -0.1869,  ..., -0.1893, -0.2621,  0.1460],\n",
            "         [ 0.4448, -0.6527, -0.1576,  ..., -0.1649, -0.0288,  0.1678],\n",
            "         [ 0.5421, -0.5476, -0.4449,  ..., -0.0049, -0.2638,  0.1874]],\n",
            "\n",
            "        [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
            "         [-0.0593,  0.0390, -1.1301,  ...,  0.2510,  0.1292,  0.1516],\n",
            "         [-0.6350,  0.4372, -0.6334,  ..., -0.4268,  1.2088, -0.3026],\n",
            "         ...,\n",
            "         [ 0.4184, -0.4752, -0.1869,  ..., -0.1893, -0.2621,  0.1460],\n",
            "         [ 0.4448, -0.6527, -0.1576,  ..., -0.1649, -0.0288,  0.1678],\n",
            "         [ 0.5421, -0.5476, -0.4449,  ..., -0.0049, -0.2638,  0.1874]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.0256e-01,  2.3198e-03, -4.2731e-02,  ..., -2.4432e-02,\n",
            "           6.4562e-02,  6.2041e-02],\n",
            "         [-1.4804e-01, -2.9355e-01, -1.1544e+00,  ..., -1.0853e+00,\n",
            "           2.6968e-01, -3.9402e-01],\n",
            "         [ 4.3746e-01, -3.0844e-01, -2.1448e-01,  ..., -1.6673e+00,\n",
            "           1.0622e+00,  5.8761e-01],\n",
            "         ...,\n",
            "         [ 7.0979e-02, -2.9821e-01,  6.6283e-02,  ...,  1.9061e-01,\n",
            "          -4.1911e-02, -9.8069e-02],\n",
            "         [ 5.9487e-02, -4.4853e-01,  1.0836e-01,  ...,  2.3138e-01,\n",
            "           9.5528e-02, -1.0300e-01],\n",
            "         [ 1.5064e-01, -3.8647e-01, -8.8221e-02,  ...,  3.5828e-01,\n",
            "          -8.2448e-02, -9.6199e-02]],\n",
            "\n",
            "        [[ 1.2262e-01,  2.5163e-02, -1.2526e-01,  ..., -4.3500e-02,\n",
            "           4.0731e-02,  1.4491e-01],\n",
            "         [ 9.7955e-01,  4.7536e-01,  2.4943e-02,  ..., -5.7403e-01,\n",
            "           8.2456e-01, -9.1164e-02],\n",
            "         [-6.1953e-01, -3.9962e-01,  4.2159e-01,  ..., -1.5709e+00,\n",
            "           4.0415e-01, -6.1362e-02],\n",
            "         ...,\n",
            "         [ 1.2475e-01, -3.3147e-01, -4.5066e-02,  ..., -4.7186e-02,\n",
            "          -1.7110e-01, -1.7203e-01],\n",
            "         [ 1.2265e-01, -4.5309e-01,  6.5232e-03,  ...,  1.9619e-02,\n",
            "          -2.9104e-02, -1.7455e-01],\n",
            "         [ 1.8166e-01, -4.0360e-01, -1.9542e-01,  ...,  1.7274e-01,\n",
            "          -2.2062e-01, -1.6565e-01]],\n",
            "\n",
            "        [[ 1.9792e-01,  9.8990e-02, -1.0050e-01,  ..., -7.8711e-02,\n",
            "           7.6129e-03,  6.5974e-02],\n",
            "         [ 9.2543e-01,  1.4538e+00, -1.6743e-01,  ...,  5.1928e-02,\n",
            "           3.4519e-01, -3.2604e-01],\n",
            "         [ 4.2831e-01,  4.7087e-02, -1.2267e+00,  ..., -4.3673e-01,\n",
            "           1.1135e-01, -8.2251e-01],\n",
            "         ...,\n",
            "         [ 2.0720e-01, -1.4880e-01,  7.6468e-02,  ...,  4.8446e-02,\n",
            "          -1.7465e-01, -2.0497e-01],\n",
            "         [ 1.7969e-01, -3.1797e-01,  1.3046e-01,  ...,  1.0251e-01,\n",
            "          -3.9814e-02, -2.2168e-01],\n",
            "         [ 2.5247e-01, -2.2789e-01, -1.1142e-01,  ...,  2.0753e-01,\n",
            "          -2.0398e-01, -1.8195e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1605e-01,  6.8021e-02, -1.2354e-01,  ...,  3.6645e-02,\n",
            "           8.3912e-02,  5.8233e-02],\n",
            "         [ 3.5412e-01, -1.8661e-01,  9.3839e-02,  ...,  4.5692e-01,\n",
            "          -1.9826e-01, -5.3088e-01],\n",
            "         [ 2.0693e-01,  3.7774e-01, -2.4127e-01,  ...,  9.7358e-02,\n",
            "           8.4929e-01,  2.7963e-01],\n",
            "         ...,\n",
            "         [ 1.5128e-01, -4.2889e-01, -4.8318e-02,  ...,  1.8090e-01,\n",
            "          -3.4502e-03, -1.5489e-01],\n",
            "         [ 1.3960e-01, -5.5222e-01, -1.5619e-02,  ...,  2.3261e-01,\n",
            "           1.3833e-01, -1.3511e-01],\n",
            "         [ 2.0106e-01, -4.8226e-01, -1.8253e-01,  ...,  3.7899e-01,\n",
            "          -7.5947e-02, -1.4381e-01]],\n",
            "\n",
            "        [[ 1.6775e-01, -3.3940e-02, -1.0224e-01,  ..., -1.7181e-05,\n",
            "           5.6631e-02,  1.8024e-02],\n",
            "         [ 1.1203e+00,  6.6319e-01,  2.4467e-01,  ..., -5.5257e-01,\n",
            "           7.5702e-01, -5.0718e-01],\n",
            "         [-1.7535e-01, -9.2641e-03, -3.3083e-01,  ...,  1.5310e-01,\n",
            "          -3.0937e-01,  2.4387e-01],\n",
            "         ...,\n",
            "         [ 8.4402e-02, -3.5119e-01,  1.9081e-01,  ...,  4.2588e-01,\n",
            "           7.2805e-04, -2.9603e-01],\n",
            "         [ 9.4488e-02, -4.9087e-01,  2.3642e-01,  ...,  4.7048e-01,\n",
            "           1.4738e-01, -2.6722e-01],\n",
            "         [ 1.5350e-01, -4.2191e-01,  1.7663e-04,  ...,  5.9103e-01,\n",
            "          -3.8567e-02, -2.5708e-01]],\n",
            "\n",
            "        [[ 1.5846e-01,  6.8169e-02, -1.0082e-01,  ...,  2.6033e-02,\n",
            "           7.9693e-02,  7.7199e-02],\n",
            "         [ 2.8770e-01,  3.0239e-01, -1.3105e+00,  ..., -9.2485e-02,\n",
            "           4.4145e-01, -7.6754e-02],\n",
            "         [-2.6782e-01,  1.0982e+00, -6.4135e-01,  ..., -5.2281e-01,\n",
            "           1.4754e+00, -2.1978e-01],\n",
            "         ...,\n",
            "         [ 1.3068e-01, -2.2880e-01,  9.9747e-02,  ...,  2.0251e-01,\n",
            "          -5.5617e-02, -1.9694e-01],\n",
            "         [ 1.2609e-01, -3.8410e-01,  1.4878e-01,  ...,  2.6173e-01,\n",
            "           9.4320e-02, -1.6903e-01],\n",
            "         [ 2.1453e-01, -3.2876e-01, -5.8063e-02,  ...,  4.0218e-01,\n",
            "          -9.2620e-02, -1.7437e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1601, -0.1688, -0.1629,  ...,  0.1101,  0.0802,  0.0348],\n",
            "         [ 0.0793, -0.1082, -1.2164,  ..., -0.7453,  0.5630, -0.5745],\n",
            "         [ 0.3444, -0.8815, -0.7019,  ..., -1.3140,  1.1748,  0.4605],\n",
            "         ...,\n",
            "         [-0.2028, -0.1221,  0.0746,  ...,  0.6471, -0.1863, -0.1012],\n",
            "         [-0.0268, -0.1036,  0.1739,  ...,  0.6941, -0.1771, -0.1240],\n",
            "         [ 0.0800, -0.0181,  0.0819,  ...,  0.8530, -0.2371, -0.1551]],\n",
            "\n",
            "        [[ 0.0219, -0.2225, -0.2333,  ...,  0.1355,  0.1538,  0.1968],\n",
            "         [ 1.5716,  0.4317, -0.1311,  ..., -0.6329,  0.9549, -0.1393],\n",
            "         [-0.0948, -0.6569,  0.6992,  ..., -1.4588,  0.3853, -0.2372],\n",
            "         ...,\n",
            "         [ 0.0362, -0.2160, -0.0687,  ...,  0.4556, -0.3582, -0.0200],\n",
            "         [ 0.0274, -0.3065, -0.0622,  ...,  0.5528, -0.2582,  0.0059],\n",
            "         [ 0.0239, -0.3020, -0.1670,  ...,  0.5908, -0.3362, -0.0350]],\n",
            "\n",
            "        [[ 0.0802, -0.1748, -0.1594,  ...,  0.1181,  0.0576,  0.0823],\n",
            "         [ 0.4713,  1.4721,  0.1556,  ..., -0.2511,  0.3962, -0.3180],\n",
            "         [ 0.5649,  0.5068, -1.0881,  ..., -0.3595,  0.3400, -0.4758],\n",
            "         ...,\n",
            "         [ 0.0308,  0.0185,  0.2482,  ...,  0.4494, -0.1711, -0.1220],\n",
            "         [ 0.0020, -0.1617,  0.1941,  ...,  0.4972, -0.0985, -0.1272],\n",
            "         [ 0.0473, -0.0777,  0.0974,  ...,  0.5509, -0.1556, -0.1282]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0220, -0.1543, -0.1624,  ...,  0.1485,  0.0438, -0.0133],\n",
            "         [ 0.1276, -0.2423,  0.2503,  ...,  0.1227, -0.1153, -0.8560],\n",
            "         [ 0.3207, -0.0202, -0.0802,  ...,  0.3521,  0.5749,  0.1549],\n",
            "         ...,\n",
            "         [-0.1706, -0.2723, -0.0105,  ...,  0.5638, -0.1288, -0.0539],\n",
            "         [-0.0732, -0.3516,  0.0628,  ...,  0.5994, -0.1038, -0.1093],\n",
            "         [ 0.0328, -0.2450, -0.0132,  ...,  0.7405, -0.2602, -0.1448]],\n",
            "\n",
            "        [[ 0.0703, -0.2443, -0.2178,  ...,  0.0332,  0.1004,  0.0792],\n",
            "         [ 0.6371,  0.2452,  0.7190,  ..., -0.7591,  1.2384, -0.7522],\n",
            "         [-0.0243,  0.0342, -0.1947,  ..., -0.0035, -0.4212,  0.2561],\n",
            "         ...,\n",
            "         [-0.0507, -0.1724,  0.0603,  ...,  0.8198, -0.2566, -0.2987],\n",
            "         [-0.1642, -0.3302,  0.0972,  ...,  0.8250, -0.1867, -0.3014],\n",
            "         [-0.0602, -0.2839, -0.0262,  ...,  0.8256, -0.2795, -0.3457]],\n",
            "\n",
            "        [[-0.0027, -0.1748, -0.1986,  ...,  0.1086,  0.0905,  0.0455],\n",
            "         [ 0.7742,  0.4850, -0.1833,  ...,  0.0342,  0.2810, -0.4041],\n",
            "         [ 0.2513,  1.1584, -0.4501,  ..., -0.1113,  1.3309, -0.1475],\n",
            "         ...,\n",
            "         [ 0.0314,  0.0254,  0.2629,  ...,  0.5311, -0.2700, -0.2185],\n",
            "         [-0.0756, -0.1626,  0.2230,  ...,  0.6944, -0.0664, -0.1236],\n",
            "         [ 0.0386, -0.0655,  0.1181,  ...,  0.7427, -0.2009, -0.1258]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.8684e-01, -2.5465e-01, -6.0779e-02,  ...,  1.8191e-01,\n",
            "           9.5732e-02,  2.5332e-01],\n",
            "         [ 5.7114e-01, -2.9351e-01, -6.9586e-01,  ..., -2.5471e-01,\n",
            "           6.1369e-01, -5.5084e-01],\n",
            "         [ 3.5981e-01, -1.0147e+00, -5.8759e-01,  ..., -1.2651e+00,\n",
            "           6.6485e-01,  4.8037e-01],\n",
            "         ...,\n",
            "         [-5.0650e-01, -8.5202e-02,  2.1068e-01,  ...,  8.0903e-01,\n",
            "          -8.6157e-03, -1.7854e-01],\n",
            "         [-3.5502e-02,  6.2608e-02,  3.8659e-01,  ...,  7.6877e-01,\n",
            "           6.8401e-02, -1.3329e-01],\n",
            "         [ 3.4329e-02,  1.3566e-01,  3.0481e-01,  ...,  9.5534e-01,\n",
            "           2.4883e-02, -1.7025e-01]],\n",
            "\n",
            "        [[ 9.1243e-02, -2.9266e-01, -2.1513e-02,  ...,  2.5705e-01,\n",
            "           9.2782e-02,  3.1382e-01],\n",
            "         [ 1.6652e+00,  3.0862e-01,  3.5681e-01,  ..., -6.7152e-01,\n",
            "           1.4675e-01, -1.8568e-01],\n",
            "         [ 2.8268e-01, -4.5702e-01,  9.7405e-01,  ..., -1.2036e+00,\n",
            "           1.6287e-01,  1.9042e-01],\n",
            "         ...,\n",
            "         [ 8.1354e-02, -1.1369e-01,  4.5406e-01,  ...,  3.8996e-01,\n",
            "          -2.8854e-01, -1.8316e-01],\n",
            "         [ 1.9934e-02, -2.0570e-01,  4.7638e-01,  ...,  4.9839e-01,\n",
            "          -1.5046e-01, -1.0691e-01],\n",
            "         [-3.9811e-03, -2.2278e-01,  3.0821e-01,  ...,  5.3421e-01,\n",
            "          -2.4848e-01, -1.9906e-01]],\n",
            "\n",
            "        [[ 3.9078e-02, -1.6715e-01, -5.9476e-02,  ...,  1.9270e-01,\n",
            "           1.0493e-01,  1.8950e-01],\n",
            "         [ 5.3360e-01,  8.3376e-01, -7.1852e-02,  ...,  6.1328e-01,\n",
            "           1.3603e-01, -2.0599e-01],\n",
            "         [ 3.8099e-01,  3.8408e-01, -1.2284e+00,  ..., -5.8396e-01,\n",
            "           5.2881e-01, -5.3391e-01],\n",
            "         ...,\n",
            "         [-2.2441e-01,  7.9778e-02,  3.5785e-01,  ...,  5.7905e-01,\n",
            "          -9.9948e-02, -3.7230e-01],\n",
            "         [-4.7028e-01, -1.2981e-01,  2.6190e-01,  ...,  6.7393e-01,\n",
            "          -7.2109e-03, -3.0467e-01],\n",
            "         [-4.8972e-01, -1.0685e-01,  1.4016e-01,  ...,  8.4823e-01,\n",
            "          -6.7449e-03, -3.0107e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-6.2739e-03, -1.6703e-01, -8.0482e-02,  ...,  1.4066e-01,\n",
            "           1.2699e-02,  1.0340e-01],\n",
            "         [ 1.7889e-01, -4.2248e-01,  8.7657e-01,  ...,  3.8165e-02,\n",
            "          -3.4696e-01, -3.4762e-01],\n",
            "         [ 3.7073e-01, -1.4271e-01,  3.1688e-01,  ...,  4.8532e-01,\n",
            "           5.9739e-01,  3.3855e-01],\n",
            "         ...,\n",
            "         [-4.8378e-01, -2.8504e-01,  3.3132e-02,  ...,  6.0826e-01,\n",
            "          -1.0579e-02, -2.4314e-01],\n",
            "         [-1.8491e-01, -3.3942e-01,  1.9107e-01,  ...,  6.3478e-01,\n",
            "           1.0195e-03, -3.1729e-01],\n",
            "         [-4.4117e-02, -2.4846e-01,  8.6798e-02,  ...,  7.6599e-01,\n",
            "          -9.9998e-02, -2.6147e-01]],\n",
            "\n",
            "        [[ 7.7238e-04, -2.7344e-01, -6.8633e-02,  ...,  1.7534e-01,\n",
            "           6.3148e-02,  2.5080e-01],\n",
            "         [ 5.3831e-01, -8.2929e-04,  8.9093e-01,  ..., -1.0436e+00,\n",
            "           1.0473e+00, -6.1856e-01],\n",
            "         [-2.1738e-02,  3.9264e-01, -4.0734e-02,  ...,  2.1922e-01,\n",
            "          -6.1493e-01,  5.2003e-01],\n",
            "         ...,\n",
            "         [-2.1568e-01, -7.1001e-02,  3.7633e-01,  ...,  8.3971e-01,\n",
            "          -1.2772e-01, -3.5348e-01],\n",
            "         [-3.8583e-01, -2.8305e-01,  3.6974e-01,  ...,  8.0574e-01,\n",
            "          -5.9753e-02, -3.7036e-01],\n",
            "         [-2.1160e-01, -2.1773e-01,  2.0833e-01,  ...,  8.0356e-01,\n",
            "          -1.3461e-01, -3.9093e-01]],\n",
            "\n",
            "        [[ 6.2425e-02, -2.3629e-01, -2.0807e-02,  ...,  1.3008e-01,\n",
            "           5.0025e-02,  1.3319e-01],\n",
            "         [ 9.0826e-01,  4.9840e-01,  2.5906e-01,  ...,  5.6906e-02,\n",
            "           4.7896e-01, -7.5594e-01],\n",
            "         [ 3.0658e-01,  8.6174e-01, -3.1870e-01,  ..., -3.8968e-01,\n",
            "           1.2165e+00, -4.3675e-01],\n",
            "         ...,\n",
            "         [ 1.5086e-01,  3.6183e-02,  6.0807e-01,  ...,  6.2270e-01,\n",
            "          -4.7588e-02, -5.0518e-01],\n",
            "         [-1.4098e-02, -1.4830e-01,  5.3376e-01,  ...,  7.6887e-01,\n",
            "           9.2792e-02, -3.7350e-01],\n",
            "         [ 1.2183e-01, -5.4007e-02,  4.5459e-01,  ...,  7.9105e-01,\n",
            "           3.4440e-03, -3.5787e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2476, -0.5417, -0.5788,  ...,  0.3017,  0.3165,  0.5390],\n",
            "         [ 0.2807,  0.0883, -0.4903,  ...,  0.3560, -0.1071, -0.6297],\n",
            "         [ 0.5741, -1.0899, -0.8717,  ..., -1.3406,  0.5824,  0.6237],\n",
            "         ...,\n",
            "         [-0.8716, -0.5905,  0.2329,  ...,  0.2025, -0.4179, -0.1087],\n",
            "         [-0.1238, -0.4533,  0.5952,  ...,  0.3519, -0.2484, -0.2784],\n",
            "         [-0.0205, -0.4537,  0.6212,  ...,  0.5503, -0.2489, -0.3097]],\n",
            "\n",
            "        [[ 0.1452, -0.5816, -0.3244,  ...,  0.2777, -0.0723,  0.5732],\n",
            "         [ 1.3181,  0.2801,  1.2317,  ...,  0.2166, -0.2261, -0.4327],\n",
            "         [ 0.5943, -0.6212,  0.9409,  ..., -0.8601,  0.0095,  0.3848],\n",
            "         ...,\n",
            "         [-0.0190, -0.5984,  0.8018,  ...,  0.5177, -0.6738, -0.2334],\n",
            "         [ 0.0243, -0.8034,  0.6645,  ...,  0.5244, -0.5695, -0.2262],\n",
            "         [-0.0675, -0.7910,  0.4801,  ...,  0.5378, -0.6430, -0.2934]],\n",
            "\n",
            "        [[ 0.0049, -0.4117, -0.4720,  ...,  0.2328,  0.0253,  0.5752],\n",
            "         [ 0.6070,  0.7404,  0.3521,  ...,  0.6321,  0.0530, -0.4586],\n",
            "         [ 0.4581,  0.0987, -1.1230,  ..., -0.9188,  0.6900, -0.2380],\n",
            "         ...,\n",
            "         [-0.1821, -0.2743,  0.6739,  ...,  0.4118, -0.3583, -0.4451],\n",
            "         [-0.5116, -0.6439,  0.2310,  ...,  0.1885, -0.3438, -0.2638],\n",
            "         [-0.5751, -0.5612, -0.0574,  ...,  0.0839, -0.2596, -0.2779]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0748, -0.3391, -0.4408,  ...,  0.4089, -0.0805,  0.3699],\n",
            "         [ 0.4908, -0.6098,  0.9656,  ...,  0.4521, -0.4094, -0.1684],\n",
            "         [ 0.4050, -0.1615,  0.0989,  ...,  0.6853,  0.6855,  0.1179],\n",
            "         ...,\n",
            "         [-0.5213, -0.4132, -0.1086,  ...,  0.1045, -0.2925, -0.2281],\n",
            "         [-0.1064, -0.6012,  0.4609,  ...,  0.5674, -0.2132, -0.2697],\n",
            "         [ 0.0852, -0.5467,  0.5488,  ...,  0.5133, -0.2626, -0.2818]],\n",
            "\n",
            "        [[ 0.0160, -0.4838, -0.5618,  ...,  0.3310,  0.0031,  0.6241],\n",
            "         [ 0.4718, -0.3519,  0.8768,  ..., -1.0478,  0.5417, -0.7000],\n",
            "         [ 0.1386,  0.1130,  0.0592,  ...,  0.5254, -0.7922,  0.2990],\n",
            "         ...,\n",
            "         [-0.3514, -0.5912,  0.5593,  ...,  0.5242, -0.3653, -0.3028],\n",
            "         [-0.5393, -0.8239,  0.3185,  ...,  0.5242, -0.3994, -0.3294],\n",
            "         [-0.3211, -0.7773,  0.2222,  ...,  0.4970, -0.3532, -0.3314]],\n",
            "\n",
            "        [[ 0.2053, -0.5861, -0.2808,  ...,  0.4380, -0.0632,  0.2580],\n",
            "         [ 0.7958,  0.5444,  0.1206,  ...,  0.1590,  0.3953, -1.2202],\n",
            "         [ 0.2976,  0.7583, -0.5747,  ..., -0.2477,  1.4024, -0.4545],\n",
            "         ...,\n",
            "         [ 0.1133, -0.2994,  0.7494,  ...,  0.3586, -0.3020, -0.5676],\n",
            "         [-0.1022, -0.5332,  0.7309,  ...,  0.4963, -0.2791, -0.5193],\n",
            "         [ 0.0260, -0.4565,  0.6111,  ...,  0.4832, -0.3063, -0.5202]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0327, -0.6120, -0.6165,  ...,  0.1215,  0.2741,  0.6472],\n",
            "         [ 0.1919, -0.3433, -0.4303,  ...,  0.5166, -0.2034, -0.4310],\n",
            "         [ 0.3833, -0.8502, -0.7641,  ..., -0.6961,  0.3245,  0.6611],\n",
            "         ...,\n",
            "         [-0.7382, -0.4808, -0.0340,  ...,  0.0660, -0.2650,  0.1258],\n",
            "         [ 0.1367, -0.5235,  0.4285,  ..., -0.0212,  0.0447, -0.3463],\n",
            "         [ 0.2035, -0.5115,  0.5491,  ...,  0.1934,  0.0377, -0.3575]],\n",
            "\n",
            "        [[-0.2237, -0.4462, -0.3226,  ..., -0.3266,  0.1576,  0.8864],\n",
            "         [ 0.6670,  0.2536,  1.1360,  ..., -0.2301, -0.1447,  0.2240],\n",
            "         [ 0.8547, -0.5078,  1.1241,  ..., -0.9791,  0.3285,  0.3889],\n",
            "         ...,\n",
            "         [ 0.0115, -0.5217,  0.8605,  ..., -0.0438, -0.4934, -0.3142],\n",
            "         [ 0.0057, -0.7208,  0.7157,  ..., -0.0917, -0.4466, -0.2102],\n",
            "         [ 0.0201, -0.6159,  0.5460,  ...,  0.0602, -0.4869, -0.3700]],\n",
            "\n",
            "        [[-0.4882, -0.6601, -0.5036,  ..., -0.1344,  0.1046,  0.6621],\n",
            "         [ 0.7258, -0.0070,  1.0416,  ...,  0.1711, -0.0380, -0.5119],\n",
            "         [-0.0309, -0.2617, -0.2229,  ..., -1.3495,  0.9238, -0.5524],\n",
            "         ...,\n",
            "         [ 0.1559, -0.3755,  0.4904,  ...,  0.0269, -0.1934, -0.5136],\n",
            "         [-0.3866, -0.7867,  0.0738,  ..., -0.4959, -0.3619, -0.2001],\n",
            "         [-0.5722, -0.6102, -0.1753,  ..., -0.7436, -0.2950, -0.0904]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.2118, -0.3819, -0.3534,  ..., -0.1142, -0.0482,  0.5856],\n",
            "         [ 0.2456, -0.2329,  0.9473,  ...,  0.0785, -0.8010, -0.0191],\n",
            "         [ 0.7160, -0.0997, -0.1486,  ...,  0.8225,  0.8796,  0.3165],\n",
            "         ...,\n",
            "         [-0.6372, -0.2647, -0.1300,  ..., -0.4222, -0.1039, -0.1667],\n",
            "         [ 0.0811, -0.5177,  0.4840,  ...,  0.0697, -0.0212, -0.3872],\n",
            "         [ 0.2661, -0.5451,  0.5413,  ..., -0.0442, -0.1193, -0.4114]],\n",
            "\n",
            "        [[-0.3770, -0.3905, -0.5622,  ..., -0.0580,  0.0351,  0.9753],\n",
            "         [ 0.3316, -0.2370,  1.0658,  ..., -0.8641,  0.5358, -0.6764],\n",
            "         [ 0.3940,  0.2029,  0.0151,  ...,  0.8579, -0.1766,  0.4417],\n",
            "         ...,\n",
            "         [-0.3530, -0.4058,  0.5741,  ...,  0.2351, -0.1588, -0.4671],\n",
            "         [-0.4766, -0.5012,  0.3708,  ...,  0.4075, -0.2297, -0.4452],\n",
            "         [-0.2404, -0.5426,  0.1975,  ...,  0.3652, -0.2096, -0.4843]],\n",
            "\n",
            "        [[ 0.0200, -0.6940,  0.0974,  ..., -0.5188,  0.1328,  0.4846],\n",
            "         [ 0.2741,  0.4058,  0.5061,  ...,  0.0818,  0.5657, -0.9633],\n",
            "         [ 0.2867,  0.8137,  0.0413,  ..., -0.2688,  1.5407, -0.1440],\n",
            "         ...,\n",
            "         [ 0.2714, -0.1803,  1.0575,  ...,  0.1525,  0.3123, -0.8710],\n",
            "         [ 0.0500, -0.4035,  1.1660,  ...,  0.1982,  0.2132, -0.7747],\n",
            "         [ 0.2094, -0.3031,  1.0260,  ...,  0.1083,  0.2250, -0.7533]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-5.7044e-02, -4.5758e-01, -5.7003e-01,  ..., -2.6114e-01,\n",
            "           6.5613e-01,  3.8708e-01],\n",
            "         [-4.0165e-01, -4.9698e-01, -5.5762e-01,  ...,  3.9401e-01,\n",
            "          -6.3101e-03, -9.9631e-01],\n",
            "         [ 9.4343e-02, -8.6652e-01, -2.8843e-01,  ..., -6.8832e-01,\n",
            "           3.7359e-01,  3.7130e-01],\n",
            "         ...,\n",
            "         [-1.1302e+00, -3.1697e-01, -4.8212e-02,  ..., -3.7908e-01,\n",
            "          -2.0387e-01,  2.1792e-01],\n",
            "         [ 7.1419e-02, -5.1983e-01,  6.3480e-01,  ...,  1.3179e-02,\n",
            "           1.1739e-01, -3.3819e-01],\n",
            "         [ 1.0719e-01, -4.8192e-01,  7.7034e-01,  ...,  2.2219e-01,\n",
            "           3.5524e-03, -3.7729e-01]],\n",
            "\n",
            "        [[-5.2884e-01, -4.5284e-01, -6.8047e-01,  ..., -6.5842e-01,\n",
            "           5.7952e-02,  1.1157e+00],\n",
            "         [ 4.0880e-01,  6.2670e-01,  1.4661e+00,  ..., -1.8471e-01,\n",
            "          -2.2074e-01, -6.3684e-02],\n",
            "         [ 8.3680e-01, -5.4112e-01,  1.0464e+00,  ..., -9.8700e-01,\n",
            "          -4.4518e-01,  2.2132e-01],\n",
            "         ...,\n",
            "         [-1.2794e-01, -5.6094e-01,  1.1517e+00,  ...,  6.5851e-02,\n",
            "          -4.7988e-01, -3.7402e-01],\n",
            "         [-1.1051e-01, -6.8866e-01,  1.0483e+00,  ...,  1.6546e-02,\n",
            "          -3.9123e-01, -2.3968e-01],\n",
            "         [-2.4413e-01, -6.0266e-01,  8.4605e-01,  ...,  1.0161e-01,\n",
            "          -4.8950e-01, -3.8999e-01]],\n",
            "\n",
            "        [[-4.6726e-01, -9.9137e-01, -4.2927e-01,  ..., -9.6999e-01,\n",
            "           1.3795e-01,  6.9517e-01],\n",
            "         [ 7.6266e-01, -4.1326e-01,  1.2150e+00,  ...,  3.5577e-01,\n",
            "           1.7101e-01, -6.1963e-01],\n",
            "         [ 2.0439e-01, -9.1906e-01,  1.2700e-01,  ..., -1.6479e+00,\n",
            "           8.7884e-01, -4.1486e-01],\n",
            "         ...,\n",
            "         [ 1.8627e-01, -4.3434e-01,  8.7871e-01,  ...,  1.0889e-01,\n",
            "          -1.6361e-01, -5.1318e-01],\n",
            "         [-4.6737e-01, -6.7860e-01,  3.3346e-02,  ..., -9.2535e-01,\n",
            "          -3.1749e-01, -1.0750e-01],\n",
            "         [-6.7149e-01, -4.0756e-01, -2.6909e-01,  ..., -1.2398e+00,\n",
            "          -2.1536e-01,  1.4243e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.3385e-01, -7.2957e-01, -3.6536e-01,  ..., -7.9001e-02,\n",
            "          -1.9424e-02,  5.5541e-01],\n",
            "         [ 4.6955e-01,  1.4852e-01,  9.7736e-01,  ..., -9.8694e-02,\n",
            "          -5.8908e-01, -1.1615e-01],\n",
            "         [ 8.3012e-01, -5.1472e-02,  3.2010e-02,  ...,  9.4997e-01,\n",
            "           2.8007e-01,  6.1052e-01],\n",
            "         ...,\n",
            "         [-8.3301e-01, -8.1175e-02, -2.7877e-01,  ..., -4.5155e-01,\n",
            "          -8.6567e-02,  1.3917e-01],\n",
            "         [-4.2436e-02, -5.1744e-01,  6.5477e-01,  ...,  2.0367e-02,\n",
            "          -4.3225e-02, -1.8962e-01],\n",
            "         [ 1.0040e-01, -5.1006e-01,  7.3923e-01,  ..., -2.7596e-02,\n",
            "          -1.4169e-01, -3.6868e-01]],\n",
            "\n",
            "        [[-7.4716e-01, -4.7368e-01, -9.1667e-01,  ..., -4.9066e-01,\n",
            "           2.1764e-03,  1.0660e+00],\n",
            "         [-1.0260e-02, -4.7204e-01,  7.9193e-01,  ..., -8.9122e-01,\n",
            "           6.9367e-01, -6.8558e-01],\n",
            "         [ 2.3384e-01, -3.5261e-02, -2.0813e-01,  ...,  4.7200e-01,\n",
            "          -2.4458e-01,  9.7556e-01],\n",
            "         ...,\n",
            "         [-4.4043e-01, -2.8947e-01,  7.6053e-01,  ...,  2.3425e-01,\n",
            "           1.4335e-02, -5.2963e-01],\n",
            "         [-7.2820e-01, -4.4835e-01,  4.8820e-01,  ...,  3.0791e-01,\n",
            "          -4.3883e-02, -4.2211e-01],\n",
            "         [-4.2781e-01, -4.6063e-01,  4.3963e-01,  ...,  3.8750e-01,\n",
            "           1.4107e-03, -4.8695e-01]],\n",
            "\n",
            "        [[ 9.7531e-02, -6.8906e-01,  2.1163e-01,  ..., -4.0179e-01,\n",
            "           1.2066e-01,  4.7378e-01],\n",
            "         [-3.5437e-02,  7.8179e-02,  4.0764e-01,  ...,  5.0667e-01,\n",
            "           5.6246e-01, -7.1395e-01],\n",
            "         [ 3.5538e-01,  4.5514e-01,  9.9416e-02,  ..., -7.5822e-01,\n",
            "           1.4088e+00, -5.0552e-02],\n",
            "         ...,\n",
            "         [ 2.5363e-01,  1.1549e-02,  1.2901e+00,  ...,  1.5309e-01,\n",
            "           8.5049e-02, -8.3570e-01],\n",
            "         [-7.1961e-03, -2.0370e-01,  1.4560e+00,  ...,  2.4765e-01,\n",
            "          -1.3768e-02, -7.3581e-01],\n",
            "         [ 1.3208e-01, -1.3524e-01,  1.3059e+00,  ...,  1.5995e-01,\n",
            "           5.3427e-02, -7.3582e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0190, -0.6924, -0.9904,  ..., -0.6892,  0.8400,  0.4417],\n",
            "         [-0.0986, -0.4686, -0.5386,  ..., -0.1649,  0.6568, -0.5597],\n",
            "         [ 0.0484, -1.1849,  0.1152,  ..., -0.5486,  1.1785,  0.5550],\n",
            "         ...,\n",
            "         [-0.8154, -0.6188, -1.2465,  ..., -0.6325, -0.0508,  0.6725],\n",
            "         [ 0.3309, -0.2759,  0.2041,  ...,  0.0862,  0.0478, -0.0814],\n",
            "         [ 0.3462, -0.2927,  0.3975,  ...,  0.2923, -0.0286, -0.1633]],\n",
            "\n",
            "        [[-0.4612, -0.4882, -0.4103,  ..., -0.6421,  0.2794,  1.3339],\n",
            "         [ 0.6212, -0.1313,  1.2957,  ..., -0.3293,  0.5332,  0.1960],\n",
            "         [ 0.9115, -0.7437,  1.2579,  ..., -0.1909, -0.1512, -0.2803],\n",
            "         ...,\n",
            "         [-0.1296, -0.6962,  1.2043,  ...,  0.3406, -0.1396,  0.0295],\n",
            "         [-0.1704, -0.8072,  1.0760,  ...,  0.2831, -0.1127,  0.1859],\n",
            "         [-0.2782, -0.8170,  0.8560,  ...,  0.2520, -0.1139,  0.0740]],\n",
            "\n",
            "        [[-0.4306, -1.1802, -0.3820,  ..., -0.8002,  0.3733,  1.0971],\n",
            "         [ 0.4898, -0.9690,  0.8257,  ...,  0.1971,  0.7009, -0.2995],\n",
            "         [-0.1492, -1.2558,  0.2049,  ..., -1.5720,  1.0326, -0.2118],\n",
            "         ...,\n",
            "         [ 0.3507, -0.1960,  1.0084,  ...,  0.2947, -0.0379, -0.4168],\n",
            "         [-0.7389, -1.1386, -0.6437,  ..., -1.2026, -0.1225,  0.2897],\n",
            "         [-0.8760, -1.0058, -0.9777,  ..., -1.4860,  0.0452,  0.4291]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1018, -0.2752, -0.2036,  ..., -0.3982,  0.2860,  0.7565],\n",
            "         [ 0.1042,  0.5030,  1.1592,  ...,  0.0341, -0.1472,  0.0619],\n",
            "         [ 0.8206, -0.1252,  0.3238,  ...,  0.7623,  0.8424,  0.1675],\n",
            "         ...,\n",
            "         [-0.7731, -0.4002, -0.7882,  ..., -0.6916,  0.5139,  0.3847],\n",
            "         [-0.0081,  0.0282,  0.5307,  ...,  0.0603,  0.4535,  0.0335],\n",
            "         [ 0.0977,  0.0146,  0.8634,  ...,  0.1036,  0.2196, -0.1257]],\n",
            "\n",
            "        [[-0.8987, -0.5712, -0.8491,  ..., -0.6671,  0.5541,  1.1541],\n",
            "         [ 0.1138, -0.6072,  0.8497,  ..., -0.8294,  0.9886, -0.7405],\n",
            "         [-0.0635, -0.5072, -0.2118,  ...,  0.0421,  0.3905,  0.7558],\n",
            "         ...,\n",
            "         [-0.3178, -0.2773,  0.6433,  ...,  0.3878,  0.2257, -0.4575],\n",
            "         [-0.6388, -0.6088,  0.2784,  ...,  0.0876,  0.3239, -0.4215],\n",
            "         [-0.3068, -0.3968,  0.4425,  ...,  0.3632,  0.2537, -0.4632]],\n",
            "\n",
            "        [[ 0.0570, -0.5903,  0.1046,  ..., -0.5459,  0.2764,  0.4857],\n",
            "         [ 0.0273,  0.4239,  0.7469,  ...,  0.2486,  0.6136, -0.8815],\n",
            "         [ 0.3827,  0.6839,  0.1084,  ..., -0.9718,  1.5592, -0.2389],\n",
            "         ...,\n",
            "         [ 0.2103,  0.3918,  1.4733,  ...,  0.0631,  0.3204, -0.5333],\n",
            "         [-0.0681,  0.1662,  1.5771,  ...,  0.1781,  0.1791, -0.5119],\n",
            "         [ 0.1023,  0.2034,  1.4613,  ...,  0.1218,  0.2748, -0.5341]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0853, -0.3859, -0.9887,  ..., -0.8937,  0.8065,  0.5765],\n",
            "         [ 0.1796, -0.5832, -0.5276,  ..., -0.2012,  1.0295, -0.6870],\n",
            "         [ 0.2108, -1.1775,  0.0259,  ..., -0.3623,  1.2410,  0.2302],\n",
            "         ...,\n",
            "         [-1.2248, -0.6714, -1.1967,  ..., -0.8371, -0.5664,  0.6943],\n",
            "         [ 0.2920, -0.2319, -0.0475,  ..., -0.1490, -0.2348,  0.0294],\n",
            "         [ 0.3811, -0.2157,  0.2090,  ...,  0.0399, -0.3211, -0.1042]],\n",
            "\n",
            "        [[-0.2645, -0.5605, -0.8985,  ..., -1.2017,  0.0065,  1.3170],\n",
            "         [ 0.7639, -0.1020,  0.8867,  ...,  0.0348,  0.3362,  0.0384],\n",
            "         [ 0.5887, -0.5086,  0.7386,  ...,  0.0283, -0.4732,  0.0539],\n",
            "         ...,\n",
            "         [ 0.2662, -0.5123,  0.9652,  ...,  0.2653, -0.4266,  0.2494],\n",
            "         [ 0.2614, -0.6217,  0.8518,  ...,  0.1588, -0.3996,  0.4772],\n",
            "         [ 0.0080, -0.6618,  0.7146,  ...,  0.1885, -0.4933,  0.2081]],\n",
            "\n",
            "        [[-0.2088, -1.2686, -0.7522,  ..., -0.8678,  0.0609,  0.6617],\n",
            "         [ 0.6008, -0.7495,  0.6141,  ...,  0.1395,  0.5673, -0.2393],\n",
            "         [ 0.0720, -1.0976, -0.1411,  ..., -1.1516,  0.8120, -0.0930],\n",
            "         ...,\n",
            "         [ 0.5199,  0.0244,  0.5824,  ...,  0.2850, -0.2358, -0.9565],\n",
            "         [-0.9731, -1.0886, -1.1229,  ..., -1.2382, -0.5768, -0.1575],\n",
            "         [-1.0882, -1.0562, -1.3779,  ..., -1.3614, -0.3789,  0.0979]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0141, -0.1849, -0.2167,  ..., -0.4665,  0.1231,  0.6209],\n",
            "         [ 0.3324,  0.3532,  1.1572,  ...,  0.0391, -0.1986, -0.0791],\n",
            "         [ 0.6263, -0.0301, -0.2366,  ...,  0.8203,  0.1626,  0.0803],\n",
            "         ...,\n",
            "         [-1.0849, -0.5459, -0.7072,  ..., -0.5200,  0.3127,  0.1902],\n",
            "         [-0.1967,  0.0703,  0.4715,  ...,  0.0797,  0.6685,  0.0539],\n",
            "         [ 0.2617, -0.0294,  0.8110,  ...,  0.2134,  0.3684, -0.3256]],\n",
            "\n",
            "        [[-0.8328, -0.5400, -0.7550,  ..., -0.8903,  0.4671,  0.8455],\n",
            "         [ 0.1341, -1.0340,  0.6763,  ..., -1.1209,  1.0164, -0.9055],\n",
            "         [-0.1277, -0.2843, -0.5556,  ..., -0.2405,  0.4270,  0.8027],\n",
            "         ...,\n",
            "         [-0.2109, -0.2607,  0.6300,  ...,  0.3934,  0.2598, -0.4841],\n",
            "         [-0.8013, -0.6438,  0.3828,  ..., -0.0345,  0.2236, -0.4927],\n",
            "         [-0.2280, -0.3204,  0.5630,  ...,  0.3907,  0.3227, -0.5496]],\n",
            "\n",
            "        [[ 0.0257, -0.4867, -0.2042,  ..., -0.5894,  0.2308,  0.7288],\n",
            "         [-0.0310,  0.1611,  1.0002,  ...,  0.2494,  0.7800, -0.7076],\n",
            "         [ 0.5865,  0.7410,  0.1088,  ..., -0.9593,  1.1910, -0.0773],\n",
            "         ...,\n",
            "         [ 0.2430,  0.4810,  1.5404,  ...,  0.1312,  0.2946, -0.5315],\n",
            "         [-0.0279,  0.2004,  1.5905,  ...,  0.2420,  0.1218, -0.4862],\n",
            "         [ 0.1338,  0.2037,  1.5028,  ...,  0.1776,  0.2555, -0.5439]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 9.0650e-02, -1.3615e-02, -8.6362e-01,  ..., -1.1858e+00,\n",
            "           4.5209e-01,  4.9231e-01],\n",
            "         [ 7.4603e-01, -4.4850e-01, -2.2320e-01,  ..., -5.8826e-01,\n",
            "           1.0008e+00, -5.5779e-01],\n",
            "         [ 3.7753e-01, -9.8528e-01,  1.2300e-01,  ..., -7.1711e-01,\n",
            "           1.3144e+00,  9.1935e-02],\n",
            "         ...,\n",
            "         [-8.5329e-01, -5.8140e-01, -9.6458e-01,  ..., -8.3848e-01,\n",
            "          -3.4744e-01,  3.0224e-01],\n",
            "         [ 3.8567e-01, -2.5687e-01,  2.3116e-01,  ..., -1.2512e-01,\n",
            "          -2.4311e-01,  7.8270e-02],\n",
            "         [ 5.7046e-01, -2.0070e-01,  3.6593e-01,  ..., -8.2149e-02,\n",
            "          -3.8990e-01,  1.7937e-02]],\n",
            "\n",
            "        [[-1.0873e+00, -3.6503e-01, -9.6483e-01,  ..., -1.1478e+00,\n",
            "          -1.4712e-01,  1.1512e+00],\n",
            "         [ 5.0144e-01, -1.1883e-01,  9.1475e-01,  ...,  1.4367e-03,\n",
            "           3.5702e-01, -5.1098e-02],\n",
            "         [ 6.9027e-01, -5.7847e-01,  9.3517e-01,  ...,  1.0091e-01,\n",
            "          -3.1188e-01,  3.3676e-02],\n",
            "         ...,\n",
            "         [ 4.8414e-01, -4.2836e-01,  7.5864e-01,  ...,  6.4406e-03,\n",
            "          -2.0093e-01,  1.8158e-01],\n",
            "         [ 4.4117e-01, -5.1710e-01,  7.3296e-01,  ..., -1.1796e-01,\n",
            "          -1.1981e-01,  3.9202e-01],\n",
            "         [ 1.0969e-01, -5.0277e-01,  5.4429e-01,  ..., -3.3994e-02,\n",
            "          -3.5740e-01,  2.0319e-01]],\n",
            "\n",
            "        [[-3.0727e-01, -7.4262e-01, -5.7930e-01,  ..., -5.7058e-01,\n",
            "          -3.2927e-01,  6.4543e-01],\n",
            "         [ 8.2669e-01, -4.6840e-01,  6.4349e-01,  ...,  3.4634e-01,\n",
            "           2.4264e-01, -4.2620e-01],\n",
            "         [ 2.3539e-01, -5.6045e-01,  7.4219e-02,  ..., -9.8389e-01,\n",
            "           1.3739e-02, -3.2688e-01],\n",
            "         ...,\n",
            "         [ 4.9581e-01,  1.7531e-01,  4.8847e-01,  ...,  1.7269e-01,\n",
            "          -5.2259e-01, -8.9526e-01],\n",
            "         [-6.6164e-01, -8.1623e-01, -1.0358e+00,  ..., -1.1396e+00,\n",
            "          -7.9641e-01, -1.9318e-01],\n",
            "         [-7.5390e-01, -7.0969e-01, -1.0623e+00,  ..., -1.1948e+00,\n",
            "          -6.8371e-01, -6.6365e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.4526e-01, -1.1958e-01, -2.1946e-01,  ...,  3.9111e-02,\n",
            "           8.3655e-03,  6.0608e-01],\n",
            "         [ 8.5411e-02,  5.9141e-01,  1.2350e+00,  ...,  2.7195e-01,\n",
            "           1.4363e-01,  1.0900e-01],\n",
            "         [ 5.1623e-01,  2.8567e-01,  1.6728e-02,  ...,  1.2552e+00,\n",
            "           3.6696e-01, -1.7107e-01],\n",
            "         ...,\n",
            "         [-8.3656e-01, -2.0255e-01, -6.5016e-01,  ..., -3.1409e-01,\n",
            "          -7.5766e-03, -8.8994e-02],\n",
            "         [-2.9160e-01,  3.3667e-01, -9.0600e-02,  ...,  4.4304e-01,\n",
            "           2.7113e-01,  2.0058e-01],\n",
            "         [ 2.6798e-01,  3.1504e-01,  3.3539e-01,  ...,  6.4862e-01,\n",
            "           3.9750e-03, -1.4793e-01]],\n",
            "\n",
            "        [[-1.0778e+00, -1.4854e-01, -2.5368e-01,  ..., -1.1258e+00,\n",
            "           3.2895e-01,  7.3161e-01],\n",
            "         [-3.8281e-01, -7.4275e-01,  7.3569e-01,  ..., -1.4308e+00,\n",
            "           1.1959e+00, -8.9143e-01],\n",
            "         [-9.0325e-01, -5.8050e-02, -3.7585e-01,  ..., -8.9170e-01,\n",
            "           6.1420e-01,  5.1995e-01],\n",
            "         ...,\n",
            "         [-5.1835e-01,  6.4924e-02,  3.8636e-01,  ...,  4.2559e-01,\n",
            "           5.4129e-01, -3.0666e-01],\n",
            "         [-9.1868e-01, -3.3148e-01,  2.8035e-01,  ..., -3.2367e-02,\n",
            "           2.8371e-01, -4.4562e-01],\n",
            "         [-4.6668e-01,  5.7353e-02,  4.9730e-01,  ...,  4.0591e-01,\n",
            "           4.2049e-01, -3.4559e-01]],\n",
            "\n",
            "        [[-5.2330e-01, -3.5556e-01, -2.6558e-01,  ..., -6.3744e-01,\n",
            "           2.6128e-01,  3.6987e-01],\n",
            "         [-1.0858e-01,  5.9535e-01,  1.0118e+00,  ..., -4.9675e-01,\n",
            "           5.3743e-01, -6.5156e-01],\n",
            "         [ 4.9635e-01,  1.0535e+00,  4.3412e-01,  ..., -9.3060e-01,\n",
            "           6.7988e-01, -2.3947e-01],\n",
            "         ...,\n",
            "         [ 4.8956e-01,  6.2696e-01,  1.5484e+00,  ...,  2.1282e-02,\n",
            "           3.7492e-01, -4.7722e-01],\n",
            "         [ 2.3206e-01,  4.5981e-01,  1.6041e+00,  ...,  1.3747e-01,\n",
            "           1.6729e-01, -4.3357e-01],\n",
            "         [ 3.2107e-01,  4.4032e-01,  1.5548e+00,  ...,  8.3301e-02,\n",
            "           2.6250e-01, -5.0760e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0760, -0.0923, -0.3451,  ..., -0.9935,  0.2165,  0.2749],\n",
            "         [ 1.0498, -0.8813, -0.2533,  ..., -0.5614,  0.8500, -0.8224],\n",
            "         [ 0.4364, -1.3079,  0.0473,  ..., -0.9196,  1.1802,  0.1474],\n",
            "         ...,\n",
            "         [-1.0383, -1.1971, -0.6049,  ..., -0.3903, -0.1173,  0.2731],\n",
            "         [ 0.1965, -0.2511,  0.2825,  ..., -0.2659, -0.1308, -0.0139],\n",
            "         [ 0.4764, -0.1571,  0.3306,  ..., -0.2891, -0.1954,  0.0091]],\n",
            "\n",
            "        [[-1.1552, -0.4767, -0.2124,  ..., -0.9395, -0.6610,  1.3787],\n",
            "         [ 0.4823, -0.3090,  1.0064,  ..., -0.1459,  0.0914, -0.3583],\n",
            "         [ 0.4945, -0.7935,  0.9360,  ...,  0.1151, -0.5844,  0.0131],\n",
            "         ...,\n",
            "         [ 0.4817, -0.4499,  0.5035,  ..., -0.1991, -0.7429,  0.2049],\n",
            "         [ 0.4206, -0.5152,  0.5112,  ..., -0.2840, -0.7159,  0.3272],\n",
            "         [ 0.0758, -0.4755,  0.5321,  ..., -0.1747, -0.8994,  0.3204]],\n",
            "\n",
            "        [[-0.8227, -0.7984, -0.4472,  ..., -0.2828, -0.9279,  0.4156],\n",
            "         [ 0.6149, -0.6181,  0.7110,  ...,  0.0162, -0.1377, -0.5724],\n",
            "         [ 0.0363, -0.8531,  0.0882,  ..., -0.9798, -0.0030, -0.4156],\n",
            "         ...,\n",
            "         [ 0.4510, -0.0064,  0.4034,  ..., -0.0040, -0.4688, -0.6133],\n",
            "         [-0.4755, -0.8670, -0.9142,  ..., -1.0435, -0.7199, -0.1234],\n",
            "         [-0.5692, -0.6966, -1.0111,  ..., -1.0591, -0.6217, -0.0155]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.7967, -0.1384, -0.2509,  ...,  0.0052, -0.1927,  0.1763],\n",
            "         [-0.0547,  0.3855,  0.9468,  ...,  0.1013, -0.0577, -0.0462],\n",
            "         [ 0.5693, -0.1226,  0.1616,  ...,  1.0619,  0.6676, -0.2444],\n",
            "         ...,\n",
            "         [-1.0314, -0.4969, -0.5110,  ...,  0.1963,  0.1735, -0.3489],\n",
            "         [-0.6094,  0.0992,  0.2356,  ...,  1.0304,  0.1285,  0.2143],\n",
            "         [-0.1332,  0.0701,  0.5500,  ...,  0.6740, -0.0066, -0.2226]],\n",
            "\n",
            "        [[-1.3985, -0.5732, -0.1660,  ..., -1.3131,  0.2981,  0.2211],\n",
            "         [-0.7466, -1.1243,  0.9379,  ..., -2.1025,  1.0365, -0.9452],\n",
            "         [-1.2476, -0.3071,  0.4152,  ..., -1.3598,  0.7715, -0.2381],\n",
            "         ...,\n",
            "         [-0.7279,  0.0683,  0.9017,  ..., -0.3525,  0.6799, -0.7928],\n",
            "         [-1.0609, -0.4322,  0.8299,  ..., -0.3028,  0.4680, -0.8600],\n",
            "         [-0.8507,  0.0082,  1.0417,  ..., -0.1801,  0.5270, -0.7312]],\n",
            "\n",
            "        [[-1.0113, -0.2886,  0.0344,  ..., -0.6395,  0.1200,  0.1098],\n",
            "         [-0.2631,  0.3461,  1.1671,  ..., -0.9439,  0.6598, -0.8768],\n",
            "         [ 0.0849,  0.6586,  0.3681,  ..., -1.3215,  0.1072, -0.4226],\n",
            "         ...,\n",
            "         [ 0.3735,  0.5345,  1.4248,  ..., -0.5429,  0.1293, -0.4250],\n",
            "         [ 0.0597,  0.4251,  1.4889,  ..., -0.3712, -0.0922, -0.4481],\n",
            "         [ 0.1203,  0.3144,  1.4674,  ..., -0.4007,  0.0169, -0.5464]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.7171e-01, -1.1311e-01, -1.5557e-01,  ..., -7.7602e-01,\n",
            "           7.8066e-02, -2.0402e-02],\n",
            "         [ 1.1130e+00, -2.9833e-01, -2.9320e-01,  ..., -4.9640e-01,\n",
            "           6.4020e-01, -5.6225e-01],\n",
            "         [ 5.0763e-01, -1.0130e+00, -2.4711e-01,  ..., -7.0181e-01,\n",
            "           1.0104e+00,  2.9565e-01],\n",
            "         ...,\n",
            "         [-5.4470e-01, -1.0118e+00, -4.0687e-01,  ...,  1.1869e-01,\n",
            "           1.0412e-01, -8.7507e-03],\n",
            "         [ 5.0976e-01, -1.3576e-01,  1.6290e-01,  ..., -4.8377e-01,\n",
            "          -1.0354e-01, -3.4757e-02],\n",
            "         [ 7.8619e-01, -1.1177e-02,  1.8779e-01,  ..., -5.1192e-01,\n",
            "          -1.3012e-01,  2.4834e-02]],\n",
            "\n",
            "        [[-8.1190e-01, -1.9488e-01, -2.1497e-01,  ..., -8.0873e-01,\n",
            "          -3.3837e-01,  9.9555e-01],\n",
            "         [ 6.9206e-01, -6.5930e-02,  5.4332e-01,  ..., -2.3913e-01,\n",
            "          -1.8229e-01, -9.1208e-02],\n",
            "         [ 7.1617e-01, -5.0179e-01,  7.1387e-01,  ..., -3.7119e-02,\n",
            "          -6.9212e-01,  3.9745e-02],\n",
            "         ...,\n",
            "         [ 5.4623e-01, -3.6854e-01,  1.1655e-01,  ..., -2.4174e-01,\n",
            "          -8.4039e-01,  2.3795e-01],\n",
            "         [ 5.0785e-01, -4.2456e-01,  1.2808e-01,  ..., -2.9010e-01,\n",
            "          -8.1137e-01,  3.4813e-01],\n",
            "         [ 3.0982e-01, -2.1311e-01,  1.4149e-01,  ..., -2.5365e-01,\n",
            "          -9.8750e-01,  1.6755e-01]],\n",
            "\n",
            "        [[-6.5933e-01, -4.9022e-01, -1.3521e-01,  ..., -6.1760e-01,\n",
            "          -1.0097e+00,  1.2048e-01],\n",
            "         [ 3.6707e-01, -5.8696e-01,  3.1535e-01,  ..., -4.7431e-01,\n",
            "          -4.9063e-01, -4.5805e-01],\n",
            "         [-2.2147e-01, -3.9689e-01,  1.5269e-01,  ..., -9.3818e-01,\n",
            "          -3.7142e-01, -1.6544e-01],\n",
            "         ...,\n",
            "         [ 2.2377e-01, -3.8337e-01,  2.4211e-01,  ..., -2.0952e-01,\n",
            "          -6.2173e-01, -4.4908e-01],\n",
            "         [-2.0183e-01, -9.4369e-01, -6.9205e-01,  ..., -6.9606e-01,\n",
            "          -8.4224e-01, -4.3080e-01],\n",
            "         [-2.8149e-01, -7.9235e-01, -8.3195e-01,  ..., -7.2148e-01,\n",
            "          -7.9624e-01, -3.2109e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-6.6066e-01, -8.5240e-02, -4.3396e-01,  ..., -5.2909e-02,\n",
            "          -3.5699e-01, -2.8221e-01],\n",
            "         [-8.4687e-02,  5.0725e-02,  1.9508e-01,  ...,  3.0044e-01,\n",
            "           1.8996e-01, -2.5410e-01],\n",
            "         [ 7.6397e-01, -1.5225e-02, -1.4585e-01,  ...,  6.4991e-01,\n",
            "           5.8998e-01, -5.9614e-01],\n",
            "         ...,\n",
            "         [-7.5897e-01, -5.0684e-01, -6.6293e-01,  ...,  5.6639e-01,\n",
            "          -6.4150e-04, -8.3741e-01],\n",
            "         [-6.3273e-01,  1.3903e-01, -1.1229e-01,  ...,  7.9802e-01,\n",
            "          -2.1529e-01, -2.3344e-01],\n",
            "         [-2.9887e-01, -3.6108e-01,  2.7048e-01,  ...,  6.5488e-01,\n",
            "          -1.5956e-01, -4.1493e-01]],\n",
            "\n",
            "        [[-1.1764e+00, -1.7553e-01, -4.0178e-01,  ..., -1.2818e+00,\n",
            "           2.9518e-01,  2.4226e-02],\n",
            "         [-6.8425e-01, -9.0364e-01,  4.3258e-01,  ..., -1.9868e+00,\n",
            "           8.5142e-01, -9.0237e-01],\n",
            "         [-9.9336e-01, -6.4241e-02, -2.6392e-02,  ..., -1.2727e+00,\n",
            "           5.4489e-01, -3.2649e-01],\n",
            "         ...,\n",
            "         [-1.1056e+00, -2.2091e-02,  9.6310e-01,  ..., -5.6744e-01,\n",
            "           4.1858e-01, -4.7419e-01],\n",
            "         [-1.2645e+00, -3.9906e-01,  7.8451e-01,  ..., -3.2236e-01,\n",
            "           2.6166e-01, -7.8413e-01],\n",
            "         [-1.1704e+00, -7.0397e-02,  8.9474e-01,  ..., -3.0069e-01,\n",
            "           3.3217e-01, -5.4202e-01]],\n",
            "\n",
            "        [[-7.9740e-01,  3.7670e-01,  8.8222e-02,  ..., -5.9614e-01,\n",
            "          -4.7586e-03, -1.2145e-01],\n",
            "         [-1.0726e-01,  4.8058e-01,  5.4293e-01,  ..., -1.0978e+00,\n",
            "           3.6767e-01, -5.7244e-01],\n",
            "         [ 4.3598e-02,  9.5238e-01, -1.8758e-01,  ..., -1.7479e+00,\n",
            "          -1.0308e-01, -2.0003e-01],\n",
            "         ...,\n",
            "         [ 4.2242e-01,  6.3070e-01,  1.2532e+00,  ..., -1.0746e+00,\n",
            "          -8.5430e-02, -3.5102e-01],\n",
            "         [ 6.0776e-02,  4.4284e-01,  1.2749e+00,  ..., -9.2410e-01,\n",
            "          -2.3326e-01, -3.9792e-01],\n",
            "         [ 1.3860e-01,  3.6495e-01,  1.2538e+00,  ..., -8.6513e-01,\n",
            "          -1.8124e-01, -4.9298e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.1008e-01,  2.0528e-01, -1.4847e-01,  ..., -5.3590e-01,\n",
            "           4.2048e-01,  2.7359e-01],\n",
            "         [ 1.0002e+00,  4.1562e-02, -2.7572e-01,  ..., -8.5971e-01,\n",
            "           4.9414e-01, -3.6458e-01],\n",
            "         [ 4.6754e-01, -4.8304e-01, -9.1953e-02,  ..., -8.7756e-01,\n",
            "           9.0378e-01, -1.3082e-01],\n",
            "         ...,\n",
            "         [ 1.0677e-01, -8.5856e-01, -1.3957e-01,  ...,  3.6273e-01,\n",
            "           5.2746e-01, -2.3511e-01],\n",
            "         [ 3.8209e-01, -5.2103e-02,  1.6364e-02,  ..., -2.6334e-01,\n",
            "           3.0532e-01, -1.5548e-03],\n",
            "         [ 4.9810e-01,  7.9557e-03,  4.3164e-02,  ..., -2.6970e-01,\n",
            "           2.5408e-01,  1.9860e-02]],\n",
            "\n",
            "        [[-5.9768e-01,  1.7950e-01, -2.0828e-01,  ..., -4.4094e-01,\n",
            "           2.8299e-01,  9.0861e-01],\n",
            "         [ 4.5615e-01, -2.4149e-01,  3.0020e-03,  ...,  1.2696e-01,\n",
            "           5.2834e-01,  5.6157e-02],\n",
            "         [ 3.9882e-01, -6.0239e-01,  3.0984e-01,  ...,  1.0672e-01,\n",
            "          -3.5830e-01,  2.5975e-01],\n",
            "         ...,\n",
            "         [ 3.3780e-01, -1.7308e-01,  1.6457e-01,  ...,  3.3213e-02,\n",
            "          -1.6684e-01,  1.5242e-01],\n",
            "         [ 3.2737e-01, -1.8516e-01,  1.6065e-01,  ...,  9.4583e-03,\n",
            "          -1.5629e-01,  1.9401e-01],\n",
            "         [ 2.5044e-01, -1.6344e-01,  1.5810e-01,  ...,  6.4392e-02,\n",
            "          -2.0145e-01,  1.3012e-01]],\n",
            "\n",
            "        [[-2.8598e-01,  1.4456e-02,  5.6552e-02,  ..., -3.4352e-01,\n",
            "          -1.5251e-01,  1.8009e-01],\n",
            "         [ 3.4474e-01, -6.1410e-01,  2.0272e-01,  ..., -5.7820e-01,\n",
            "           7.9265e-02, -1.9771e-01],\n",
            "         [-1.6459e-01, -1.2269e-01, -1.3902e-01,  ..., -7.7720e-01,\n",
            "          -3.5327e-01, -2.0034e-01],\n",
            "         ...,\n",
            "         [ 9.8724e-02, -2.1614e-01,  2.1267e-01,  ..., -5.4895e-02,\n",
            "           8.5158e-03, -5.4150e-04],\n",
            "         [ 3.2965e-03, -5.4691e-01, -3.6725e-01,  ..., -1.7426e-01,\n",
            "           3.4314e-02, -2.4477e-01],\n",
            "         [-5.3488e-02, -4.7498e-01, -5.1290e-01,  ..., -1.5896e-01,\n",
            "          -1.1705e-02, -2.7569e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.4540e-01,  4.3532e-02, -1.5263e-01,  ..., -1.6401e-01,\n",
            "           1.3079e-01,  6.4017e-02],\n",
            "         [ 1.1451e-01,  2.6388e-02, -5.6629e-02,  ...,  5.7179e-01,\n",
            "           5.9083e-01,  3.5776e-01],\n",
            "         [ 5.4174e-01, -1.1679e-01, -3.1221e-01,  ...,  4.1923e-01,\n",
            "           6.4285e-01, -8.7305e-02],\n",
            "         ...,\n",
            "         [-3.4530e-01, -5.5465e-01, -4.0498e-01,  ...,  5.7313e-01,\n",
            "           3.7932e-01, -4.2157e-01],\n",
            "         [-2.0110e-01, -6.6158e-02, -4.2682e-02,  ...,  3.4893e-01,\n",
            "           8.2896e-02, -4.6332e-02],\n",
            "         [-7.9667e-02, -1.2613e-01,  1.4653e-01,  ...,  3.1945e-01,\n",
            "           1.1273e-01, -1.7358e-01]],\n",
            "\n",
            "        [[-5.1595e-01, -7.7584e-02, -2.9248e-01,  ..., -6.4900e-01,\n",
            "           2.8751e-01,  2.0538e-01],\n",
            "         [-3.3158e-01, -5.1478e-01, -1.3732e-01,  ..., -4.6023e-01,\n",
            "           1.0187e+00, -2.0837e-01],\n",
            "         [ 3.5053e-02, -2.3717e-01, -1.7723e-01,  ..., -9.5049e-01,\n",
            "          -2.1213e-02, -3.5468e-01],\n",
            "         ...,\n",
            "         [-2.9136e-01, -1.4139e-01,  4.4569e-01,  ..., -1.6129e-01,\n",
            "           2.6986e-01, -2.2288e-01],\n",
            "         [-3.1349e-01, -2.9573e-01,  3.3053e-01,  ...,  8.4915e-02,\n",
            "           3.3582e-01, -3.6331e-01],\n",
            "         [-2.7864e-01, -1.1149e-01,  3.9131e-01,  ..., -1.0200e-02,\n",
            "           2.5705e-01, -2.4975e-01]],\n",
            "\n",
            "        [[-4.1400e-01,  1.4952e-01,  6.1106e-02,  ..., -5.6709e-01,\n",
            "           3.2079e-01,  4.3859e-02],\n",
            "         [ 1.8934e-01,  2.1137e-01,  2.9948e-01,  ..., -8.3885e-01,\n",
            "           3.6380e-01, -4.6225e-01],\n",
            "         [-1.0355e-01,  4.5097e-01, -2.7926e-01,  ..., -9.5024e-01,\n",
            "          -1.8997e-01, -2.1213e-01],\n",
            "         ...,\n",
            "         [ 1.8047e-01,  1.4699e-01,  4.0174e-01,  ..., -2.9283e-01,\n",
            "           6.8139e-02, -7.4605e-02],\n",
            "         [ 7.2907e-02,  1.0501e-01,  4.2132e-01,  ..., -2.4687e-01,\n",
            "           3.2011e-02, -8.2694e-02],\n",
            "         [ 9.2091e-02,  8.2251e-02,  4.1939e-01,  ..., -2.2621e-01,\n",
            "           3.4394e-02, -1.2638e-01]]], grad_fn=<NativeLayerNormBackward0>))\n"
          ]
        }
      ],
      "source": [
        "print(f\"size of last hidden state: {output[0].shape}\\n type of last hidden state: {type(output[0])}\\nlast hidden state: {output[0]}\")\n",
        "print(f\"\\n\\nsize of pooler output: {output[1].shape}\\n type of pooler output: {type(output[1])}\\npooler output: {output[1]}\")\n",
        "print(f\"\\n\\nsize of hidden states: {output[2].__len__()}\\n type of hidden states: {type(output[2])}\\nhidden states: {output[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcQ-i-ZMq8Fy"
      },
      "outputs": [],
      "source": [
        "bn = nn.BatchNorm1d(768)(output.pooler_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjSRCON7q8Fy",
        "outputId": "fa31569f-64e9-4966-9b2f-60f205df005c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 768]), torch.Size([32, 768]))"
            ]
          },
          "execution_count": 1239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bn.shape , output.pooler_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0gSn-6Bq8Fz",
        "outputId": "612cc078-e657-4b80-e84a-6145614ae40b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 5])"
            ]
          },
          "execution_count": 1236,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testvec= torch.Tensor([range(1,6), range(6,11), range(11,16)])\n",
        "testvec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0huwzoJq8Fz",
        "outputId": "ce7dd572-e48c-4b36-e4f4-06822ec84803"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 50, 768])"
            ]
          },
          "execution_count": 1248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "last4layers =torch.stack( output.hidden_states[-4:] )\n",
        "last4layers.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjlvrZBTq8F0",
        "outputId": "b5079cee-477c-4df3-81d8-34460a6daf6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 50, 768])"
            ]
          },
          "execution_count": 1249,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sumoflayers = last4layers.sum(0)\n",
        "sumoflayers.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLVQ3z6rq8F0",
        "outputId": "2e8512e1-4732-4f7f-a823-80105d05f9a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "execution_count": 1251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_layer= sumoflayers.mean(1)\n",
        "mean_layer.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuEpUaIgq8F0",
        "outputId": "ab5b7c71-db06-4ef5-eae6-6bfef8a9b4a6",
        "collapsed": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "execution_count": 1252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bntest= nn.BatchNorm1d(768)(mean_layer)\n",
        "bntest.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZALakn3Wq8F1"
      },
      "outputs": [],
      "source": [
        "sumoflayers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq31WEprq8F1",
        "outputId": "467d5831-b65a-4ce3-dcb6-eea8c7fd10cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 50])"
            ]
          },
          "execution_count": 1292,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sol= sumoflayers.permute(0,2,1)\n",
        "sol.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKJ84OPhq8F1",
        "outputId": "908ba7c2-c7e4-4862-b2aa-851e64cd33ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 50])"
            ]
          },
          "execution_count": 1293,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bntest1= nn.BatchNorm1d(768)(sol)\n",
        "bntest1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi7z67kTq8F1",
        "outputId": "66d7f81f-54fb-43cd-f247-719323ca8340"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 2.7435e-01,  3.4908e-02, -6.6871e-01,  ..., -1.5974e+00,\n",
              "           5.8917e-01,  5.2032e-01],\n",
              "         [ 1.6688e+00, -6.2109e-01, -3.9560e-01,  ..., -1.0041e+00,\n",
              "           1.2838e+00, -9.2129e-01],\n",
              "         [ 7.6546e-01, -1.4964e+00, -2.8418e-02,  ..., -1.2641e+00,\n",
              "           1.8277e+00,  2.0390e-01],\n",
              "         ...,\n",
              "         [-7.0532e-01, -1.1272e+00, -6.3703e-01,  ..., -1.9944e-01,\n",
              "           9.2883e-02,  1.4552e-01],\n",
              "         [ 5.1348e-01, -1.8409e-01,  2.6236e-01,  ..., -3.2629e-01,\n",
              "          -1.5740e-02,  4.8617e-02],\n",
              "         [ 7.7319e-01, -7.4267e-02,  3.3136e-01,  ..., -3.2353e-01,\n",
              "          -1.0584e-01,  6.1969e-02]],\n",
              "\n",
              "        [[-1.6729e+00, -3.6097e-01, -7.0992e-01,  ..., -1.5250e+00,\n",
              "          -3.6396e-01,  2.1226e+00],\n",
              "         [ 9.2830e-01, -2.6635e-01,  1.0681e+00,  ..., -6.6946e-02,\n",
              "           3.7104e-01, -1.4518e-01],\n",
              "         [ 9.7254e-01, -9.6403e-01,  1.2138e+00,  ...,  1.5584e-01,\n",
              "          -7.4932e-01,  1.8042e-01],\n",
              "         ...,\n",
              "         [ 6.3104e-01, -4.1446e-01,  5.3299e-01,  ..., -8.8738e-02,\n",
              "          -5.8433e-01,  2.8794e-01],\n",
              "         [ 5.8516e-01, -4.8828e-01,  5.3240e-01,  ..., -1.7982e-01,\n",
              "          -5.4015e-01,  4.4511e-01],\n",
              "         [ 2.7415e-01, -3.8709e-01,  4.7254e-01,  ..., -8.5889e-02,\n",
              "          -7.3050e-01,  2.9790e-01]],\n",
              "\n",
              "        [[-9.3275e-01, -9.0532e-01, -4.7740e-01,  ..., -8.1034e-01,\n",
              "          -1.0943e+00,  6.8044e-01],\n",
              "         [ 9.3727e-01, -9.1316e-01,  8.2025e-01,  ..., -2.4749e-01,\n",
              "          -8.7678e-02, -6.4932e-01],\n",
              "         [-6.3586e-03, -7.4385e-01,  1.1141e-01,  ..., -1.4518e+00,\n",
              "          -2.4949e-01, -4.0935e-01],\n",
              "         ...,\n",
              "         [ 4.4538e-01, -9.8164e-02,  4.7014e-01,  ...,  8.9137e-03,\n",
              "          -4.7353e-01, -5.8660e-01],\n",
              "         [-3.8980e-01, -9.8072e-01, -9.2785e-01,  ..., -9.4199e-01,\n",
              "          -7.0757e-01, -2.7935e-01],\n",
              "         [-4.8254e-01, -8.0222e-01, -1.0366e+00,  ..., -9.4727e-01,\n",
              "          -6.2586e-01, -1.7423e-01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-9.1999e-01, -9.9322e-02, -4.5477e-01,  ..., -3.9682e-02,\n",
              "          -1.5135e-01,  3.0613e-01],\n",
              "         [ 6.5225e-02,  4.7916e-01,  1.0068e+00,  ...,  5.5894e-01,\n",
              "           4.0113e-01,  1.0934e-01],\n",
              "         [ 1.0096e+00,  5.2634e-02, -7.3394e-02,  ...,  1.4131e+00,\n",
              "           9.5940e-01, -4.0556e-01],\n",
              "         ...,\n",
              "         [-9.1083e-01, -5.2353e-01, -6.7320e-01,  ...,  3.6623e-01,\n",
              "           2.1366e-01, -5.0304e-01],\n",
              "         [-5.1809e-01,  2.0316e-01,  3.6419e-02,  ...,  8.8203e-01,\n",
              "           1.2552e-01,  8.3039e-02],\n",
              "         [-3.7332e-02,  7.2454e-03,  4.4935e-01,  ...,  7.6244e-01,\n",
              "           2.3821e-02, -2.6252e-01]],\n",
              "\n",
              "        [[-1.9154e+00, -4.1623e-01, -4.8155e-01,  ..., -2.0097e+00,\n",
              "           6.0916e-01,  5.9628e-01],\n",
              "         [-8.5388e-01, -1.3290e+00,  8.6037e-01,  ..., -2.4518e+00,\n",
              "           1.7494e+00, -1.1881e+00],\n",
              "         [-1.2207e+00, -2.3025e-01, -2.6577e-02,  ..., -1.7744e+00,\n",
              "           8.1426e-01, -1.2188e-01],\n",
              "         ...,\n",
              "         [-8.0562e-01,  2.9858e-02,  9.0184e-01,  ..., -1.7009e-01,\n",
              "           6.5013e-01, -5.3489e-01],\n",
              "         [-1.1041e+00, -4.2926e-01,  7.5499e-01,  ..., -1.4446e-01,\n",
              "           4.7334e-01, -7.4900e-01],\n",
              "         [-8.3146e-01,  2.7629e-03,  9.2867e-01,  ...,  1.2599e-02,\n",
              "           5.2311e-01, -5.4881e-01]],\n",
              "\n",
              "        [[-1.2476e+00, -1.4008e-02,  2.9118e-03,  ..., -1.1041e+00,\n",
              "           3.6865e-01,  2.3007e-01],\n",
              "         [-8.0669e-02,  7.2059e-01,  1.2989e+00,  ..., -1.3672e+00,\n",
              "           8.4361e-01, -1.0280e+00],\n",
              "         [ 2.5140e-01,  1.3033e+00,  1.7600e-01,  ..., -1.9672e+00,\n",
              "           2.4032e-01, -3.9555e-01],\n",
              "         ...,\n",
              "         [ 5.0826e-01,  6.5959e-01,  1.5194e+00,  ..., -5.6450e-01,\n",
              "           1.9523e-01, -3.8503e-01],\n",
              "         [ 1.7638e-01,  5.0022e-01,  1.5793e+00,  ..., -4.1196e-01,\n",
              "          -9.4815e-04, -3.9834e-01],\n",
              "         [ 2.5094e-01,  4.1774e-01,  1.5174e+00,  ..., -4.0405e-01,\n",
              "           8.1115e-02, -4.8734e-01]]], grad_fn=<NativeBatchNormBackward0>)"
            ]
          },
          "execution_count": 1290,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bntest1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkeZRuz3q8F1"
      },
      "outputs": [],
      "source": [
        "bnt1mean= bntest1.view(1,32,50,768).mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umh1ocJFq8F1",
        "outputId": "2abdc478-772d-4330-99b7-40e1085ddd8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 2.7435e-01,  3.4908e-02, -6.6871e-01,  ..., -1.5974e+00,\n",
              "           5.8917e-01,  5.2032e-01],\n",
              "         [ 1.6688e+00, -6.2109e-01, -3.9560e-01,  ..., -1.0041e+00,\n",
              "           1.2838e+00, -9.2129e-01],\n",
              "         [ 7.6546e-01, -1.4964e+00, -2.8418e-02,  ..., -1.2641e+00,\n",
              "           1.8277e+00,  2.0390e-01],\n",
              "         ...,\n",
              "         [-7.0532e-01, -1.1272e+00, -6.3703e-01,  ..., -1.9944e-01,\n",
              "           9.2883e-02,  1.4552e-01],\n",
              "         [ 5.1348e-01, -1.8409e-01,  2.6236e-01,  ..., -3.2629e-01,\n",
              "          -1.5740e-02,  4.8617e-02],\n",
              "         [ 7.7319e-01, -7.4267e-02,  3.3136e-01,  ..., -3.2353e-01,\n",
              "          -1.0584e-01,  6.1969e-02]],\n",
              "\n",
              "        [[-1.6729e+00, -3.6097e-01, -7.0992e-01,  ..., -1.5250e+00,\n",
              "          -3.6396e-01,  2.1226e+00],\n",
              "         [ 9.2830e-01, -2.6635e-01,  1.0681e+00,  ..., -6.6946e-02,\n",
              "           3.7104e-01, -1.4518e-01],\n",
              "         [ 9.7254e-01, -9.6403e-01,  1.2138e+00,  ...,  1.5584e-01,\n",
              "          -7.4932e-01,  1.8042e-01],\n",
              "         ...,\n",
              "         [ 6.3104e-01, -4.1446e-01,  5.3299e-01,  ..., -8.8738e-02,\n",
              "          -5.8433e-01,  2.8794e-01],\n",
              "         [ 5.8516e-01, -4.8828e-01,  5.3240e-01,  ..., -1.7982e-01,\n",
              "          -5.4015e-01,  4.4511e-01],\n",
              "         [ 2.7415e-01, -3.8709e-01,  4.7254e-01,  ..., -8.5889e-02,\n",
              "          -7.3050e-01,  2.9790e-01]],\n",
              "\n",
              "        [[-9.3275e-01, -9.0532e-01, -4.7740e-01,  ..., -8.1034e-01,\n",
              "          -1.0943e+00,  6.8044e-01],\n",
              "         [ 9.3727e-01, -9.1316e-01,  8.2025e-01,  ..., -2.4749e-01,\n",
              "          -8.7678e-02, -6.4932e-01],\n",
              "         [-6.3586e-03, -7.4385e-01,  1.1141e-01,  ..., -1.4518e+00,\n",
              "          -2.4949e-01, -4.0935e-01],\n",
              "         ...,\n",
              "         [ 4.4538e-01, -9.8164e-02,  4.7014e-01,  ...,  8.9137e-03,\n",
              "          -4.7353e-01, -5.8660e-01],\n",
              "         [-3.8980e-01, -9.8072e-01, -9.2785e-01,  ..., -9.4199e-01,\n",
              "          -7.0757e-01, -2.7935e-01],\n",
              "         [-4.8254e-01, -8.0222e-01, -1.0366e+00,  ..., -9.4727e-01,\n",
              "          -6.2586e-01, -1.7423e-01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-9.1999e-01, -9.9322e-02, -4.5477e-01,  ..., -3.9682e-02,\n",
              "          -1.5135e-01,  3.0613e-01],\n",
              "         [ 6.5225e-02,  4.7916e-01,  1.0068e+00,  ...,  5.5894e-01,\n",
              "           4.0113e-01,  1.0934e-01],\n",
              "         [ 1.0096e+00,  5.2634e-02, -7.3394e-02,  ...,  1.4131e+00,\n",
              "           9.5940e-01, -4.0556e-01],\n",
              "         ...,\n",
              "         [-9.1083e-01, -5.2353e-01, -6.7320e-01,  ...,  3.6623e-01,\n",
              "           2.1366e-01, -5.0304e-01],\n",
              "         [-5.1809e-01,  2.0316e-01,  3.6419e-02,  ...,  8.8203e-01,\n",
              "           1.2552e-01,  8.3039e-02],\n",
              "         [-3.7332e-02,  7.2454e-03,  4.4935e-01,  ...,  7.6244e-01,\n",
              "           2.3821e-02, -2.6252e-01]],\n",
              "\n",
              "        [[-1.9154e+00, -4.1623e-01, -4.8155e-01,  ..., -2.0097e+00,\n",
              "           6.0916e-01,  5.9628e-01],\n",
              "         [-8.5388e-01, -1.3290e+00,  8.6037e-01,  ..., -2.4518e+00,\n",
              "           1.7494e+00, -1.1881e+00],\n",
              "         [-1.2207e+00, -2.3025e-01, -2.6577e-02,  ..., -1.7744e+00,\n",
              "           8.1426e-01, -1.2188e-01],\n",
              "         ...,\n",
              "         [-8.0562e-01,  2.9858e-02,  9.0184e-01,  ..., -1.7009e-01,\n",
              "           6.5013e-01, -5.3489e-01],\n",
              "         [-1.1041e+00, -4.2926e-01,  7.5499e-01,  ..., -1.4446e-01,\n",
              "           4.7334e-01, -7.4900e-01],\n",
              "         [-8.3146e-01,  2.7629e-03,  9.2867e-01,  ...,  1.2599e-02,\n",
              "           5.2311e-01, -5.4881e-01]],\n",
              "\n",
              "        [[-1.2476e+00, -1.4008e-02,  2.9118e-03,  ..., -1.1041e+00,\n",
              "           3.6865e-01,  2.3007e-01],\n",
              "         [-8.0669e-02,  7.2059e-01,  1.2989e+00,  ..., -1.3672e+00,\n",
              "           8.4361e-01, -1.0280e+00],\n",
              "         [ 2.5140e-01,  1.3033e+00,  1.7600e-01,  ..., -1.9672e+00,\n",
              "           2.4032e-01, -3.9555e-01],\n",
              "         ...,\n",
              "         [ 5.0826e-01,  6.5959e-01,  1.5194e+00,  ..., -5.6450e-01,\n",
              "           1.9523e-01, -3.8503e-01],\n",
              "         [ 1.7638e-01,  5.0022e-01,  1.5793e+00,  ..., -4.1196e-01,\n",
              "          -9.4815e-04, -3.9834e-01],\n",
              "         [ 2.5094e-01,  4.1774e-01,  1.5174e+00,  ..., -4.0405e-01,\n",
              "           8.1115e-02, -4.8734e-01]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "execution_count": 1272,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bnt1mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r43M5NoJq8F2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jTgJE4idq8FL",
        "Avz4hKDOq8FR",
        "jMWzffMdq8FZ",
        "wtuNdaU9q8Fc",
        "CGQlu2_fq8Fj",
        "pkjWaMp9q8Fl",
        "wme9xd0It7Ok",
        "SqAVkwSkq8Fo",
        "FKK16C-yq8Fp",
        "kBFOad5Iq8Fp",
        "zRiIWNU-q8Fq",
        "M97OXP5jq8Fu"
      ]
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b2b5d9155204ae6b1344b2341b6f623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18026e903e764ad1b4fb625eadda641e",
              "IPY_MODEL_87df88b8775a4405be41b879dfa47480",
              "IPY_MODEL_a9a786df8e2f4548a3b474a11a0f1f99"
            ],
            "layout": "IPY_MODEL_2557296f57a243e2ad2fb672aa0ceb66"
          }
        },
        "18026e903e764ad1b4fb625eadda641e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d134ede89774f108a18df74d0ad4ea8",
            "placeholder": "​",
            "style": "IPY_MODEL_0b3fff64d0254888ba5e85fac07758a6",
            "value": "Downloading: 100%"
          }
        },
        "87df88b8775a4405be41b879dfa47480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0fe8afb716543d3a1b05cb80cccf645",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a2023d6cca84ca8a2d556b6a8e62e27",
            "value": 231508
          }
        },
        "a9a786df8e2f4548a3b474a11a0f1f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f9f752d4c7f4d3e8377c5cf95832704",
            "placeholder": "​",
            "style": "IPY_MODEL_8bc9a56fac744b19a35de6b20a6fdcd3",
            "value": " 226k/226k [00:00&lt;00:00, 1.05MB/s]"
          }
        },
        "2557296f57a243e2ad2fb672aa0ceb66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d134ede89774f108a18df74d0ad4ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b3fff64d0254888ba5e85fac07758a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0fe8afb716543d3a1b05cb80cccf645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2023d6cca84ca8a2d556b6a8e62e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f9f752d4c7f4d3e8377c5cf95832704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc9a56fac744b19a35de6b20a6fdcd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1437cbf59fbe4293987c3891a06e82b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ef46eefed6b40809ac7400b7966d4fd",
              "IPY_MODEL_dd497e9863a84a13b846a6416fe20e27",
              "IPY_MODEL_28b4dcb1aca549c0b99c61c2d46eeed5"
            ],
            "layout": "IPY_MODEL_3950f1e69ca642eea38407cd4cd6df60"
          }
        },
        "7ef46eefed6b40809ac7400b7966d4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_401dddf58e634617b22a5e050288f4ef",
            "placeholder": "​",
            "style": "IPY_MODEL_68140a16ba084d2d8dcac19280519382",
            "value": "Downloading: 100%"
          }
        },
        "dd497e9863a84a13b846a6416fe20e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_196a3d5d57fd45918173749e32ddac70",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a7832b284344c0e92b3372f964c15cf",
            "value": 28
          }
        },
        "28b4dcb1aca549c0b99c61c2d46eeed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bcd9292e2ce4140ba14bee9a9998a14",
            "placeholder": "​",
            "style": "IPY_MODEL_f2d6d8a9731e4161bb2da99ab7623fb9",
            "value": " 28.0/28.0 [00:00&lt;00:00, 649B/s]"
          }
        },
        "3950f1e69ca642eea38407cd4cd6df60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "401dddf58e634617b22a5e050288f4ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68140a16ba084d2d8dcac19280519382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "196a3d5d57fd45918173749e32ddac70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7832b284344c0e92b3372f964c15cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bcd9292e2ce4140ba14bee9a9998a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d6d8a9731e4161bb2da99ab7623fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10e52e18a3754354a9541be8a5019fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2978a7ad33442429fc9344405a6c5f8",
              "IPY_MODEL_7b7052eb10504ec88bdb8a3eda51aaf9",
              "IPY_MODEL_6c77e47405f24b9ea21b288a4aee06e1"
            ],
            "layout": "IPY_MODEL_443b9ec2fab740efa025b56c346cfff8"
          }
        },
        "c2978a7ad33442429fc9344405a6c5f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a51c8347b63499cbbbf767c54732d0c",
            "placeholder": "​",
            "style": "IPY_MODEL_c006b2c659e9480194520fbb5a6f443c",
            "value": "Downloading: 100%"
          }
        },
        "7b7052eb10504ec88bdb8a3eda51aaf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46c2917ddf564299820e6c258345df5e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a03bc90aa5ad4685b2bee138f51f6bf8",
            "value": 570
          }
        },
        "6c77e47405f24b9ea21b288a4aee06e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f82ce3bc30e41c0a19e4bafd4b19a98",
            "placeholder": "​",
            "style": "IPY_MODEL_9d5b4c6088a94e23bdd4aca71d325c05",
            "value": " 570/570 [00:00&lt;00:00, 13.8kB/s]"
          }
        },
        "443b9ec2fab740efa025b56c346cfff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a51c8347b63499cbbbf767c54732d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c006b2c659e9480194520fbb5a6f443c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46c2917ddf564299820e6c258345df5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a03bc90aa5ad4685b2bee138f51f6bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f82ce3bc30e41c0a19e4bafd4b19a98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d5b4c6088a94e23bdd4aca71d325c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}