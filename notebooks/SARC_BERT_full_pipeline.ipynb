{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "puzI3_xUNIXY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing packages"
      ],
      "metadata": {
        "id": "M9ayl2J9NEqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn import model_selection\n",
        "import shutil"
      ],
      "metadata": {
        "id": "A-4gO9hEEL1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "egDIgdo0TDb2",
        "outputId": "2eea0606-3c29-4a63-8465-0bd30c55b148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.11.0+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurations"
      ],
      "metadata": {
        "id": "puzI3_xUNIXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6fB6EqPjNne",
        "outputId": "b9f5d45d-e58e-46c9-9dd7-707b3005e696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE= 16\n",
        "MAXLEN=512\n",
        "LR= 0.005\n",
        "EPOCHS= 4\n"
      ],
      "metadata": {
        "id": "bOOJfZdZibMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "KVnqX13TOex6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWc79qPWD_9x"
      },
      "outputs": [],
      "source": [
        "keys= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/key.csv',sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyList= keys.columns.tolist()"
      ],
      "metadata": {
        "id": "y3Mi2g08FJkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/test-balanced.csv',delimiter= '\\t', names=keyList, header=None)\n",
        "train_df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/proj_sem4/data/train-balanced.csv',delimiter= '\\t', names=keyList, header=None)"
      ],
      "metadata": {
        "id": "rDJeg6MfEoRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "hU8vTd2RiHNc",
        "outputId": "ca981ce2-f703-439f-f7fa-b9d534c2715b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                            comment  \\\n",
              "0      0  I highly doubt this mostly ignored, surely uns...   \n",
              "1      0  Holy shit they are dropping an Halloween surpr...   \n",
              "2      0  Chafetz is a known liar (see PP vids) why does...   \n",
              "3      0  Kansas Number 1 in imaginary Muslim terrorists...   \n",
              "4      1  wow it is totally unreasonable to assume that ...   \n",
              "\n",
              "              author subreddit  score  ups  downs     date  created_utc  \\\n",
              "0     What_I_Thought  politics      0    0      0  2016-09   1473426915   \n",
              "1        Quinnjester  politics      0   -1     -1  2016-11   1477961322   \n",
              "2    TrumpsMonkeyPaw  politics      8   -1     -1  2016-10   1477928901   \n",
              "3  Ginsengstrip_2002  politics     19   -1     -1  2016-10   1477864377   \n",
              "4           pb2crazy  politics      2   -1     -1  2016-11   1477968131   \n",
              "\n",
              "                                      parent_comment  \n",
              "0  The GOP has the reputation, in recent times, o...  \n",
              "1  Donald Trump Used Legally Dubious Method to Av...  \n",
              "2                   Some principles you've got there  \n",
              "3  Kansas is probably the last state to have a te...  \n",
              "4  Clinton campaign accuses FBI of 'blatant doubl...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a2ec5b4-6a48-472a-afc7-23bee0639acc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I highly doubt this mostly ignored, surely uns...</td>\n",
              "      <td>What_I_Thought</td>\n",
              "      <td>politics</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>1473426915</td>\n",
              "      <td>The GOP has the reputation, in recent times, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Holy shit they are dropping an Halloween surpr...</td>\n",
              "      <td>Quinnjester</td>\n",
              "      <td>politics</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>1477961322</td>\n",
              "      <td>Donald Trump Used Legally Dubious Method to Av...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Chafetz is a known liar (see PP vids) why does...</td>\n",
              "      <td>TrumpsMonkeyPaw</td>\n",
              "      <td>politics</td>\n",
              "      <td>8</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>1477928901</td>\n",
              "      <td>Some principles you've got there</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Kansas Number 1 in imaginary Muslim terrorists...</td>\n",
              "      <td>Ginsengstrip_2002</td>\n",
              "      <td>politics</td>\n",
              "      <td>19</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>1477864377</td>\n",
              "      <td>Kansas is probably the last state to have a te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>wow it is totally unreasonable to assume that ...</td>\n",
              "      <td>pb2crazy</td>\n",
              "      <td>politics</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>1477968131</td>\n",
              "      <td>Clinton campaign accuses FBI of 'blatant doubl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a2ec5b4-6a48-472a-afc7-23bee0639acc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a2ec5b4-6a48-472a-afc7-23bee0639acc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a2ec5b4-6a48-472a-afc7-23bee0639acc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAVE AND LOAD FUNCTIONS"
      ],
      "metadata": {
        "id": "riSBQ0DsZ2xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckp(ckpt_path,model,optimizer=None):\n",
        "    checkpoint= torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    if optimizer!=None:\n",
        "      optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    valid_loss_min= checkpoint['valid_loss_min']\n",
        "    return model, optimizer, checkpoint['epoch'],valid_loss_min.item()\n",
        "def save_ckp(state, is_best, ckpt_path,best_model_path):\n",
        "    torch.save(state, ckpt_path)\n",
        "    if is_best:\n",
        "        best_path = best_model_path\n",
        "        shutil.copyfile(ckpt_path,best_path)"
      ],
      "metadata": {
        "id": "gzvZTDOpZ6gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download BERT Model and Tokenizer"
      ],
      "metadata": {
        "id": "BAGz7V3SOiD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1YJR-P3jpCy",
        "outputId": "a9627249-59d6-451d-e295-0856df3d858a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "rCdO8L-njPWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert= BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
      ],
      "metadata": {
        "id": "YhtP9alkjTu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save({'state_dict': bert.state_dict()}, \"/content/drive/MyDrive/Colab Notebooks/proj_sem4/bert_state_dict/bert.pt\")"
      ],
      "metadata": {
        "id": "azcqKc1Wc8Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpt = torch.load(\"/content/drive/MyDrive/Colab Notebooks/proj_sem4/bert_state_dict/bert.pt\")\n",
        "#use same initial weights for all models\n",
        "bert = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True, state_dict = checkpt['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmlGKD1EfKvF",
        "outputId": "b445f277-5e1f-4f38-f259-37b18e3aa566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "9SHWw8jvjWRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUSTOM DATA CLEANING"
      ],
      "metadata": {
        "id": "JwFhQO3AFLpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text= str(text)\n",
        "    text=  \" \".join(text.split())\n",
        "    text = text.lower()\n",
        "    if len(text) <=1:\n",
        "      return []\n",
        "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    text = pattern.sub('', text)\n",
        "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
        "    emoji = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    text = emoji.sub(r'', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\" im |^im\", \"where is\", text)\n",
        "\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\" hes |^hes\", \"he is\", text)\n",
        "\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\" shes |^shes\", \"she is\", text)\n",
        "\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\" thats |^thats\", \"that is\", text)\n",
        "\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\" whats |^whats\", \"what is\", text)\n",
        "\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\" wheres |^wheres\", \"where is\", text)\n",
        "\n",
        "    text = re.sub(r\"ain't\", \"is not\", text)\n",
        "    text = re.sub(r\" aint |^aint\", \"is not\", text)\n",
        "\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\" wont |^wont\", \"will not\", text)\n",
        "\n",
        "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
        "    text = re.sub(r\" wasnt |^wasnt\", \"was not\", text)\n",
        "\n",
        "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
        "    text = re.sub(r\" hasnt |^hasnt\", \"has not\", text)\n",
        "\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\" dont |^dont\", \"do not\", text)\n",
        "\n",
        "    text = re.sub(r\"didn't\", \"did not\", text)\n",
        "    text = re.sub(r\" didnt |^didnt\", \"did not\", text)\n",
        "\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\" cant |^cant\", \"can not\", text)\n",
        "\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\" its |^its\", \"it is\", text)\n",
        "\n",
        "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "    text = re.sub(r\" coudlnt |^couldnt\", \"could not\", text)\n",
        "\n",
        "    text = re.sub(r\"haven't\", \"have not\", text)\n",
        "    text = re.sub(r\" havent |^havent\", \"have not\", text)\n",
        "\n",
        "    text = re.sub(r\" theyre |^theyre\", \"they are\", text)\n",
        "\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "\n",
        "    #based on EDA, symbols{. ! ? } should be preserved\n",
        "    text = re.sub(r\"[,\\\"'@#$%^&*(){}/;`~:<>+=-]\", \" \", text)\n",
        "    \"\"\"\n",
        "    rather than subbing with \"\", it's better to sub with \" \", to show that there was something there,\n",
        "    not necessarily useful, but not worth nothing either. Situations in which 2 words are separated by , or - for example,\n",
        "    would retain their form as individual words. Subbing with \"\" would cause them to effectively get concatenated.\n",
        "    \"\"\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "ajY2soJ2FRkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "    sent = { 'comment': [] , 'parent_comment': [], 'label': [] }\n",
        "    for row in df.values:\n",
        "        comment = clean_text(row[1])\n",
        "        context = clean_text(row[9])\n",
        "        if len(comment)<=1 or len(context) <=1 :\n",
        "          continue\n",
        "        else:\n",
        "          sent['comment'].append(comment)\n",
        "          sent['parent_comment'].append(context)\n",
        "          sent['label'].append(row[0])\n",
        "    return sent"
      ],
      "metadata": {
        "id": "ynFULV2F2t-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train_df['comment']= train_df['comment'].apply(lambda x: clean_text(x))\n",
        "train_df['comment']= train_df['comment'].apply(lambda x: clean_text(x))\n",
        "train_df['parent_comment'] = train_df['parent_comment'].apply(lambda x: clean_text(x))\n",
        "test_df['parent_comment'] = test_df['parent_comment'].apply(lambda x: clean_text(x))\n",
        "\"\"\";"
      ],
      "metadata": {
        "id": "soW8yZqZ5g8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_set = clean_data(train_df)\n",
        "train_df2= pd.DataFrame(train_df_set)\n",
        "\n",
        "test_df_set = clean_data(test_df)\n",
        "test_df2 = pd.DataFrame(test_df_set)"
      ],
      "metadata": {
        "id": "6V1m29UN6zdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df2['comment'] = train_df['comment'].dropna(axis=0)\n",
        "test_df2['comment'] = test_df['comment'].dropna(axis=0)\n",
        "train_df2['parent_comment'] = train_df['parent_comment'].dropna(axis=0)\n",
        "test_df2['parent_comment'] = test_df['parent_comment'].dropna(axis=0)"
      ],
      "metadata": {
        "id": "2lu4oFI0sQBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df2.shape , train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRv4_7887vhk",
        "outputId": "25fa9c0b-45d0-418f-b71b-098b5b399498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47959, 3), (47970, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phTiOd7u7vCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUSTOM TRUNCATION FUNCTION"
      ],
      "metadata": {
        "id": "h-dBZrcBEvW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Truncation: longest first, middle part, last\n",
        "\n",
        "def truncation(text,\n",
        "               maxLen,\n",
        "               main_comment_Length=0,\n",
        "               is_context=False,\n",
        "               main_comment_has_special_tokens=False ,\n",
        "               has_special_tokens=False,\n",
        "               truncation_strategy='longest_first',\n",
        "               is_tokenized=False):\n",
        "\n",
        "    trunc=dict()\n",
        "    textList= list()\n",
        "\n",
        "    assert maxLen>=0 , \"Max length can't be <=0 if you want to truncate\"\n",
        "    assert not (is_context and has_special_tokens) , \"Context shouldn't have special tokens while using this function\"\n",
        "    #has_special_tokens(B)\n",
        "    # (A.B)'\n",
        "\n",
        "    assert is_context or main_comment_has_special_tokens==has_special_tokens, \"If text is main comment, then it can't be tokenized and not tokenized at the same time\"\n",
        "    # B-> main_comment_has_special_tokens==has_special_tokens\n",
        "    # (A+B)\n",
        "\n",
        "    assert text!=None or text!=[] , \"Text can't be None or empty list, i.e, []\"\n",
        "    assert not (is_context and main_comment_Length <=0) , \"Main comment length cannot be 0\"\n",
        "    \"\"\"\n",
        "    is_context(A) | main_comment == 0 (B)  |  Bool\n",
        "    1          |    1               |     0\n",
        "    1          |    0               |     1\n",
        "    0          |    1               |     0\n",
        "    0          |    0               |     0\n",
        "\n",
        "    -> A'B\n",
        "    \"\"\"\n",
        "\n",
        "    assert not is_tokenized or type(text)==list , \"Tokenized input has to be a list\"\n",
        "    #is_tokenized(A) ; type(text)==list (B)\n",
        "    #(A'+B)\n",
        "    if not is_tokenized:\n",
        "        textList=tokenizer.tokenize(text) #global tokenizer\n",
        "    else:\n",
        "        textList = text\n",
        "\n",
        "\n",
        "    length= len(textList)\n",
        "\n",
        "    #maxLen = maxLen-2 if has_special_tokens else maxLen\n",
        "\n",
        "    maxLen=  (maxLen- main_comment_Length - 2) if main_comment_has_special_tokens else (maxLen - main_comment_Length)\n",
        "\n",
        "    if length<=maxLen:\n",
        "        return textList\n",
        "\n",
        "    mid= length/2\n",
        "    split = maxLen/2\n",
        "    assert type(maxLen)==int\n",
        "    trunc['longest_first'] = textList[0:maxLen]\n",
        "    trunc['middle_part'] = textList[int ( mid-split ) : int (mid + split) ]\n",
        "    trunc['last']  = textList[-maxLen:]\n",
        "\n",
        "    assert len (trunc[truncation_strategy])>0, f\"length cannot be 0, for \\ntext: {text}\\ntextList: {textList}\\nlength: {length}\\nmaxLen:{maxLen}\\nmid:{mid}\\nsplit:{split}\\nmain_comment_length:{main_comment_Length}\"\n",
        "    assert len (trunc[truncation_strategy])<=512, f\"length cannot be > 512, for \\ntext: {text}\\ntextList: {textList}\\nlength: {length}\\nmaxLen:{maxLen}\\nmid:{mid}\\nsplit:{split}\\nmain_comment_length:{main_comment_Length}\"\n",
        "\n",
        "    return trunc[truncation_strategy]"
      ],
      "metadata": {
        "id": "9V8rrgGjE0_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split into Training and validation sets"
      ],
      "metadata": {
        "id": "Ig8GIUXyvVsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#take a fraction of the data\n",
        "training_df = train_df2.sample(frac=0.2, random_state=200).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "voSaRHfLnAHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmUuL7vz6vb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_examples =  training_df.sample(frac=0.8, random_state= 200)\n",
        "val_examples = training_df.drop(training_examples.index).reset_index(drop=True)\n",
        "training_examples= training_examples.reset_index(drop=True)\n",
        "\"\"\"\n",
        "trainSet, valSet = model_selection.train_test_split(train_df,train_size= 0.8, test_size=0.2, stratify=train_df.label.values)\n",
        "trainSet = trainSet.reset_index(drop=True )\n",
        "valSet = valSet.reset_index(drop=True )\n",
        "training_examples =  train_df.sample(frac=0.001, random_state= 200). reset_index(drop=True)\n",
        "train_df= df_train.sample(frac=0.8,random_state=200).reset_index(drop=True)\n",
        "val_df= df_train.drop(train_df.index).reset_index(drop=True)\n",
        "print (\"training example shape: \", training_examples.shape)\n",
        "\"\"\";"
      ],
      "metadata": {
        "id": "PCqvCxPHvTdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_examples.shape , val_examples.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3bI5bTMFrTm",
        "outputId": "b2de549b-2b46-42d3-fdcf-b21fd91927d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7674, 3), (1918, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DATASET Class**"
      ],
      "metadata": {
        "id": "05n-zsQzzFHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class my_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,df, tokenizer, max_len):\n",
        "        self.df = df\n",
        "        self.tokenizer= tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.text= self.df['comment']\n",
        "        self.context= self.df['parent_comment']\n",
        "        self.targets = self.df['label'].values\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        text= str(self.text[index])\n",
        "        text=  \" \".join(text.split())\n",
        "        \"\"\"\n",
        "        Can skip cleaning for now as its been done in earlier stages\n",
        "        \"\"\"\n",
        "        #cleaned_text= clean_text(text)\n",
        "        #if len (cleaned_text)<=1:\n",
        "        if len(text) <=1 :\n",
        "          print(\"text is empty or has length =1 :\",text)\n",
        "        #text= '[CLS]' + text + '[SEP]'\n",
        "\n",
        "        \"\"\"\n",
        "        Keep it like this till I don't figure out the issues with Truncate\n",
        "        trunc_text = truncation(cleaned_text,maxLen= self.max_len, truncation_strategy= 'middle_part')\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        #assert len(cleaned_text)>0 , f\"length of text must be > 0\\ntext: {text}\\ncleaned: {cleaned_text}\"\n",
        "\n",
        "        context= str(self.context[index])\n",
        "        context=  \" \".join(context.split())\n",
        "        #cleaned_context = clean_text(context)\n",
        "        \"\"\"\n",
        "        Keep it like this till I don't figure out the issues with Truncate\n",
        "        context = truncation(context,\n",
        "                             maxLen=self.max_len,\n",
        "                             is_context=True,\n",
        "                             truncation_strategy='middle_part', main_comment_Length=len(trunc_text))\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer.encode_plus(text ,\n",
        "                                      context,\n",
        "                               max_length=self.max_len,\n",
        "                               add_special_tokens=True,\n",
        "                               is_split_into_words=False, #Keep FALSE till I don't figure out the issues in truncate\n",
        "                               truncation= 'longest_first',\n",
        "                               padding='max_length',\n",
        "                               return_tensors='pt',\n",
        "                               return_attention_mask=True,\n",
        "                               return_token_type_ids=True )\n",
        "        assert len(inputs['input_ids'])<=512 , f\"input length can't be >512\\n index: {index}\\n text: {text}\\n context:{context}\"\n",
        "        target=self.targets[index]\n",
        "\n",
        "        return {\n",
        "            'input_ids':inputs['input_ids'].flatten(),\n",
        "            'attention_mask':inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids':inputs['token_type_ids'].flatten(),\n",
        "            'targets':torch.FloatTensor([target])\n",
        "        }"
      ],
      "metadata": {
        "id": "mTEHaq8izCbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Initialize my_Dataset object**"
      ],
      "metadata": {
        "id": "jcEXmef6ocB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set= my_Dataset(training_examples, tokenizer, MAXLEN)\n",
        "val_set= my_Dataset(val_examples, tokenizer, MAXLEN)"
      ],
      "metadata": {
        "id": "qEoBYtNuztlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader objects"
      ],
      "metadata": {
        "id": "EvCB0sNbNewi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    shuffle=True,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    num_workers= 0,\n",
        "    )\n",
        "valLoader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    shuffle=True,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    num_workers= 0,\n",
        "    )"
      ],
      "metadata": {
        "id": "mKH3o1CUGCru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function ,optimizer and accuracy functions"
      ],
      "metadata": {
        "id": "b13_mdcbNjwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return nn.BCEWithLogitsLoss()(outputs,targets)\n",
        "\n",
        "def optimizer_fn(model , learn_rate):\n",
        "  return torch.optim.Adam(params=model.parameters(),lr=learn_rate)\n",
        "\n",
        "def accuracy(outputs, targets):\n",
        "  sigmoid = nn.Sigmoid()\n",
        "  output= sigmoid (outputs )\n",
        "  output = output.cpu().detach().flatten().numpy()\n",
        "  target= targets.cpu().detach().flatten().numpy()\n",
        "  print(\"\\noutputs:{}\\ntargets: {}\".format( (output>0.5) , target==1.) )\n",
        "  return np.sum ((output>0.5) == (target==1.)) / len(output) *100"
      ],
      "metadata": {
        "id": "kfJdprVtGixr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing functions"
      ],
      "metadata": {
        "id": "dsD69GhhQVxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dictionary"
      ],
      "metadata": {
        "id": "-Kp7h0FkOVxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_dict = {'Vanilla_BUPO': {'train': {'predictions':[], 'targets': [] , 'loss':[], 'accuracy':[]},\n",
        "                                     'val': {'predictions':[], 'targets': [] , 'loss':[], 'accuracy':[]}\n",
        "                                    } ,\n",
        "                    'Vanilla_SOL': {'train': {'predictions':[], 'targets': [] , 'loss':[], 'accuracy':[]},\n",
        "                                     'val': {'predictions':[], 'targets': [] , 'loss':[], 'accuracy':[]}\n",
        "                                    }\n",
        "                    }"
      ],
      "metadata": {
        "id": "IZHImITHRi6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training fn"
      ],
      "metadata": {
        "id": "rdnawrkkOYXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(trainLoader,\n",
        "          model,\n",
        "          model_name,\n",
        "          optimizer,\n",
        "          epochs,\n",
        "          save=False,\n",
        "          validate=False,\n",
        "          valLoader=None,\n",
        "          curr_ckpt=None,\n",
        "          best_ckpt=None,\n",
        "          ):\n",
        "\n",
        "  assert model_name=='Vanilla_BUPO' or model_name=='Vanilla_SOL', \"model_name doesn't exist\"\n",
        "\n",
        "  assert not validate or valLoader!=None, \"valLoader can't be None if you want to perform the validation step\"\n",
        "\n",
        "  assert not save or (curr_ckpt!=None and best_ckpt!=None), \"To save the model, mention paths curr_ckpt and best_ckpt\"\n",
        "\n",
        "  train_loss_track = []\n",
        "  val_loss_track = []\n",
        "  val_loss_min= np.Inf\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_loss=0\n",
        "    #val_loss =0\n",
        "    model.train()\n",
        "    for index, batch in tqdm (enumerate (trainLoader), total= len(trainLoader)):\n",
        "\n",
        "      targets = batch['targets'].to(device, dtype=torch.float)\n",
        "      input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "      token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "      attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs= model(input_ids, token_type_ids, attention_mask)\n",
        "      #print( outputs.shape)\n",
        "      #print(targets.shape)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      train_batch_accuracy = accuracy(outputs, targets)\n",
        "\n",
        "      train_loss=train_loss + ((1/(index+1))*(loss.item()-train_loss))\n",
        "\n",
        "      train_loss_track.append(train_loss)\n",
        "\n",
        "      print(\"\\nAvg train_loss :{:.8f} | accuracy :{:.8f} | loss :{:.8f}\".format(train_loss,train_batch_accuracy,loss.item()) )\n",
        "\n",
        "      \"\"\"\n",
        "      prediction_dict[model_name]['train']['predictions'].extend(outputs.cpu().detach().numpy().tolist())\n",
        "      prediction_dict[model_name]['train']['targets'].extend(targets.cpu().detach().numpy().tolist())\n",
        "      prediction_dict[model_name]['train']['loss'].append(loss.item())\n",
        "      prediction_dict[model_name]['train']['accuracy'].append(train_batch_accuracy)\n",
        "      \"\"\"\n",
        "\n",
        "    if validate==True:\n",
        "      vlt , val_loss_min = validation(model, model_name, valLoader,  val_loss_min, optimizer, epoch, curr_ckpt, best_ckpt, save)\n",
        "      val_loss_track.extend(vlt)\n",
        "\n",
        "  return train_loss_track, val_loss_track, prediction_dict"
      ],
      "metadata": {
        "id": "ayjFpczzQY76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation fn"
      ],
      "metadata": {
        "id": "vbE6S5xlObgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validation(model, model_name, valLoader, val_loss_min ,optimizer, epoch, ckpt_path=None, best_model_path=None, save=False):\n",
        "      val_loss_track = []\n",
        "      val_loss=0\n",
        "      assert not save or (ckpt_path!=None and best_model_path!=None), \"To save the model, mention paths curr_ckpt and best_ckpt\"\n",
        "\n",
        "      #VALIDATION\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for index, batch in tqdm( enumerate(valLoader), total= len(valLoader)):\n",
        "\n",
        "          targets = batch['targets'].to(device, dtype=torch.float)\n",
        "          input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "          token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "          attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "          outputs= model(input_ids, token_type_ids, attention_mask)\n",
        "\n",
        "          loss = loss_fn(outputs, targets)\n",
        "\n",
        "          val_batch_acc= accuracy(outputs,targets)\n",
        "          val_loss= (val_loss + (1/(index+1))*(loss.item()-val_loss))\n",
        "\n",
        "          val_loss_track.append(val_loss)\n",
        "\n",
        "          print(\"\\nAvg val_loss :{:.8f} | val_accuracy :{:.8f} | val_loss :{:.8f}\".format(val_loss,val_batch_acc,loss.item()) )\n",
        "\n",
        "          prediction_dict[model_name]['val']['predictions'].extend(outputs.cpu().detach().numpy().tolist())\n",
        "          prediction_dict[model_name]['val']['targets'].extend(targets.cpu().detach().numpy().tolist())\n",
        "          prediction_dict[model_name]['val']['loss'].append(loss.item())\n",
        "          prediction_dict[model_name]['val']['accuracy'].append(val_batch_acc)\n",
        "\n",
        "          checkpoint ={\n",
        "            'epoch': epoch,\n",
        "            'valid_loss_min':val_loss,\n",
        "            'state_dict':model.state_dict(),\n",
        "            'optimizer':optimizer.state_dict()\n",
        "          }\n",
        "          save_ckp(checkpoint,False,ckpt_path,best_model_path)\n",
        "\n",
        "          if val_loss < val_loss_min:\n",
        "            print(\"previous Val_loss_min={:.4f}; new val_loss_min={:.4f}\".format(val_loss_min, val_loss))\n",
        "            save_ckp(checkpoint, True, ckpt_path, best_model_path)\n",
        "            print(\"SAVED\")\n",
        "            val_loss_min = val_loss\n",
        "          print(\"epoch {} end\".format(epoch))\n",
        "          return val_loss_track , val_loss_min"
      ],
      "metadata": {
        "id": "djYdRY6ME7tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "r8fCeTLvNrDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla_BERT Model using pooler output\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "s9CG-MJnDWbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vanilla_Bert_Using_Pooler_Output(nn.Module):\n",
        "  def __init__(self,model):\n",
        "    super(Vanilla_Bert_Using_Pooler_Output,self).__init__()\n",
        "    self.bert = model #want to use same initial BERT weights for all models\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.linear_layer = nn.Linear(768,1)\n",
        "    #self.activation_layer = nn.Sigmoid()\n",
        "  def forward(self, input_ids,attention_mask,token_type_ids):\n",
        "\n",
        "    bert_output = self.bert(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "    dropout_output= self.dropout(bert_output.pooler_output)\n",
        "\n",
        "    linear_output = self.linear_layer(dropout_output)\n",
        "\n",
        "    #activation_output =  self.activation_layer(linear_output)\n",
        "\n",
        "    return linear_output"
      ],
      "metadata": {
        "id": "XFhkvrgiNwNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vanilla_BUPO = Vanilla_Bert_Using_Pooler_Output(bert)\n",
        "Vanilla_BUPO.to(device);"
      ],
      "metadata": {
        "id": "Td1Y9_2gP7qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "optimizer_BUPO= optimizer_fn(Vanilla_BUPO, LR)\n",
        "torch.cuda.empty_cache()\n",
        "train_loss_track, val_loss_track, prediction_dict = train(model=Vanilla_BUPO,\n",
        "      model_name= \"Vanilla_BUPO\",\n",
        "      trainLoader= trainLoader,\n",
        "      valLoader= valLoader,\n",
        "      optimizer=optimizer_BUPO,\n",
        "      epochs=EPOCHS,\n",
        "      save= True,\n",
        "      validate= True,\n",
        "      curr_ckpt= VANILLA_BUPO_CKPT,\n",
        "      best_ckpt=VANILLA_BUPO_BEST );"
      ],
      "metadata": {
        "id": "_Knbhl30YF8O",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla_BERT Model using sum of last 4 layers"
      ],
      "metadata": {
        "id": "UU54cRfnHB84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bert models using sum of last 4 layers\n",
        "class Vanilla_Bert_Using_Sum_of_Layers(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(self, Vanilla_Bert_Using_Sum_of_Layers).__init__()\n",
        "    self.bert=model\n",
        "    self.linear_layer1 = nn.Linear(768,1)\n",
        "    self.linear_layer2 = nn.Linear(512,1)\n",
        "    self.activation1 = nn.Sigmoid()\n",
        "    self.activation2 = nn.Sigmoid()\n",
        "    self.dropout1 = nn.Dropout(0.3)\n",
        "    self.dropout2 = nn.Dropout(0.3)\n",
        "  def forward(self, input_ids,attention_mask,token_type_ids):\n",
        "\n",
        "    bert_output = self.bert(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "    ##get sum of last 4 hidden layers\n",
        "    hidden_states = bert_output[2]\n",
        "    sum_of_last_4_layers = torch.stack(hidden_states[-4:]).sum(0)\n",
        "\n",
        "    dropout_output = self.dropout1(sum_of_last_4_layers)\n",
        "\n",
        "    linear1_output = self.linear_layer1(dropout_output)\n",
        "    activation1_output = self.activation1(linear1_output.view(1,512))\n",
        "\n",
        "    linear2_output = self.linear_layer2(activation1_output)\n",
        "    activation2_output = self.activation2(linear2_output.flatten)"
      ],
      "metadata": {
        "id": "Ub_qOh1nDN5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vanilla_SOL = Vanilla_Bert_Using_Sum_of_Layers(bert)"
      ],
      "metadata": {
        "id": "_PqmDP0QC9Qt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}